# set 0 if using inference, 1 if using local ollama
is_local: 1

# leave empty or access_token for inference endpoint
access_token: "" 

# inference endpoint
base_url: "https://data-portal-dev.cels.anl.gov/resource_server/sophia/vllm/v1"

# https://github.com/argonne-lcf/inference-endpoints/tree/main eg: "meta-llama/Meta-Llama-3.1-8B-Instruct", or "llama3.1" ... for local ollama
# model: "meta-llama/Meta-Llama-3.1-8B-Instruct" 
model: "llama3.1"

# Answer or context distractors
distractor: "answer"

# Documents directory
document_dir: "/lus/eagle/projects/HPCBot/workspace/hpcbot/data"

# Generated QA save location, json file eg: "../output/QA_answer_distractor.json"
out_dir: "/lus/eagle/projects/HPCBot/workspace/hpcbot/output/QA_answer_2510.json"
