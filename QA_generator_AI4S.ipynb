{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from openai import OpenAI\n",
    "from glob import glob\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunk(path = \"data\", type = \"pdf\", chunk_size = 2048, chunk_overlap = 100, references = False): \n",
    "    \"\"\"\n",
    "    Only support pdf for now, references = True to keep reference section, refereces = False to remove reference section\n",
    "    \"\"\"\n",
    "    if type != \"pdf\":\n",
    "        raise TypeError(\"Currently only support pdf files\")\n",
    "\n",
    "    chunks_dict = defaultdict(list)\n",
    "\n",
    "    if references == True:\n",
    "        loader = PyPDFDirectoryLoader(path, glob = \"**/[!.]*.pdf\")\n",
    "        chunks = loader.load_and_split(RecursiveCharacterTextSplitter(chunk_size = chunk_size, chunk_overlap = chunk_overlap))\n",
    "        for chunk in chunks:\n",
    "            chunks_dict[chunk.metadata['source']].append(chunk.page_content)\n",
    "        \n",
    "        return chunks_dict\n",
    "\n",
    "    else:\n",
    "        pdf_files = glob(os.path.join(path, \"**/[!.]*.pdf\"), recursive= True)\n",
    "        for f in pdf_files:\n",
    "            loader = PyPDFLoader(f)\n",
    "            chunks = loader.load_and_split(RecursiveCharacterTextSplitter(chunk_size = chunk_size, chunk_overlap = chunk_overlap))\n",
    "            \n",
    "            possible_name = [\"\\nReferences\\n\", \"\\nREFERENCES\\n\"]\n",
    "            found = False\n",
    "            index = 0\n",
    "            n = len(chunks)\n",
    "            for j in possible_name:\n",
    "                if found == True:\n",
    "                    break\n",
    "                for i in range(n-1, -1, -1):\n",
    "                    if j in chunks[i].page_content:\n",
    "                        chunks[i].page_content = chunks[i].page_content[:chunks[i].page_content.rindex(j)]\n",
    "                        index = i\n",
    "                        found = True\n",
    "                        break\n",
    "            chunks = chunks[:index+1] if index != 0 else chunks    \n",
    "            chunks_dict[f].extend([chunk.page_content for chunk in chunks])\n",
    "            \n",
    "        return chunks_dict\n",
    "\n",
    "def generate_questions(chunk, num = 3, model = \"llama3\"):\n",
    "    \"\"\"\n",
    "    Generates `num` questions / use cases for `chunk`. Used when the input document is of general types \n",
    "    \"\"\"\n",
    "    messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a synthetic question-answer pair generator. Given a chunk of context about some topic(s), generate %s example questions a user could ask and that question could be able to answer using information from the chunk. For example, if the given context has information about supercomputer, an example question could be 'What is a supercomputer?'\" % (num)},\n",
    "                {\"role\": \"system\", \"content\": \"The questions should be able to be answered in a few words or less. Show the example questions in numbered list. Every questions MUST end with a question mark\"},\n",
    "                {\"role\": \"user\", \"content\": str(chunk)}\n",
    "            ]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=messages\n",
    "    )\n",
    "    queries = response.choices[0].message.content.split('\\n')\n",
    "\n",
    "    # Only include questions\n",
    "    queries = [q for q in queries if q.endswith(\"?\") and not (q.startswith(\"You are a synthetic\"))]\n",
    "\n",
    "    return [re.sub(r'^[\\d+\\.|*+\\.]+\\s', '', q) for q in queries] # If questions start with numbers or stars, remove them.\n",
    "\n",
    "def encode_question(question, chunk):\n",
    "    \"\"\"\n",
    "    Encode multiple prompt instructions into a single string to generate correct answer.\n",
    "    Using chain of thought improve answer accuracy\n",
    "    \"\"\"\n",
    "    \n",
    "    prompts = []\n",
    "        \n",
    "    prompt = \"\"\"\n",
    "        Question: {question}\\nContext: {context}\\n\n",
    "        Answer this question using the information given in the context above. Here is things to pay attention to: \n",
    "        - First provide step-by-step reasoning on how to answer the question. \n",
    "        - In the reasoning, if you need to copy paste some sentences from the context, include them in ##begin_quote## and ##end_quote##. This would mean that things outside of ##begin_quote## and ##end_quote## are not directly copy paste from the context. \n",
    "        - End your response with final answer in the form <ANSWER>: $answer, the answer should be succinct.\n",
    "        You MUST begin your final answer with the tag \"<ANSWER>:\".\n",
    "    \"\"\".format(question=question, context=str(chunk))\n",
    "    prompts.append({\"role\": \"system\", \"content\": \"You are a helpful question answerer who can provide an answer given a question and relevant context.\"})\n",
    "    prompts.append({\"role\": \"user\", \"content\": prompt})\n",
    "    return prompts\n",
    "\n",
    "def encode_question_incorrect(question, chunk, num_answer=4):\n",
    "    \"\"\"\n",
    "    Encode multiple prompt instructions into a single string to generate incorrect answers.\n",
    "    \"\"\"\n",
    "    \n",
    "    prompts = []\n",
    "        \n",
    "    prompt = \"\"\"\n",
    "        Question: {question}\\nContext: {context}\\n\n",
    "        Answer this question incorrectly in {num_answer} ways. \n",
    "        The incorrect answers should be succinct.        \n",
    "    \"\"\".format(question=question, context=str(chunk), num_answer=str(num_answer))\n",
    "    prompts.append({\"role\": \"system\", \"content\": \"You are a bad question answerer who can provide incorrect answers given a question and relevant context.\"})\n",
    "    prompts.append({\"role\": \"user\", \"content\": prompt})\n",
    "    return prompts\n",
    "\n",
    "def generate_label(question, chunk, model = \"llama3\"):\n",
    "    \"\"\"\n",
    "    Generates the correct answer to `question` using `context`.\n",
    "    \"\"\"\n",
    "    question = encode_question(question, chunk)\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=question,\n",
    "        n=1,\n",
    "        temperature=0\n",
    "    )\n",
    "    queries = response.choices[0].message.content\n",
    "\n",
    "    # Only include Answer\n",
    "    beg = \"<ANSWER>:\"\n",
    "    try:\n",
    "        start = queries.rindex(beg)\n",
    "        queries = queries[start+len(beg)+1:]\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return queries\n",
    "\n",
    "\n",
    "def generate_incorrect_answer(question, chunk, num_answer = 4, model = \"llama3\"):\n",
    "    \"\"\"\n",
    "    Generates {num_answer} incorrect answers to `question`.\n",
    "    \"\"\"\n",
    "    question = encode_question_incorrect(question, chunk, num_answer)\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=question,\n",
    "        n=1,\n",
    "        temperature=0.8 # increase temperature for LLM be more creative with incorrect answer\n",
    "    )\n",
    "    queries = response.choices[0].message.content.split('\\n')\n",
    "    pattern = r'^[\\d+\\.|\\*+\\.|\\*\\*Answer:\\*\\*|\\d+\\.+\\s+\\*\\*Answer:\\*\\*]+\\s'\n",
    "    \n",
    "    return [re.sub(pattern, '', a) for a in filter(None, queries) if a[0].isdigit()] # return list of incorrect answers\n",
    "\n",
    "def run(i, chunk, num = 3, num_answer = 4, model = \"llama3\"):\n",
    "    \"\"\"\n",
    "    Given a chunk, create {Questions, answer, incorrect answers}.\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    questions = generate_questions(chunk, num, model)\n",
    "    for j, q in enumerate(questions):\n",
    "        datapt = {\n",
    "            \"id\": None,\n",
    "            \"context\": None,\n",
    "            \"question\": None,\n",
    "            \"correct_answer\": None,\n",
    "            \"incorrect_answers\": None\n",
    "        }\n",
    "        datapt[\"id\"] = f\"chunk_{i}_question_{j}\" # id of chunk_question for easier tracking\n",
    "        datapt[\"question\"] = q\n",
    "        datapt[\"context\"] = chunk.split(\"\\n\")\n",
    "\n",
    "        # add answer to data\n",
    "        datapt[\"correct_answer\"] = generate_label(q, chunk, model) \n",
    "\n",
    "        # add incorrect answer to data\n",
    "        datapt[\"incorrect_answers\"] = generate_incorrect_answer(q, chunk, num_answer, model) \n",
    "\n",
    "        res.append(datapt)\n",
    "    return res "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZE = 2000\n",
    "NUM_INCORRECT_ANSWERS = 4\n",
    "CHUNK_OVERLAP = 100\n",
    "NUM_QUESTION = 3\n",
    "MODEL = \"llama3\" # local LLM downloadable from Ollama or gpt\n",
    "\n",
    "# init OpenAI client\n",
    "client = OpenAI(\n",
    "    base_url = 'http://localhost:11434/v1', # remove this line if using gpt\n",
    "    api_key='ollama', # [ollama, OPENAI_API_KEY] local LLM or using gpt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = defaultdict(list)\n",
    "path = \"data/pdf/RAG_papers\" # path to folder \n",
    "type = \"pdf\" # only support pdf for now\n",
    "chunks_dict = get_chunk(path=path, type=type, chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP, references = False) \n",
    "\n",
    "# generate question, correct answer, incorrect answer\n",
    "for (source, chunks) in chunks_dict.items():\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        data[source].extend(run(i, chunk, num = NUM_QUESTION, num_answer = NUM_INCORRECT_ANSWERS, model = MODEL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data to a json file\n",
    "out_path = 'output/QA_RAG_AI4S_2000.json'\n",
    "with open(out_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6a3a50cfa768e55079b834244adbc23d62a4b24bbd2f8b443e30bd8c79d3f7ea"
  },
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
