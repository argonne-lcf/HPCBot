[
    {
        "id": "seed_task_0_0",
        "question": "1. Who wrote the paper RAFT: Adapting Language Model to Domain Specific RAG?",
        "context": [
            "Preprint, Under Review\nRAFT: Adapting Language Model to Domain Specific RAG\nTianjun Zhang∗\nDepartment of Computer Science\nUC Berkeley\nBerkeley, CA 94720, USA\n{tianjunz}@berkeley.eduShishir G. Patil, Naman Jain, Sheng Shen\nDepartment of Computer Science\nUC Berkeley\nBerkeley, CA 94720, USA\n{shishirpatil,naman_jain,sheng.s}@berkeley.edu\nMatei Zaharia, Ion Stoica, Joseph E. Gonzalez\nDepartment of Computer Science\nUC Berkeley\nBerkeley, CA 94720, USA\n{matei,istoica,jegonzal}@berkeley.edu\nAbstract",
            "demonstrates a scenario where the DSF model becomes confused by a question asking for\nthe identity of a screenwriter. Instead of providing the correct name, it mistakenly cites one\nof the films written by the screenwriter. In contrast, the RAFT model accurately answers the\nquestion. This discrepancy suggests that training a model solely with question-answer pairs\nmay impair its ability to derive relevant context from provided documents. The comparison",
            "indicates that training your LLM without the correct corresponding context at times can be\nbeneficial for the downstream task of answering questions related to the documents. In our\ntraining setup, we include four distractor documents alongside the golden document, and at\ntest time, we maintain this format by providing the golden document with four distractors.\nOur findings suggest that, for domain-specific RAG tasks, including a certain percentage of",
            "the learning opportunity afforded by the fixed domain setting and early access to the test\ndocuments. Alternatively, supervised fine-tuning offers the opportunity to learn more\ngeneral patterns in the documents and better align to end tasks and user preferences Zhou\net al. (2023). However, existing fine-tuning based approaches either fail to leverage the\ndocuments at test time (don’t incorporate RAG) or fail to account for the imperfections in\nretrieval process during training."
        ],
        "oracle_context": "Preprint, Under Review\nRAFT: Adapting Language Model to Domain Specific RAG\nTianjun Zhang∗\nDepartment of Computer Science\nUC Berkeley\nBerkeley, CA 94720, USA\n{tianjunz}@berkeley.eduShishir G. Patil, Naman Jain, Sheng Shen\nDepartment of Computer Science\nUC Berkeley\nBerkeley, CA 94720, USA\n{shishirpatil,naman_jain,sheng.s}@berkeley.edu\nMatei Zaharia, Ion Stoica, Joseph E. Gonzalez\nDepartment of Computer Science\nUC Berkeley\nBerkeley, CA 94720, USA\n{matei,istoica,jegonzal}@berkeley.edu\nAbstract",
        "cot_answer": "To answer this question, I will follow these steps:\n\n1. Identify the authors listed in the context.\n2. Check if any of the authors are mentioned as writing the paper \"RAFT: Adapting Language Model to Domain Specific RAG\".\n\nHere's my reasoning:\n\nThe context provides a list of authors with their affiliations and email addresses. The paper being referred to is \"RAFT: Adapting Language Model to Domain Specific RAG\". To find out who wrote this paper, I will scan the author list.\n\n##begin_quote##\nTianjun Zhang∗\nDepartment of Computer Science\nUC Berkeley\nBerkeley, CA 94720, USA\n{tianjunz}@berkeley.edu\nShishir G. Patil, Naman Jain, Sheng Shen\nDepartment of Computer Science\nUC Berkeley\nBerkeley, CA 94720, USA\n{shishirpatil,naman_jain,sheng.s}@berkeley.edu\nMatei Zaharia, Ion Stoica, Joseph E. Gonzalez\nDepartment of Computer Science\nUC Berkeley\nBerkeley, CA 94720, USA\n{matei,istoica,jegonzal}@berkeley.edu\n##end_quote##\n\nThe asterisk symbol (∗) next to Tianjun Zhang's name suggests that he might be the lead author or the one who wrote the paper. Therefore, I will check if his name is mentioned as writing the paper.\n\nAccording to the abstract, which is not provided in the context, it seems that Tianjun Zhang is indeed the author of the paper \"RAFT: Adapting Language Model to Domain Specific RAG\".\n\nFinal answer:\n\n<ANSWER>: Tianjun Zhang",
        "instruction": "<DOCUMENT>Preprint, Under Review\nRAFT: Adapting Language Model to Domain Specific RAG\nTianjun Zhang∗\nDepartment of Computer Science\nUC Berkeley\nBerkeley, CA 94720, USA\n{tianjunz}@berkeley.eduShishir G. Patil, Naman Jain, Sheng Shen\nDepartment of Computer Science\nUC Berkeley\nBerkeley, CA 94720, USA\n{shishirpatil,naman_jain,sheng.s}@berkeley.edu\nMatei Zaharia, Ion Stoica, Joseph E. Gonzalez\nDepartment of Computer Science\nUC Berkeley\nBerkeley, CA 94720, USA\n{matei,istoica,jegonzal}@berkeley.edu\nAbstract</DOCUMENT>\n<DOCUMENT>demonstrates a scenario where the DSF model becomes confused by a question asking for\nthe identity of a screenwriter. Instead of providing the correct name, it mistakenly cites one\nof the films written by the screenwriter. In contrast, the RAFT model accurately answers the\nquestion. This discrepancy suggests that training a model solely with question-answer pairs\nmay impair its ability to derive relevant context from provided documents. The comparison</DOCUMENT>\n<DOCUMENT>indicates that training your LLM without the correct corresponding context at times can be\nbeneficial for the downstream task of answering questions related to the documents. In our\ntraining setup, we include four distractor documents alongside the golden document, and at\ntest time, we maintain this format by providing the golden document with four distractors.\nOur findings suggest that, for domain-specific RAG tasks, including a certain percentage of</DOCUMENT>\n<DOCUMENT>the learning opportunity afforded by the fixed domain setting and early access to the test\ndocuments. Alternatively, supervised fine-tuning offers the opportunity to learn more\ngeneral patterns in the documents and better align to end tasks and user preferences Zhou\net al. (2023). However, existing fine-tuning based approaches either fail to leverage the\ndocuments at test time (don’t incorporate RAG) or fail to account for the imperfections in\nretrieval process during training.</DOCUMENT>\n1. Who wrote the paper RAFT: Adapting Language Model to Domain Specific RAG?"
    },
    {
        "id": "seed_task_0_1",
        "question": "2. Where is the Department of Computer Science located?",
        "context": [
            "model does not know, it can still refer to the context.\n4.1 Results\nUsing the above datasets and baselines, we evaluate our model RAFT and demonstrate\nthe effectiveness of RAFT in Tab. 1. We see that RAFT consistently and significantly\noutperforms the baselines. Compared with the base Llama-2 instruction-tuned model,\nRAFT with RAG does much better in terms of extracting information as well as being\nrobust towards distractors. The gain can be as big as 35.25% on Hotpot QA and 76.35% on",
            "Preprint, Under Review\n2 4 6 8 10\n# T est Documents (T op-k)0.220.240.260.280.300.32Final Accuracy\nNatural Questions\nTrain D*\nTrain D* + 1D\nTrain D* + 2D\nTrain D* + 3D\n2 4 6 8 10\n# T est Documents (T op-k)0.1250.1500.1750.2000.2250.250Final Accuracy\nHotpot QA\nTrain D*\nTrain D* + 1D\nTrain D* + 2D\nTrain D* + 3D\nFigure 6: Test-Time Documents Varying : To analyze how robust RAFT is to varying number\nof test-time documents, we study three domains – NQ, Trivia QA and HotPot QA. In NQ,",
            "Preprint, Under Review\nShi, W., Min, S., Lomeli, M., Zhou, C., Li, M., Lin, V ., Smith, N. A., Zettlemoyer, L., Yih, S.,\nand Lewis, M. In-context pretraining: Language modeling beyond document boundaries.\narXiv preprint arXiv:2310.10638 , 2023c.\nShi, W., Min, S., Yasunaga, M., Seo, M., James, R., Lewis, M., Zettlemoyer, L., and Yih, W.-t.\nReplug: Retrieval-augmented black-box language models. arXiv preprint arXiv:2301.12652 ,\n2023d.",
            "language models. Journal of Machine Learning Research , 24(251):1–43, 2023. URL http:\n//jmlr.org/papers/v24/23-0037.html .\nJin, Q., Dhingra, B., Liu, Z., Cohen, W. W., and Lu, X. Pubmedqa: A dataset for biomedical\nresearch question answering. arXiv preprint arXiv:1909.06146 , 2019.\n10"
        ],
        "oracle_context": "Preprint, Under Review\nRAFT: Adapting Language Model to Domain Specific RAG\nTianjun Zhang∗\nDepartment of Computer Science\nUC Berkeley\nBerkeley, CA 94720, USA\n{tianjunz}@berkeley.eduShishir G. Patil, Naman Jain, Sheng Shen\nDepartment of Computer Science\nUC Berkeley\nBerkeley, CA 94720, USA\n{shishirpatil,naman_jain,sheng.s}@berkeley.edu\nMatei Zaharia, Ion Stoica, Joseph E. Gonzalez\nDepartment of Computer Science\nUC Berkeley\nBerkeley, CA 94720, USA\n{matei,istoica,jegonzal}@berkeley.edu\nAbstract",
        "cot_answer": "To answer this question, I will follow these steps:\n\n1. Identify the relevant information in the context that mentions the location of the Department of Computer Science.\n\n##begin_quote##\nDepartment of Computer Science\nUC Berkeley\nBerkeley, CA 94720, USA\n{tianjunz}@berkeley.edu\nShishir G. Patil, Naman Jain, Sheng Shen\nDepartment of Computer Science\nUC Berkeley\nBerkeley, CA 94720, USA\n{shishirpatil,naman_jain,sheng.s}@berkeley.edu\nMatei Zaharia, Ion Stoica, Joseph E. Gonzalez\nDepartment of Computer Science\nUC Berkeley\nBerkeley, CA 94720, USA\n{matei,istoica,jegonzal}@berkeley.edu\n##end_quote##\n\n2. Extract the location information from the relevant text.\n\nThe extracted information is:\n\n* UC Berkeley\n* Berkeley, CA 94720, USA\n\n3. Combine the extracted information to form the answer.\n\nBased on the steps above, I conclude that:\n\n<ANSWER>: The Department of Computer Science is located at UC Berkeley in Berkeley, CA 94720, USA.",
        "instruction": "<DOCUMENT>model does not know, it can still refer to the context.\n4.1 Results\nUsing the above datasets and baselines, we evaluate our model RAFT and demonstrate\nthe effectiveness of RAFT in Tab. 1. We see that RAFT consistently and significantly\noutperforms the baselines. Compared with the base Llama-2 instruction-tuned model,\nRAFT with RAG does much better in terms of extracting information as well as being\nrobust towards distractors. The gain can be as big as 35.25% on Hotpot QA and 76.35% on</DOCUMENT>\n<DOCUMENT>Preprint, Under Review\n2 4 6 8 10\n# T est Documents (T op-k)0.220.240.260.280.300.32Final Accuracy\nNatural Questions\nTrain D*\nTrain D* + 1D\nTrain D* + 2D\nTrain D* + 3D\n2 4 6 8 10\n# T est Documents (T op-k)0.1250.1500.1750.2000.2250.250Final Accuracy\nHotpot QA\nTrain D*\nTrain D* + 1D\nTrain D* + 2D\nTrain D* + 3D\nFigure 6: Test-Time Documents Varying : To analyze how robust RAFT is to varying number\nof test-time documents, we study three domains – NQ, Trivia QA and HotPot QA. In NQ,</DOCUMENT>\n<DOCUMENT>Preprint, Under Review\nShi, W., Min, S., Lomeli, M., Zhou, C., Li, M., Lin, V ., Smith, N. A., Zettlemoyer, L., Yih, S.,\nand Lewis, M. In-context pretraining: Language modeling beyond document boundaries.\narXiv preprint arXiv:2310.10638 , 2023c.\nShi, W., Min, S., Yasunaga, M., Seo, M., James, R., Lewis, M., Zettlemoyer, L., and Yih, W.-t.\nReplug: Retrieval-augmented black-box language models. arXiv preprint arXiv:2301.12652 ,\n2023d.</DOCUMENT>\n<DOCUMENT>language models. Journal of Machine Learning Research , 24(251):1–43, 2023. URL http:\n//jmlr.org/papers/v24/23-0037.html .\nJin, Q., Dhingra, B., Liu, Z., Cohen, W. W., and Lu, X. Pubmedqa: A dataset for biomedical\nresearch question answering. arXiv preprint arXiv:1909.06146 , 2019.\n10</DOCUMENT>\n2. Where is the Department of Computer Science located?"
    },
    {
        "id": "seed_task_0_2",
        "question": "3. What does the \"∗\" symbol represent in the author list?",
        "context": [
            "except here we include reference documents. This is a popular technique when\ndealing with domain-specific QA tasks.\n•Domain-Specific Finetuning with 0-shot prompting (DSF): Standard supervised-\nfinetuning, without documents in context. We find that its mostly useful to align\nthe answering style of the model as well as get familiar with the domain context.\n•Domain-Specific Finetuning with RAG (DSF + RAG): Equip a domain-specific\nfinetuned-model with external knowledge using RAG. So, for the “knowledge” the",
            "but also to ensure robustness against distracting retrieved information. This is achieved\nby training the models to understand the dynamics between the question (prompt), the\ndomain-specific documents retrieved, and the right answer. Going back to our analogy to\nthe open book exam, our approach is analogous to studying for an open-book exam by\nrecognizing relevant, and irrelevant retrieved documents.\nIn RAFT, we train the model to answer the question (Q) from Document(s) (D*) to generate",
            "important it is for the model to learn with Chain-of-Thought responses. In this section,\nwe will first introduce all the datasets we used in the experiments, then all the baseline\nmodel/fine-tuning techniques that we benchmark against.\n4",
            "the context. The training data for RAFT is as follows, and an example training data can be\nseen in Fig. 3:\nP% of data: Q+D∗+D1+D2+ . . . + Dk→A∗\n(1−P) % of data: Q+D1+D2+ . . . + Dk→A∗\nSubsequently, for the test scenario, the model is provided with the Q and top-k documents\nretrieved by the RAG pipeline. Note that RAFT is independent of the retriever used.\nA key factor in enhancing training quality is the generation of a reasoning process, such"
        ],
        "oracle_context": "Preprint, Under Review\nRAFT: Adapting Language Model to Domain Specific RAG\nTianjun Zhang∗\nDepartment of Computer Science\nUC Berkeley\nBerkeley, CA 94720, USA\n{tianjunz}@berkeley.eduShishir G. Patil, Naman Jain, Sheng Shen\nDepartment of Computer Science\nUC Berkeley\nBerkeley, CA 94720, USA\n{shishirpatil,naman_jain,sheng.s}@berkeley.edu\nMatei Zaharia, Ion Stoica, Joseph E. Gonzalez\nDepartment of Computer Science\nUC Berkeley\nBerkeley, CA 94720, USA\n{matei,istoica,jegonzal}@berkeley.edu\nAbstract",
        "cot_answer": "To answer this question, I will follow these steps:\n\n1. Look at the author list and identify the symbol \"∗\" next to a name.\n\n##begin_quote##\nTianjun Zhang∗\n##end_quote##\n\n2. Compare the format of the names with and without the \"∗\" symbol.\n\nThe names without the \"∗\" symbol follow this format: First Name Last Name, while the name with the \"∗\" symbol is just First Name.\n\n3. Based on these observations, conclude that the \"∗\" symbol represents something specific to Tianjun Zhang's authorship.\n\n<ANSWER>: The \"∗\" symbol represents the lead author or primary contact for Tianjun Zhang.",
        "instruction": "<DOCUMENT>except here we include reference documents. This is a popular technique when\ndealing with domain-specific QA tasks.\n•Domain-Specific Finetuning with 0-shot prompting (DSF): Standard supervised-\nfinetuning, without documents in context. We find that its mostly useful to align\nthe answering style of the model as well as get familiar with the domain context.\n•Domain-Specific Finetuning with RAG (DSF + RAG): Equip a domain-specific\nfinetuned-model with external knowledge using RAG. So, for the “knowledge” the</DOCUMENT>\n<DOCUMENT>but also to ensure robustness against distracting retrieved information. This is achieved\nby training the models to understand the dynamics between the question (prompt), the\ndomain-specific documents retrieved, and the right answer. Going back to our analogy to\nthe open book exam, our approach is analogous to studying for an open-book exam by\nrecognizing relevant, and irrelevant retrieved documents.\nIn RAFT, we train the model to answer the question (Q) from Document(s) (D*) to generate</DOCUMENT>\n<DOCUMENT>important it is for the model to learn with Chain-of-Thought responses. In this section,\nwe will first introduce all the datasets we used in the experiments, then all the baseline\nmodel/fine-tuning techniques that we benchmark against.\n4</DOCUMENT>\n<DOCUMENT>the context. The training data for RAFT is as follows, and an example training data can be\nseen in Fig. 3:\nP% of data: Q+D∗+D1+D2+ . . . + Dk→A∗\n(1−P) % of data: Q+D1+D2+ . . . + Dk→A∗\nSubsequently, for the test scenario, the model is provided with the Q and top-k documents\nretrieved by the RAG pipeline. Note that RAFT is independent of the retriever used.\nA key factor in enhancing training quality is the generation of a reasoning process, such</DOCUMENT>\n3. What does the \"∗\" symbol represent in the author list?"
    },
    {
        "id": "seed_task_2_0",
        "question": "1. What is RAFT trained to do with distractor documents?",
        "context": [
            "Preprint, Under Review\nFigure 2: Overview of our RAFT method. The top-left figure depicts our approach of\nadapting LLMs to reading solution from a set of positive and distractor documents in\ncontrast to standard RAG setup where models are trained based on the retriever outputs,\nwhich is a mixture of both memorization and reading. At test time, all methods follow the\nstandard RAG setting, provided with a top-k retrieved documents in the context.",
            "to answer questions in \"open-book\" in-domain settings. In training RAFT,\ngiven a question, and a set of retrieved documents, we train the model to\nignore those documents that don’t help in answering the question, which\nwe call, distractor documents. RAFT accomplishes this by citing verbatim\nthe right sequence from the relevant document to help answer the question.\nThis coupled with RAFT’s chain-of-thought-style response helps improve\nthe model’s ability to reason. In domain specific RAG, RAFT consistently",
            "constructed after the training cutoff) is essential to many emerging applications (Vu et al.,\n2023; Lazaridou et al., 2022) and is the focus of this work.\nThis paper studies the following question – How do we adapt pre-trained LLMs for Retrieval\nAugmented Generation (RAG) in specialized domains?\nWhen it comes to adapting LLMs to specialized domains, we consider the following two\ncandidates: in-context learning through Retrieval-Augmented Generation (RAG) and super-",
            "text in retrieved documents, we adopted a finetuning approach that incorporates both\ngolden (highly relevant) documents and distractor (irrelevant) documents. The model was\ntrained with varying numbers of distractor documents, but consistently evaluated using\nthe top-3 documents obtained from the retriever - not to be confused with p. Our findings,\ndetailed in Fig. 6, reveal that finetuning with only the golden document frequently results in"
        ],
        "oracle_context": "to answer questions in \"open-book\" in-domain settings. In training RAFT,\ngiven a question, and a set of retrieved documents, we train the model to\nignore those documents that don’t help in answering the question, which\nwe call, distractor documents. RAFT accomplishes this by citing verbatim\nthe right sequence from the relevant document to help answer the question.\nThis coupled with RAFT’s chain-of-thought-style response helps improve\nthe model’s ability to reason. In domain specific RAG, RAFT consistently",
        "cot_answer": "To answer this question, I'll provide step-by-step reasoning.\n\nStep 1: Understand the context. The context states that RAFT is trained to answer questions in \"open-book\" in-domain settings. This means that RAFT is given a question and a set of retrieved documents related to the question.\n\nStep 2: Identify the specific task mentioned in the context. According to the context, RAFT is trained to ignore distractor documents, which are documents that don't help answer the question.\n\nStep 3: Understand how RAFT accomplishes this task. The context states that RAFT \"cites verbatim the right sequence from the relevant document to help answer the question.\" This means that RAFT extracts a specific part of the relevant document and uses it to answer the question.\n\nStep 4: Relate this back to the original question. The question asks what RAFT is trained to do with distractor documents. Based on the context, we can conclude that RAFT is trained to ignore distractor documents by extracting relevant information from the relevant documents.\n\nFinal Answer:\n<ANSWER>: RAFT is trained to ignore distractor documents by citing verbatim the right sequence from the relevant document to help answer the question.",
        "instruction": "<DOCUMENT>Preprint, Under Review\nFigure 2: Overview of our RAFT method. The top-left figure depicts our approach of\nadapting LLMs to reading solution from a set of positive and distractor documents in\ncontrast to standard RAG setup where models are trained based on the retriever outputs,\nwhich is a mixture of both memorization and reading. At test time, all methods follow the\nstandard RAG setting, provided with a top-k retrieved documents in the context.</DOCUMENT>\n<DOCUMENT>to answer questions in \"open-book\" in-domain settings. In training RAFT,\ngiven a question, and a set of retrieved documents, we train the model to\nignore those documents that don’t help in answering the question, which\nwe call, distractor documents. RAFT accomplishes this by citing verbatim\nthe right sequence from the relevant document to help answer the question.\nThis coupled with RAFT’s chain-of-thought-style response helps improve\nthe model’s ability to reason. In domain specific RAG, RAFT consistently</DOCUMENT>\n<DOCUMENT>constructed after the training cutoff) is essential to many emerging applications (Vu et al.,\n2023; Lazaridou et al., 2022) and is the focus of this work.\nThis paper studies the following question – How do we adapt pre-trained LLMs for Retrieval\nAugmented Generation (RAG) in specialized domains?\nWhen it comes to adapting LLMs to specialized domains, we consider the following two\ncandidates: in-context learning through Retrieval-Augmented Generation (RAG) and super-</DOCUMENT>\n<DOCUMENT>text in retrieved documents, we adopted a finetuning approach that incorporates both\ngolden (highly relevant) documents and distractor (irrelevant) documents. The model was\ntrained with varying numbers of distractor documents, but consistently evaluated using\nthe top-3 documents obtained from the retriever - not to be confused with p. Our findings,\ndetailed in Fig. 6, reveal that finetuning with only the golden document frequently results in</DOCUMENT>\n1. What is RAFT trained to do with distractor documents?"
    },
    {
        "id": "seed_task_2_1",
        "question": "2. How does RAFT help improve its ability to reason?",
        "context": [
            "Preprint, Under Review\nJoshi, M., Choi, E., Weld, D. S., and Zettlemoyer, L. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551 ,\n2017.\nKandpal, N., Wallace, E., and Raffel, C. Deduplicating training data mitigates privacy risks\nin language models. In International Conference on Machine Learning , pp. 10697–10707.\nPMLR, 2022.\nKhandelwal, U., Levy, O., Jurafsky, D., Zettlemoyer, L., and Lewis, M. General-",
            "language models. Journal of Machine Learning Research , 24(251):1–43, 2023. URL http:\n//jmlr.org/papers/v24/23-0037.html .\nJin, Q., Dhingra, B., Liu, Z., Cohen, W. W., and Lu, X. Pubmedqa: A dataset for biomedical\nresearch question answering. arXiv preprint arXiv:1909.06146 , 2019.\n10",
            "to answer questions in \"open-book\" in-domain settings. In training RAFT,\ngiven a question, and a set of retrieved documents, we train the model to\nignore those documents that don’t help in answering the question, which\nwe call, distractor documents. RAFT accomplishes this by citing verbatim\nthe right sequence from the relevant document to help answer the question.\nThis coupled with RAFT’s chain-of-thought-style response helps improve\nthe model’s ability to reason. In domain specific RAG, RAFT consistently",
            "constructed after the training cutoff) is essential to many emerging applications (Vu et al.,\n2023; Lazaridou et al., 2022) and is the focus of this work.\nThis paper studies the following question – How do we adapt pre-trained LLMs for Retrieval\nAugmented Generation (RAG) in specialized domains?\nWhen it comes to adapting LLMs to specialized domains, we consider the following two\ncandidates: in-context learning through Retrieval-Augmented Generation (RAG) and super-"
        ],
        "oracle_context": "to answer questions in \"open-book\" in-domain settings. In training RAFT,\ngiven a question, and a set of retrieved documents, we train the model to\nignore those documents that don’t help in answering the question, which\nwe call, distractor documents. RAFT accomplishes this by citing verbatim\nthe right sequence from the relevant document to help answer the question.\nThis coupled with RAFT’s chain-of-thought-style response helps improve\nthe model’s ability to reason. In domain specific RAG, RAFT consistently",
        "cot_answer": "To answer this question, I'll provide step-by-step reasoning:\n\n1. The question asks how RAFT helps improve its ability to reason.\n2. According to the context, RAFT is trained to ignore distractor documents and focus on relevant ones that help answer a given question.\n3. To accomplish this, RAFT cites verbatim the right sequence from the relevant document, which helps in answering the question.\n4. This process, combined with RAFT's chain-of-thought-style response, improves the model's ability to reason.\n\n##begin_quote##In training RAFT, given a question, and a set of retrieved documents, we train the model to ignore those documents that don’t help in answering the question, which we call, distractor documents. RAFT accomplishes this by citing verbatim the right sequence from the relevant document to help answer the question. This coupled with RAFT’s chain-of-thought-style response helps improve the model’s ability to reason.##end_quote##\n\nBased on the reasoning above, the final answer is:\n\n<ANSWER>: RAFT improves its ability to reason through its training process of ignoring distractor documents and citing relevant sequences from relevant documents, combined with its chain-of-thought-style responses.",
        "instruction": "<DOCUMENT>Preprint, Under Review\nJoshi, M., Choi, E., Weld, D. S., and Zettlemoyer, L. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551 ,\n2017.\nKandpal, N., Wallace, E., and Raffel, C. Deduplicating training data mitigates privacy risks\nin language models. In International Conference on Machine Learning , pp. 10697–10707.\nPMLR, 2022.\nKhandelwal, U., Levy, O., Jurafsky, D., Zettlemoyer, L., and Lewis, M. General-</DOCUMENT>\n<DOCUMENT>language models. Journal of Machine Learning Research , 24(251):1–43, 2023. URL http:\n//jmlr.org/papers/v24/23-0037.html .\nJin, Q., Dhingra, B., Liu, Z., Cohen, W. W., and Lu, X. Pubmedqa: A dataset for biomedical\nresearch question answering. arXiv preprint arXiv:1909.06146 , 2019.\n10</DOCUMENT>\n<DOCUMENT>to answer questions in \"open-book\" in-domain settings. In training RAFT,\ngiven a question, and a set of retrieved documents, we train the model to\nignore those documents that don’t help in answering the question, which\nwe call, distractor documents. RAFT accomplishes this by citing verbatim\nthe right sequence from the relevant document to help answer the question.\nThis coupled with RAFT’s chain-of-thought-style response helps improve\nthe model’s ability to reason. In domain specific RAG, RAFT consistently</DOCUMENT>\n<DOCUMENT>constructed after the training cutoff) is essential to many emerging applications (Vu et al.,\n2023; Lazaridou et al., 2022) and is the focus of this work.\nThis paper studies the following question – How do we adapt pre-trained LLMs for Retrieval\nAugmented Generation (RAG) in specialized domains?\nWhen it comes to adapting LLMs to specialized domains, we consider the following two\ncandidates: in-context learning through Retrieval-Augmented Generation (RAG) and super-</DOCUMENT>\n2. How does RAFT help improve its ability to reason?"
    },
    {
        "id": "seed_task_2_2",
        "question": "3. What style does RAFT's response follow?",
        "context": [
            "not have access to any additional documents or references to answer the questions during\n2",
            "language models. Journal of Machine Learning Research , 24(251):1–43, 2023. URL http:\n//jmlr.org/papers/v24/23-0037.html .\nJin, Q., Dhingra, B., Liu, Z., Cohen, W. W., and Lu, X. Pubmedqa: A dataset for biomedical\nresearch question answering. arXiv preprint arXiv:1909.06146 , 2019.\n10",
            "Preprint, Under Review\n0 20 40 60 80 100\nP % Golden Retrieved Context at Training0.250.300.350.400.45Final Accuracy\nT est Domain: NQ\n0 20 40 60 80 100\n% Golden Retrieved Context at Training0.500.550.600.65Final Accuracy\nT est Domain: TQA\n0 20 40 60 80 100\nP % Golden Retrieved Context at Training0.400.450.500.550.60Final Accuracy\nT est Domain: HoPo\nFigure 5: How many golden documents to involve? We study the hyperparameter P%",
            "to answer questions in \"open-book\" in-domain settings. In training RAFT,\ngiven a question, and a set of retrieved documents, we train the model to\nignore those documents that don’t help in answering the question, which\nwe call, distractor documents. RAFT accomplishes this by citing verbatim\nthe right sequence from the relevant document to help answer the question.\nThis coupled with RAFT’s chain-of-thought-style response helps improve\nthe model’s ability to reason. In domain specific RAG, RAFT consistently"
        ],
        "oracle_context": "to answer questions in \"open-book\" in-domain settings. In training RAFT,\ngiven a question, and a set of retrieved documents, we train the model to\nignore those documents that don’t help in answering the question, which\nwe call, distractor documents. RAFT accomplishes this by citing verbatim\nthe right sequence from the relevant document to help answer the question.\nThis coupled with RAFT’s chain-of-thought-style response helps improve\nthe model’s ability to reason. In domain specific RAG, RAFT consistently",
        "cot_answer": "To answer this question, I'll follow these steps:\n\n1. Understand the question: The question is asking what style RAFT's response follows.\n\n2. Find relevant information in the context: In the given context, it mentions that RAFT's chain-of-thought-style response helps improve the model's ability to reason.\n\n3. Provide step-by-step reasoning: ##begin_quote##This coupled with RAFT’s chain-of-thought-style response helps improve the model’s ability to reason.##end_quote##\n\n4. End with a final answer: Based on the context, it is clear that RAFT's response follows a **chain-of-thought-style**.\n\n<ANSWER>: Chain-of-thought-style",
        "instruction": "<DOCUMENT>not have access to any additional documents or references to answer the questions during\n2</DOCUMENT>\n<DOCUMENT>language models. Journal of Machine Learning Research , 24(251):1–43, 2023. URL http:\n//jmlr.org/papers/v24/23-0037.html .\nJin, Q., Dhingra, B., Liu, Z., Cohen, W. W., and Lu, X. Pubmedqa: A dataset for biomedical\nresearch question answering. arXiv preprint arXiv:1909.06146 , 2019.\n10</DOCUMENT>\n<DOCUMENT>Preprint, Under Review\n0 20 40 60 80 100\nP % Golden Retrieved Context at Training0.250.300.350.400.45Final Accuracy\nT est Domain: NQ\n0 20 40 60 80 100\n% Golden Retrieved Context at Training0.500.550.600.65Final Accuracy\nT est Domain: TQA\n0 20 40 60 80 100\nP % Golden Retrieved Context at Training0.400.450.500.550.60Final Accuracy\nT est Domain: HoPo\nFigure 5: How many golden documents to involve? We study the hyperparameter P%</DOCUMENT>\n<DOCUMENT>to answer questions in \"open-book\" in-domain settings. In training RAFT,\ngiven a question, and a set of retrieved documents, we train the model to\nignore those documents that don’t help in answering the question, which\nwe call, distractor documents. RAFT accomplishes this by citing verbatim\nthe right sequence from the relevant document to help answer the question.\nThis coupled with RAFT’s chain-of-thought-style response helps improve\nthe model’s ability to reason. In domain specific RAG, RAFT consistently</DOCUMENT>\n3. What style does RAFT's response follow?"
    },
    {
        "id": "seed_task_3_0",
        "question": "1. What kind of data are Large Language Models trained on?",
        "context": [
            "language models through few-shot prompting for open-domain question answering.\narXiv preprint arXiv:2203.05115 , 2022.\nLewis, P ., Perez, E., Piktus, A., Petroni, F., Karpukhin, V ., Goyal, N., Küttler, H., Lewis, M.,\nYih, W.-t., Rocktäschel, T., et al. Retrieval-augmented generation for knowledge-intensive\nnlp tasks. Advances in Neural Information Processing Systems , 33:9459–9474, 2020.\nLin, X. V ., Chen, X., Chen, M., Shi, W., Lomeli, M., James, R., Rodriguez, P ., Kahn, J., Szilvasy,",
            "Generalization to a variable number of test-time documents. We extended our research\nto examine the impact of different quantities of test-time documents on the model’s per-\nformance. Specifically, our experiments focused on assessing how models, trained with\nvarying numbers of distractor documents, respond to changes in the number of documents\npresented at test time. The results, illustrated in Fig. 6, confirm that the inclusion of distrac-",
            "relevant information. As an implementation detail, the ‘golden’ document doesn’t need to\nbe a single document, but can be more than one document, as is the case in HotpotQA Yang\net al. (2018). Then, for Pfraction of the questions ( qi) in the dataset, we retain the golden\ndocument ( d∗\ni) along with distractor documents ( dk−1). For (1−P)fraction of the questions\n(qi) in the dataset, we include no golden document and only include distractor documents",
            "version of general instruction tuning. Lastly, we provide an overview of the experiments to\nexpect in the later sections.\nSupervised Finetuning\nConsider the supervised fine-tuning (SFT) setting for a Question-Answer dataset. The\nformulation consists of the Dataset ( D) from which a set of Question ( Q) and corresponding\nanswer ( A) pairs are derived or already available. In the classical SFT setting, the model is\ntrained to improve it’s ability to answer the questions based on it’s knowledge - obtained"
        ],
        "oracle_context": "improves the model’s performance across PubMed, HotpotQA, and Gorilla\ndatasets, presenting a post-training recipe to improve pre-trained LLMs to\nin-domain RAG.\n1 Introduction\nTrained on vast quantities of public data, Large Language Models LLMs have achieved\nsignificant advances in a wide range of general knowledge reasoning tasks Brown et al.\n(2020); Wei et al. (2022). However, increasingly LLMs are being employed in specialized",
        "cot_answer": "Here's my step-by-step reasoning to answer the question:\n\n1. Identify the relevant information in the context that talks about the training data of Large Language Models (LLMs).\n2. Look for sentences or phrases that describe what kind of data LLMs are trained on.\n\nBased on the provided context, I find the following relevant sentence:\n##begin_quote##\nTrained on vast quantities of public data, Large Language Models LLMs have achieved significant advances in a wide range of general knowledge reasoning tasks Brown et al. (2020); Wei et al. (2022).\n##end_quote##\n\nFrom this sentence, we can infer that LLMs are trained on \"vast quantities of public data\".\n\nTherefore, the answer to the question is:\n\n<ANSWER>: Large Language Models are trained on vast quantities of public data.\n\nNote: The context also mentions specific datasets like PubMed, HotpotQA, and Gorilla, but these are not directly related to the training data of LLMs.",
        "instruction": "<DOCUMENT>language models through few-shot prompting for open-domain question answering.\narXiv preprint arXiv:2203.05115 , 2022.\nLewis, P ., Perez, E., Piktus, A., Petroni, F., Karpukhin, V ., Goyal, N., Küttler, H., Lewis, M.,\nYih, W.-t., Rocktäschel, T., et al. Retrieval-augmented generation for knowledge-intensive\nnlp tasks. Advances in Neural Information Processing Systems , 33:9459–9474, 2020.\nLin, X. V ., Chen, X., Chen, M., Shi, W., Lomeli, M., James, R., Rodriguez, P ., Kahn, J., Szilvasy,</DOCUMENT>\n<DOCUMENT>Generalization to a variable number of test-time documents. We extended our research\nto examine the impact of different quantities of test-time documents on the model’s per-\nformance. Specifically, our experiments focused on assessing how models, trained with\nvarying numbers of distractor documents, respond to changes in the number of documents\npresented at test time. The results, illustrated in Fig. 6, confirm that the inclusion of distrac-</DOCUMENT>\n<DOCUMENT>relevant information. As an implementation detail, the ‘golden’ document doesn’t need to\nbe a single document, but can be more than one document, as is the case in HotpotQA Yang\net al. (2018). Then, for Pfraction of the questions ( qi) in the dataset, we retain the golden\ndocument ( d∗\ni) along with distractor documents ( dk−1). For (1−P)fraction of the questions\n(qi) in the dataset, we include no golden document and only include distractor documents</DOCUMENT>\n<DOCUMENT>version of general instruction tuning. Lastly, we provide an overview of the experiments to\nexpect in the later sections.\nSupervised Finetuning\nConsider the supervised fine-tuning (SFT) setting for a Question-Answer dataset. The\nformulation consists of the Dataset ( D) from which a set of Question ( Q) and corresponding\nanswer ( A) pairs are derived or already available. In the classical SFT setting, the model is\ntrained to improve it’s ability to answer the questions based on it’s knowledge - obtained</DOCUMENT>\n1. What kind of data are Large Language Models trained on?"
    },
    {
        "id": "seed_task_3_1",
        "question": "2. Who have achieved significant advances in general knowledge reasoning tasks?",
        "context": [
            "Oberoi Group is a hotel company with its head office in Delhi.]\nInstruction: Given the question, context and answer above, provide a logical\nreasoning for that answer. Please use the format of: ##Reason: {reason}\n##Answer: {answer}.\n--------------------------------------------------------------------------------------\nCoT Answer: ##Reason: The document ##begin_quote## The Oberoi family is an\nIndian family that is famous for its involvement in hotels, namely through The",
            "the learning opportunity afforded by the fixed domain setting and early access to the test\ndocuments. Alternatively, supervised fine-tuning offers the opportunity to learn more\ngeneral patterns in the documents and better align to end tasks and user preferences Zhou\net al. (2023). However, existing fine-tuning based approaches either fail to leverage the\ndocuments at test time (don’t incorporate RAG) or fail to account for the imperfections in\nretrieval process during training.",
            "improves the model’s performance across PubMed, HotpotQA, and Gorilla\ndatasets, presenting a post-training recipe to improve pre-trained LLMs to\nin-domain RAG.\n1 Introduction\nTrained on vast quantities of public data, Large Language Models LLMs have achieved\nsignificant advances in a wide range of general knowledge reasoning tasks Brown et al.\n(2020); Wei et al. (2022). However, increasingly LLMs are being employed in specialized",
            "Preprint, Under Review\nShi, W., Min, S., Lomeli, M., Zhou, C., Li, M., Lin, V ., Smith, N. A., Zettlemoyer, L., Yih, S.,\nand Lewis, M. In-context pretraining: Language modeling beyond document boundaries.\narXiv preprint arXiv:2310.10638 , 2023c.\nShi, W., Min, S., Yasunaga, M., Seo, M., James, R., Lewis, M., Zettlemoyer, L., and Yih, W.-t.\nReplug: Retrieval-augmented black-box language models. arXiv preprint arXiv:2301.12652 ,\n2023d."
        ],
        "oracle_context": "improves the model’s performance across PubMed, HotpotQA, and Gorilla\ndatasets, presenting a post-training recipe to improve pre-trained LLMs to\nin-domain RAG.\n1 Introduction\nTrained on vast quantities of public data, Large Language Models LLMs have achieved\nsignificant advances in a wide range of general knowledge reasoning tasks Brown et al.\n(2020); Wei et al. (2022). However, increasingly LLMs are being employed in specialized",
        "cot_answer": "Here's my step-by-step reasoning to answer the question:\n\n1. Identify the relevant information in the context that answers the question \"Who have achieved significant advances in general knowledge reasoning tasks?\"\n2. Look for sentences or phrases in the context that mention achievements in general knowledge reasoning tasks.\n\nAccording to the context, the relevant sentence is: ##begin_quote##Trained on vast quantities of public data, Large Language Models LLMs have achieved significant advances in a wide range of general knowledge reasoning tasks Brown et al. (2020); Wei et al. (2022).##end_quote##\n\nFrom this sentence, we can infer that the Large Language Models (LLMs) have achieved significant advances in general knowledge reasoning tasks.\n\nTherefore, my final answer is:\n\n<ANSWER>: LLMs",
        "instruction": "<DOCUMENT>Oberoi Group is a hotel company with its head office in Delhi.]\nInstruction: Given the question, context and answer above, provide a logical\nreasoning for that answer. Please use the format of: ##Reason: {reason}\n##Answer: {answer}.\n--------------------------------------------------------------------------------------\nCoT Answer: ##Reason: The document ##begin_quote## The Oberoi family is an\nIndian family that is famous for its involvement in hotels, namely through The</DOCUMENT>\n<DOCUMENT>the learning opportunity afforded by the fixed domain setting and early access to the test\ndocuments. Alternatively, supervised fine-tuning offers the opportunity to learn more\ngeneral patterns in the documents and better align to end tasks and user preferences Zhou\net al. (2023). However, existing fine-tuning based approaches either fail to leverage the\ndocuments at test time (don’t incorporate RAG) or fail to account for the imperfections in\nretrieval process during training.</DOCUMENT>\n<DOCUMENT>improves the model’s performance across PubMed, HotpotQA, and Gorilla\ndatasets, presenting a post-training recipe to improve pre-trained LLMs to\nin-domain RAG.\n1 Introduction\nTrained on vast quantities of public data, Large Language Models LLMs have achieved\nsignificant advances in a wide range of general knowledge reasoning tasks Brown et al.\n(2020); Wei et al. (2022). However, increasingly LLMs are being employed in specialized</DOCUMENT>\n<DOCUMENT>Preprint, Under Review\nShi, W., Min, S., Lomeli, M., Zhou, C., Li, M., Lin, V ., Smith, N. A., Zettlemoyer, L., Yih, S.,\nand Lewis, M. In-context pretraining: Language modeling beyond document boundaries.\narXiv preprint arXiv:2310.10638 , 2023c.\nShi, W., Min, S., Yasunaga, M., Seo, M., James, R., Lewis, M., Zettlemoyer, L., and Yih, W.-t.\nReplug: Retrieval-augmented black-box language models. arXiv preprint arXiv:2301.12652 ,\n2023d.</DOCUMENT>\n2. Who have achieved significant advances in general knowledge reasoning tasks?"
    },
    {
        "id": "seed_task_3_2",
        "question": "3. Why are Large Language Models increasingly used in specialized domains?",
        "context": [
            "GPT-4-1106 to generate our Chain-of-Thought prompts and include an example of the\nprompt we used in Figure 3.\n4.3 Qualitative Analysis\nTo illustrate the potential advantages of RAFT over the domain-specifically fine-tuned\n(DSF) approach, we present a comparative example in Figure 4. This example qualitatively\n6",
            "demonstrates a scenario where the DSF model becomes confused by a question asking for\nthe identity of a screenwriter. Instead of providing the correct name, it mistakenly cites one\nof the films written by the screenwriter. In contrast, the RAFT model accurately answers the\nquestion. This discrepancy suggests that training a model solely with question-answer pairs\nmay impair its ability to derive relevant context from provided documents. The comparison",
            "domain RAG In RAFT, we prepare the training data such that each data point contains a\nquestion ( Q), a set of documents ( Dk), and a corresponding Chain-of-though style answer\n(A∗) generated from one of the document ( D∗). We differentiate between two types of\ndocuments: ‘golden’ documents ( D∗) i.e. the documents from which the answer to the\nquestion can be deduced, and ‘distractor’ documents ( Di) that do not contain answer-",
            "improves the model’s performance across PubMed, HotpotQA, and Gorilla\ndatasets, presenting a post-training recipe to improve pre-trained LLMs to\nin-domain RAG.\n1 Introduction\nTrained on vast quantities of public data, Large Language Models LLMs have achieved\nsignificant advances in a wide range of general knowledge reasoning tasks Brown et al.\n(2020); Wei et al. (2022). However, increasingly LLMs are being employed in specialized"
        ],
        "oracle_context": "improves the model’s performance across PubMed, HotpotQA, and Gorilla\ndatasets, presenting a post-training recipe to improve pre-trained LLMs to\nin-domain RAG.\n1 Introduction\nTrained on vast quantities of public data, Large Language Models LLMs have achieved\nsignificant advances in a wide range of general knowledge reasoning tasks Brown et al.\n(2020); Wei et al. (2022). However, increasingly LLMs are being employed in specialized",
        "cot_answer": "Here's my step-by-step reasoning to answer the question:\n\n1. Identify the key point that suggests why Large Language Models are increasingly used in specialized domains.\n\n##begin_quote##\nTrained on vast quantities of public data, Large Language Models LLMs have achieved significant advances in a wide range of general knowledge reasoning tasks Brown et al. (2020); Wei et al. (2022). However, increasingly LLMs are being employed in specialized\n##end_quote##\n\nThe key point is that LLMs are being employed in specialized domains.\n\n2. Identify the specific context that suggests why LLMs are used in these domains.\n\n##begin_quote##\nimproves the model’s performance across PubMed, HotpotQA, and Gorilla datasets, presenting a post-training recipe to improve pre-trained LLMs to in-domain RAG.\n##end_quote##\n\nThe specific context is that improving the model's performance across certain datasets (PubMed, HotpotQA, and Gorilla) allows for better performance in specialized domains.\n\n3. Draw a conclusion based on the reasoning above.\n\nBased on the key point and specific context, it can be concluded that Large Language Models are increasingly used in specialized domains because they can improve their performance across specific datasets, which enables them to perform well in those domains.\n\n<ANSWER>: Large Language Models are increasingly used in specialized domains because they can improve their performance across specific datasets.",
        "instruction": "<DOCUMENT>GPT-4-1106 to generate our Chain-of-Thought prompts and include an example of the\nprompt we used in Figure 3.\n4.3 Qualitative Analysis\nTo illustrate the potential advantages of RAFT over the domain-specifically fine-tuned\n(DSF) approach, we present a comparative example in Figure 4. This example qualitatively\n6</DOCUMENT>\n<DOCUMENT>demonstrates a scenario where the DSF model becomes confused by a question asking for\nthe identity of a screenwriter. Instead of providing the correct name, it mistakenly cites one\nof the films written by the screenwriter. In contrast, the RAFT model accurately answers the\nquestion. This discrepancy suggests that training a model solely with question-answer pairs\nmay impair its ability to derive relevant context from provided documents. The comparison</DOCUMENT>\n<DOCUMENT>domain RAG In RAFT, we prepare the training data such that each data point contains a\nquestion ( Q), a set of documents ( Dk), and a corresponding Chain-of-though style answer\n(A∗) generated from one of the document ( D∗). We differentiate between two types of\ndocuments: ‘golden’ documents ( D∗) i.e. the documents from which the answer to the\nquestion can be deduced, and ‘distractor’ documents ( Di) that do not contain answer-</DOCUMENT>\n<DOCUMENT>improves the model’s performance across PubMed, HotpotQA, and Gorilla\ndatasets, presenting a post-training recipe to improve pre-trained LLMs to\nin-domain RAG.\n1 Introduction\nTrained on vast quantities of public data, Large Language Models LLMs have achieved\nsignificant advances in a wide range of general knowledge reasoning tasks Brown et al.\n(2020); Wei et al. (2022). However, increasingly LLMs are being employed in specialized</DOCUMENT>\n3. Why are Large Language Models increasingly used in specialized domains?"
    },
    {
        "id": "seed_task_5_0",
        "question": "1. What are two candidates for adapting LLMs in specialized domains?",
        "context": [
            "constructed after the training cutoff) is essential to many emerging applications (Vu et al.,\n2023; Lazaridou et al., 2022) and is the focus of this work.\nThis paper studies the following question – How do we adapt pre-trained LLMs for Retrieval\nAugmented Generation (RAG) in specialized domains?\nWhen it comes to adapting LLMs to specialized domains, we consider the following two\ncandidates: in-context learning through Retrieval-Augmented Generation (RAG) and super-",
            "of the 52nd Annual ACM SIGACT Symposium on Theory of Computing , pp. 954–959, 2020.\nGuu, K., Lee, K., Tung, Z., Pasupat, P ., and Chang, M. Retrieval augmented language model\npre-training. In International conference on machine learning , pp. 3929–3938. PMLR, 2020.\nIzacard, G., Lewis, P ., Lomeli, M., Hosseini, L., Petroni, F., Schick, T., Dwivedi-Yu, J.,\nJoulin, A., Riedel, S., and Grave, E. Atlas: Few-shot learning with retrieval augmented",
            "domain RAG In RAFT, we prepare the training data such that each data point contains a\nquestion ( Q), a set of documents ( Dk), and a corresponding Chain-of-though style answer\n(A∗) generated from one of the document ( D∗). We differentiate between two types of\ndocuments: ‘golden’ documents ( D∗) i.e. the documents from which the answer to the\nquestion can be deduced, and ‘distractor’ documents ( Di) that do not contain answer-",
            "6 Related Works\nRetrieval-Augmented Language Models Retrieval-Augmented Language Models (RALMs)\nenhance LLMs by integrating a retrieval module that sources relevant information from\nexternal knowledge bases, significantly improving performance across various NLP tasks,\nincluding language modeling (Guu et al., 2020; Borgeaud et al., 2022; Khandelwal et al.,\n2019; Shi et al., 2023d; Lin et al., 2023b; Shi et al., 2023c; Asai et al., 2023; Xu et al., 2023;"
        ],
        "oracle_context": "constructed after the training cutoff) is essential to many emerging applications (Vu et al.,\n2023; Lazaridou et al., 2022) and is the focus of this work.\nThis paper studies the following question – How do we adapt pre-trained LLMs for Retrieval\nAugmented Generation (RAG) in specialized domains?\nWhen it comes to adapting LLMs to specialized domains, we consider the following two\ncandidates: in-context learning through Retrieval-Augmented Generation (RAG) and super-",
        "cot_answer": "Here's my step-by-step reasoning to answer the question:\n\n1. Identify the relevant information from the context that can help answer the question.\n\nFrom the context, I see that the paper is studying how to adapt pre-trained LLMs for Retrieval Augmented Generation (RAG) in specialized domains. The passage mentions two candidates for adapting LLMs in specialized domains.\n\n2. Extract the two candidates mentioned in the context as potential answers.\n\nThe context states: \"When it comes to adapting LLMs to specialized domains, we consider the following two candidates: in-context learning through Retrieval-Augmented Generation (RAG) and super-\"\n\n3. Provide the final answer based on the extracted information.\n\n<ANSWER>: The two candidates for adapting LLMs in specialized domains are in-context learning through Retrieval-Augmented Generation (RAG) and super-.",
        "instruction": "<DOCUMENT>constructed after the training cutoff) is essential to many emerging applications (Vu et al.,\n2023; Lazaridou et al., 2022) and is the focus of this work.\nThis paper studies the following question – How do we adapt pre-trained LLMs for Retrieval\nAugmented Generation (RAG) in specialized domains?\nWhen it comes to adapting LLMs to specialized domains, we consider the following two\ncandidates: in-context learning through Retrieval-Augmented Generation (RAG) and super-</DOCUMENT>\n<DOCUMENT>of the 52nd Annual ACM SIGACT Symposium on Theory of Computing , pp. 954–959, 2020.\nGuu, K., Lee, K., Tung, Z., Pasupat, P ., and Chang, M. Retrieval augmented language model\npre-training. In International conference on machine learning , pp. 3929–3938. PMLR, 2020.\nIzacard, G., Lewis, P ., Lomeli, M., Hosseini, L., Petroni, F., Schick, T., Dwivedi-Yu, J.,\nJoulin, A., Riedel, S., and Grave, E. Atlas: Few-shot learning with retrieval augmented</DOCUMENT>\n<DOCUMENT>domain RAG In RAFT, we prepare the training data such that each data point contains a\nquestion ( Q), a set of documents ( Dk), and a corresponding Chain-of-though style answer\n(A∗) generated from one of the document ( D∗). We differentiate between two types of\ndocuments: ‘golden’ documents ( D∗) i.e. the documents from which the answer to the\nquestion can be deduced, and ‘distractor’ documents ( Di) that do not contain answer-</DOCUMENT>\n<DOCUMENT>6 Related Works\nRetrieval-Augmented Language Models Retrieval-Augmented Language Models (RALMs)\nenhance LLMs by integrating a retrieval module that sources relevant information from\nexternal knowledge bases, significantly improving performance across various NLP tasks,\nincluding language modeling (Guu et al., 2020; Borgeaud et al., 2022; Khandelwal et al.,\n2019; Shi et al., 2023d; Lin et al., 2023b; Shi et al., 2023c; Asai et al., 2023; Xu et al., 2023;</DOCUMENT>\n1. What are two candidates for adapting LLMs in specialized domains?"
    },
    {
        "id": "seed_task_5_1",
        "question": "2. What is Retrieval-Augmented Generation (RAG) used for?",
        "context": [
            "contrasting them with the correct reasoning and answers. The LLM is prompted to identify\nerrors in its reasoning and extract key insights for improvement. This figure specifically\nrepresents the ‘GenerateExplanation‘ step in the RAFT algorithm (Section 3).\nTable 1: RAFT improves RAG performance for all specialized domains : Across PubMed,\nHotPot, HuggingFace, Torch Hub, and Tensorflow Hub, we see that Domain-specific Fine-",
            "constructed after the training cutoff) is essential to many emerging applications (Vu et al.,\n2023; Lazaridou et al., 2022) and is the focus of this work.\nThis paper studies the following question – How do we adapt pre-trained LLMs for Retrieval\nAugmented Generation (RAG) in specialized domains?\nWhen it comes to adapting LLMs to specialized domains, we consider the following two\ncandidates: in-context learning through Retrieval-Augmented Generation (RAG) and super-",
            "dataset tailored only for biomedical-research question-answering. It mainly focuses on\nanswering medical and biology questions based on a given set of documents. We would\n5",
            "Shi, W., Ajith, A., Xia, M., Huang, Y., Liu, D., Blevins, T., Chen, D., and Zettlemoyer, L.\nDetecting pretraining data from large language models. arXiv preprint arXiv:2310.16789 ,\n2023b.\n11"
        ],
        "oracle_context": "constructed after the training cutoff) is essential to many emerging applications (Vu et al.,\n2023; Lazaridou et al., 2022) and is the focus of this work.\nThis paper studies the following question – How do we adapt pre-trained LLMs for Retrieval\nAugmented Generation (RAG) in specialized domains?\nWhen it comes to adapting LLMs to specialized domains, we consider the following two\ncandidates: in-context learning through Retrieval-Augmented Generation (RAG) and super-",
        "cot_answer": "Here's my step-by-step reasoning to answer the question:\n\n1. Identify the relevant information in the context that relates to Retrieval-Augmented Generation (RAG).\n2. Understand what RAG is used for based on the provided context.\n\nReasoning:\n##begin_quote##constructed after the training cutoff) is essential to many emerging applications (Vu et al., 2023; Lazaridou et al., 2022) and is the focus of this work.##end_quote##\nThis sentence suggests that RAG is used for emerging applications, which are not specified in detail.\n\nHowever, we can infer more information from the context:\n##begin_quote##When it comes to adapting LLMs to specialized domains, we consider the following two candidates: in-context learning through Retrieval-Augmented Generation (RAG) and super-##end_quote##\nThis sentence implies that RAG is used for adapting pre-trained language models (LLMs) to specialized domains.\n\nCombining this information, I can conclude that RAG is used for:\n\n<ANSWER>: Retrieval-Augmented Generation (RAG) is used for adapting pre-trained language models (LLMs) to specialized domains and emerging applications.",
        "instruction": "<DOCUMENT>contrasting them with the correct reasoning and answers. The LLM is prompted to identify\nerrors in its reasoning and extract key insights for improvement. This figure specifically\nrepresents the ‘GenerateExplanation‘ step in the RAFT algorithm (Section 3).\nTable 1: RAFT improves RAG performance for all specialized domains : Across PubMed,\nHotPot, HuggingFace, Torch Hub, and Tensorflow Hub, we see that Domain-specific Fine-</DOCUMENT>\n<DOCUMENT>constructed after the training cutoff) is essential to many emerging applications (Vu et al.,\n2023; Lazaridou et al., 2022) and is the focus of this work.\nThis paper studies the following question – How do we adapt pre-trained LLMs for Retrieval\nAugmented Generation (RAG) in specialized domains?\nWhen it comes to adapting LLMs to specialized domains, we consider the following two\ncandidates: in-context learning through Retrieval-Augmented Generation (RAG) and super-</DOCUMENT>\n<DOCUMENT>dataset tailored only for biomedical-research question-answering. It mainly focuses on\nanswering medical and biology questions based on a given set of documents. We would\n5</DOCUMENT>\n<DOCUMENT>Shi, W., Ajith, A., Xia, M., Huang, Y., Liu, D., Blevins, T., Chen, D., and Zettlemoyer, L.\nDetecting pretraining data from large language models. arXiv preprint arXiv:2310.16789 ,\n2023b.\n11</DOCUMENT>\n2. What is Retrieval-Augmented Generation (RAG) used for?"
    },
    {
        "id": "seed_task_5_2",
        "question": "3. Why is pre-trained LLM adaptation important?",
        "context": [
            "Preprint, Under Review\nTable 2: Ablation on Chain-of-Thought : The numbers of RAFT and RAFT without\nCoT. Results on various datasets show that adding CoT can significantly improve the\nperformance of the finetuned model. With a gains of 9.66% and 14.93% in the Hotpot QA\nand HuggingFace datasets respectively.\nPubMed HotpotQA HuggingFace Torch Hub TensorFlow\nRAFT w.o CoT 68.30 25.62 59.07 86.56 83.21\nRAFT 73.30 35.28 74.00 84.95 86.86",
            "Zhou, C., Liu, P ., Xu, P ., Iyer, S., Sun, J., Mao, Y., Ma, X., Efrat, A., Yu, P ., Yu, L., et al. Lima:\nLess is more for alignment. arXiv preprint arXiv:2305.11206 , 2023.\n12",
            "ization through memorization: Nearest neighbor language models. arXiv preprint\narXiv:1911.00172 , 2019.\nKwiatkowski, T., Palomaki, J., Redfield, O., Collins, M., Parikh, A., Alberti, C., Epstein, D.,\nPolosukhin, I., Devlin, J., Lee, K., et al. Natural questions: a benchmark for question\nanswering research. Transactions of the Association for Computational Linguistics , 7:453–466,\n2019.\nLazaridou, A., Gribovskaya, E., Stokowiec, W., and Grigorev, N. Internet-augmented",
            "Torch Hub evaluation. Compared with DSF on the specific dataset, our model does better at\nrelying on the provided context to solve the problem. RAFT does much better on the tasks\nlike Hotpot and HuggingFace datasets (30.87% on Hotpot and 31.41% on HuggingFace).\nNote that for PubMed QA, since it is a binary yes/no question, we don’t observe significant\ngains when we compare our model with DSF + RAG. Even compared with a much larger\nand better model GPT-3.5, RAFT demonstrates significant advantages."
        ],
        "oracle_context": "constructed after the training cutoff) is essential to many emerging applications (Vu et al.,\n2023; Lazaridou et al., 2022) and is the focus of this work.\nThis paper studies the following question – How do we adapt pre-trained LLMs for Retrieval\nAugmented Generation (RAG) in specialized domains?\nWhen it comes to adapting LLMs to specialized domains, we consider the following two\ncandidates: in-context learning through Retrieval-Augmented Generation (RAG) and super-",
        "cot_answer": "To answer this question, I will follow these steps:\n\n1. Identify the relevant information from the context that is related to pre-trained LLM adaptation.\n2. Analyze the provided context and extract the key points that explain why pre-trained LLM adaptation is important.\n\nStep 1: Relevant information identification\nThe relevant information that stands out in the context is:\n##begin_quote##\nconstructed after the training cutoff) is essential to many emerging applications (Vu et al., 2023; Lazaridou et al., 2022) and is the focus of this work.\n##end_quote##\n\nStep 2: Analysis and extraction of key points\nFrom the provided context, we can extract the following key points:\n\n* Pre-trained LLM adaptation is essential to many emerging applications.\n* The focus of this work is on adapting pre-trained LLMs for Retrieval Augmented Generation (RAG) in specialized domains.\n\nNow, let's connect these dots to answer the question:\n\nPre-trained LLM adaptation is important because it allows us to adapt large language models to specific domains and tasks, which is essential for many emerging applications. This adaptation enables the model to learn from new data and fine-tune its performance on a particular task or domain.\n\n<ANSWER>: Pre-trained LLM adaptation is important because it enables the model to learn from new data and fine-tune its performance on a specific task or domain, which is essential for many emerging applications.",
        "instruction": "<DOCUMENT>Preprint, Under Review\nTable 2: Ablation on Chain-of-Thought : The numbers of RAFT and RAFT without\nCoT. Results on various datasets show that adding CoT can significantly improve the\nperformance of the finetuned model. With a gains of 9.66% and 14.93% in the Hotpot QA\nand HuggingFace datasets respectively.\nPubMed HotpotQA HuggingFace Torch Hub TensorFlow\nRAFT w.o CoT 68.30 25.62 59.07 86.56 83.21\nRAFT 73.30 35.28 74.00 84.95 86.86</DOCUMENT>\n<DOCUMENT>Zhou, C., Liu, P ., Xu, P ., Iyer, S., Sun, J., Mao, Y., Ma, X., Efrat, A., Yu, P ., Yu, L., et al. Lima:\nLess is more for alignment. arXiv preprint arXiv:2305.11206 , 2023.\n12</DOCUMENT>\n<DOCUMENT>ization through memorization: Nearest neighbor language models. arXiv preprint\narXiv:1911.00172 , 2019.\nKwiatkowski, T., Palomaki, J., Redfield, O., Collins, M., Parikh, A., Alberti, C., Epstein, D.,\nPolosukhin, I., Devlin, J., Lee, K., et al. Natural questions: a benchmark for question\nanswering research. Transactions of the Association for Computational Linguistics , 7:453–466,\n2019.\nLazaridou, A., Gribovskaya, E., Stokowiec, W., and Grigorev, N. Internet-augmented</DOCUMENT>\n<DOCUMENT>Torch Hub evaluation. Compared with DSF on the specific dataset, our model does better at\nrelying on the provided context to solve the problem. RAFT does much better on the tasks\nlike Hotpot and HuggingFace datasets (30.87% on Hotpot and 31.41% on HuggingFace).\nNote that for PubMed QA, since it is a binary yes/no question, we don’t observe significant\ngains when we compare our model with DSF + RAG. Even compared with a much larger\nand better model GPT-3.5, RAFT demonstrates significant advantages.</DOCUMENT>\n3. Why is pre-trained LLM adaptation important?"
    },
    {
        "id": "seed_task_6_0",
        "question": "1. What is RAG?",
        "context": [
            "DSF + RAG 71.6 4.41 42.59 82.80 60.29\nRAFT (LLaMA2-7B) 73.30 35.28 74.00 84.95 86.86\nDatasets In our experiments, we use the following datasets to evaluate our model and\nall baselines. We selected these datasets to represent both popular and diverse domains\nincluding Wikipedia, Coding/API documents, and question-answering on medical docu-\nments. Natural Questions (NQ) Kwiatkowski et al. (2019), Trivia QA Joshi et al. (2017) and",
            "2023d.\nTänzer, M., Ruder, S., and Rei, M. Memorisation versus generalisation in pre-trained lan-\nguage models. In Proceedings of the 60th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pp. 7564–7578, 2022.\nVu, T., Iyyer, M., Wang, X., Constant, N., Wei, J., Wei, J., Tar, C., Sung, Y.-H., Zhou, D., Le,\nQ., et al. Freshllms: Refreshing large language models with search engine augmentation.\narXiv preprint arXiv:2310.03214 , 2023.",
            "Carlini, N., Ippolito, D., Jagielski, M., Lee, K., Tramer, F., and Zhang, C. Quantifying\nmemorization across neural language models. In The Eleventh International Conference on\nLearning Representations , 2022.\nDernoncourt, F. and Lee, J. Y. Pubmed 200k rct: a dataset for sequential sentence classification\nin medical abstracts. arXiv preprint arXiv:1710.06071 , 2017.\nFeldman, V . Does learning require memorization? a short tale about a long tail. In Proceedings",
            "inferior performance compared to configurations that include a greater number of distractor\ndocuments. As we can see in the figure, the better performance for Natural Questions is\n8"
        ],
        "oracle_context": "vised fine-tuning. RAG based methods allow the LLM to reference the documents when\n∗Corresponding author, personal website: tianjunz.github.io\n1arXiv:2403.10131v2  [cs.CL]  5 Jun 2024",
        "cot_answer": "To answer this question, I will follow these steps:\n\n1. Read the provided context to understand the topic and relevant information.\n2. Identify the key phrases or sentences that might provide a clue about what RAG refers to.\n\nBased on the given context, I can see that it mentions \"RAG based methods\" in the sentence: \"RAG based methods allow the LLM to reference the documents when...\"\n\nThis suggests that RAG is related to a method or approach used for fine-tuning language models (LLM). The phrase \"reference the documents\" implies that RAG might be connected to accessing or utilizing external information, such as documents.\n\n##begin_quote##\nRAG based methods allow the LLM to reference the documents when ∗Corresponding author, personal website: tianjunz.github.io\n1arXiv:2403.10131v2  [cs.CL]  5 Jun 2024\n##end_quote##\n\nFrom this information, I can infer that RAG is likely an abbreviation or acronym referring to a specific technique or framework used for fine-tuning language models.\n\n<ANSWER>: $RAG refers to a method or approach used for fine-tuning language models (LLM) that allows them to reference external documents.",
        "instruction": "<DOCUMENT>DSF + RAG 71.6 4.41 42.59 82.80 60.29\nRAFT (LLaMA2-7B) 73.30 35.28 74.00 84.95 86.86\nDatasets In our experiments, we use the following datasets to evaluate our model and\nall baselines. We selected these datasets to represent both popular and diverse domains\nincluding Wikipedia, Coding/API documents, and question-answering on medical docu-\nments. Natural Questions (NQ) Kwiatkowski et al. (2019), Trivia QA Joshi et al. (2017) and</DOCUMENT>\n<DOCUMENT>2023d.\nTänzer, M., Ruder, S., and Rei, M. Memorisation versus generalisation in pre-trained lan-\nguage models. In Proceedings of the 60th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pp. 7564–7578, 2022.\nVu, T., Iyyer, M., Wang, X., Constant, N., Wei, J., Wei, J., Tar, C., Sung, Y.-H., Zhou, D., Le,\nQ., et al. Freshllms: Refreshing large language models with search engine augmentation.\narXiv preprint arXiv:2310.03214 , 2023.</DOCUMENT>\n<DOCUMENT>Carlini, N., Ippolito, D., Jagielski, M., Lee, K., Tramer, F., and Zhang, C. Quantifying\nmemorization across neural language models. In The Eleventh International Conference on\nLearning Representations , 2022.\nDernoncourt, F. and Lee, J. Y. Pubmed 200k rct: a dataset for sequential sentence classification\nin medical abstracts. arXiv preprint arXiv:1710.06071 , 2017.\nFeldman, V . Does learning require memorization? a short tale about a long tail. In Proceedings</DOCUMENT>\n<DOCUMENT>inferior performance compared to configurations that include a greater number of distractor\ndocuments. As we can see in the figure, the better performance for Natural Questions is\n8</DOCUMENT>\n1. What is RAG?"
    },
    {
        "id": "seed_task_6_1",
        "question": "2. How does LLM reference documents?",
        "context": [
            "Ram, O., Levine, Y., Dalmedigos, I., Muhlgay, D., Shashua, A., Leyton-Brown, K.,\nand Shoham, Y. In-context retrieval-augmented language models. arXiv preprint\narXiv:2302.00083 , 2023.\nShi, F., Chen, X., Misra, K., Scales, N., Dohan, D., Chi, E. H., Schärli, N., and Zhou, D. Large\nlanguage models can be easily distracted by irrelevant context. In International Conference\non Machine Learning , pp. 31210–31227. PMLR, 2023a.",
            "Chain-of-thought prompting elicits reasoning in large language models. Advances in\nNeural Information Processing Systems , 35:24824–24837, 2022.\nWeston, J. and Sukhbaatar, S. System 2 attention (is something you might need too). arXiv\npreprint arXiv:2311.11829 , 2023.\nXiong, W., Liu, J., Molybog, I., Zhang, H., Bhargava, P ., Hou, R., Martin, L., Rungta, R.,\nSankararaman, K. A., Oguz, B., et al. Effective long-context scaling of foundation models.\narXiv preprint arXiv:2309.16039 , 2023.",
            "vised fine-tuning. RAG based methods allow the LLM to reference the documents when\n∗Corresponding author, personal website: tianjunz.github.io\n1arXiv:2403.10131v2  [cs.CL]  5 Jun 2024",
            "For all the datasets in our experiments, we generate the answers using the technique\ndescribed above. Note that the Gorilla APIBench dataset, already includes reasoning\nin the answers. We provide an example of the generation step in Fig. 3, the detailed\nreasoning answer includes a citation from the original context inside ##begin_quote## and\n##end_quote## as well as the detailed explanation on how to reach the conclusion based on"
        ],
        "oracle_context": "vised fine-tuning. RAG based methods allow the LLM to reference the documents when\n∗Corresponding author, personal website: tianjunz.github.io\n1arXiv:2403.10131v2  [cs.CL]  5 Jun 2024",
        "cot_answer": "To answer this question, I will follow these steps:\n\n1. Identify the relevant information from the context that mentions how LLM references documents.\n\n##begin_quote##\nRAG based methods allow the LLM to reference the documents when\n##end_quote##\n\n2. Analyze the quote and extract the key point about how LLM references documents.\n\nFrom the quote, I can infer that RAG-based methods enable the LLM to reference documents.\n\n3. Provide a step-by-step explanation of how LLM references documents based on the analysis.\n\nRAG-based methods allow the LLM to reference documents by using these methods. The process involves [insert steps here].\n\n4. Conclude with the final answer in the form \"<ANSWER>: $answer\".\n\n<ANSWER>: RAG-based methods enable the LLM to reference documents.",
        "instruction": "<DOCUMENT>Ram, O., Levine, Y., Dalmedigos, I., Muhlgay, D., Shashua, A., Leyton-Brown, K.,\nand Shoham, Y. In-context retrieval-augmented language models. arXiv preprint\narXiv:2302.00083 , 2023.\nShi, F., Chen, X., Misra, K., Scales, N., Dohan, D., Chi, E. H., Schärli, N., and Zhou, D. Large\nlanguage models can be easily distracted by irrelevant context. In International Conference\non Machine Learning , pp. 31210–31227. PMLR, 2023a.</DOCUMENT>\n<DOCUMENT>Chain-of-thought prompting elicits reasoning in large language models. Advances in\nNeural Information Processing Systems , 35:24824–24837, 2022.\nWeston, J. and Sukhbaatar, S. System 2 attention (is something you might need too). arXiv\npreprint arXiv:2311.11829 , 2023.\nXiong, W., Liu, J., Molybog, I., Zhang, H., Bhargava, P ., Hou, R., Martin, L., Rungta, R.,\nSankararaman, K. A., Oguz, B., et al. Effective long-context scaling of foundation models.\narXiv preprint arXiv:2309.16039 , 2023.</DOCUMENT>\n<DOCUMENT>vised fine-tuning. RAG based methods allow the LLM to reference the documents when\n∗Corresponding author, personal website: tianjunz.github.io\n1arXiv:2403.10131v2  [cs.CL]  5 Jun 2024</DOCUMENT>\n<DOCUMENT>For all the datasets in our experiments, we generate the answers using the technique\ndescribed above. Note that the Gorilla APIBench dataset, already includes reasoning\nin the answers. We provide an example of the generation step in Fig. 3, the detailed\nreasoning answer includes a citation from the original context inside ##begin_quote## and\n##end_quote## as well as the detailed explanation on how to reach the conclusion based on</DOCUMENT>\n2. How does LLM reference documents?"
    },
    {
        "id": "seed_task_6_2",
        "question": "3. Who is the corresponding author?",
        "context": [
            "to a question may not always be adequate. This approach can lead to a rapid decrease\nin loss, resulting in the model beginning to overfit. Incorporating a reasoning chain that\nnot only guides the model to the answer but also enriches the model’s understanding can\nimprove the overall accuracy and prevent overfitting to concise answers. In our experiments,\nintegrating the Chain-of-Thought significantly enhances training robustness. We employ",
            "not have access to any additional documents or references to answer the questions during\n2",
            "Preprint, Under Review\nbe used at test-time with Retrieval Augmented Generation (RAG) setting, where additional\ndocuments can be introduced in the prompt to help the model answer the question. This\ncan be represented as follows:\n{Train: Q→A}, {0-shot Inference: Q→A}, {RAG Inference: Q+D→A}\nRAFT: Retrieval Augmented Fine-Tuning (RAFT), presents a novel recipe to prepare fine-\ntuning data to tailor the models for domain-specific open-book setting, equivalent to in-",
            "(P = 100%). However, our findings challenge this assumption: incorporating a portion of\nthe training data without the golden document in the context (P = 80%) appears to enhance\nthe model’s performance on RAG tasks.\n7"
        ],
        "oracle_context": "vised fine-tuning. RAG based methods allow the LLM to reference the documents when\n∗Corresponding author, personal website: tianjunz.github.io\n1arXiv:2403.10131v2  [cs.CL]  5 Jun 2024",
        "cot_answer": "Here's my step-by-step reasoning to answer the question:\n\n1. Identify the relevant information: The context mentions a personal website and an arXiv paper.\n2. Look for keywords related to authorship: The phrase \"Corresponding author\" is mentioned, which suggests that this person is the author of the paper.\n\n##begin_quote##\n∗Corresponding author, personal website: tianjunz.github.io\n1arXiv:2403.10131v2  [cs.CL]  5 Jun 2024\n##end_quote##\n\nBased on the context, I can see that the phrase \"Corresponding author\" is followed by a mention of a personal website, which suggests that this person is the corresponding author.\n\nTherefore, my answer is:\n\n<ANSWER>: Tianjun Zhang",
        "instruction": "<DOCUMENT>to a question may not always be adequate. This approach can lead to a rapid decrease\nin loss, resulting in the model beginning to overfit. Incorporating a reasoning chain that\nnot only guides the model to the answer but also enriches the model’s understanding can\nimprove the overall accuracy and prevent overfitting to concise answers. In our experiments,\nintegrating the Chain-of-Thought significantly enhances training robustness. We employ</DOCUMENT>\n<DOCUMENT>not have access to any additional documents or references to answer the questions during\n2</DOCUMENT>\n<DOCUMENT>Preprint, Under Review\nbe used at test-time with Retrieval Augmented Generation (RAG) setting, where additional\ndocuments can be introduced in the prompt to help the model answer the question. This\ncan be represented as follows:\n{Train: Q→A}, {0-shot Inference: Q→A}, {RAG Inference: Q+D→A}\nRAFT: Retrieval Augmented Fine-Tuning (RAFT), presents a novel recipe to prepare fine-\ntuning data to tailor the models for domain-specific open-book setting, equivalent to in-</DOCUMENT>\n<DOCUMENT>(P = 100%). However, our findings challenge this assumption: incorporating a portion of\nthe training data without the golden document in the context (P = 80%) appears to enhance\nthe model’s performance on RAG tasks.\n7</DOCUMENT>\n3. Who is the corresponding author?"
    }
]