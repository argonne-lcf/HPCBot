{
    "/home/btrungvo/workspace/HPCBot/data/pdf/RAG_Foundry/2408.02545v1.pdf": [
        {
            "id": "chunk_0_question_0",
            "context": [
                "RAG Foundry: A Framework for Enhancing LLMs for Retrieval",
                "Augmented Generation",
                "Daniel Fleischer Moshe Berchansky Moshe Wasserblat Peter Izsak",
                "Intel Labs",
                "{daniel.fleischer, moshe.berchansky, moshe.wasserblat, peter.izsak}@intel.com",
                "Abstract",
                "Implementing Retrieval-Augmented Genera-",
                "tion (RAG) systems is inherently complex,",
                "requiring deep understanding of data, use",
                "cases, and intricate design decisions. Addi-",
                "tionally, evaluating these systems presents sig-",
                "nificant challenges, necessitating assessment of",
                "both retrieval accuracy and generative quality",
                "through a multi-faceted approach. We intro-",
                "duce RAG F OUNDRY , an open-source frame-",
                "work for augmenting large language models",
                "for RAG use cases. RAG F OUNDRY inte-",
                "grates data creation, training, inference and",
                "evaluation into a single workflow, facilitating",
                "the creation of data-augmented datasets for",
                "training and evaluating large language mod-",
                "els in RAG settings. This integration en-",
                "ables rapid prototyping and experimentation",
                "with various RAG techniques, allowing users",
                "to easily generate datasets and train RAG",
                "models using internal or specialized knowl-",
                "edge sources. We demonstrate the frame-",
                "work effectiveness by augmenting and fine-",
                "tuning Llama-3 and Phi-3 models with diverse",
                "RAG configurations, showcasing consistent im-",
                "provements across three knowledge-intensive",
                "datasets. Code is released as open-source in",
                "https://github.com/IntelLabs/RAGFoundry .",
                "1 Introduction",
                "Large Language Models (LLMs) have emerged as",
                "a transformative force in the field of AI, demon-",
                "strating an impressive ability to perform a wide",
                "range of tasks that traditionally required human in-",
                "telligence (Brown et al., 2020; Kojima et al., 2022).",
                "Despite their impressive capabilities, LLMs have",
                "inherent limitations. These models can produce",
                "plausible-sounding but incorrect or nonsensical an-",
                "swers, struggle with factual accuracy, lack access",
                "to up-to-date information after their training cutoff",
                "and struggle in attending to relevant information in"
            ],
            "question": "What is RAG?",
            "correct_answer": "Retrieval-Augmented Generation.",
            "incorrect_answers": [
                "RAG stands for \"Random Algorithm Generator\".",
                "RAG is a type of pizza topping.",
                "RAG is an acronym for \"Really Amazing Guys\".",
                "RAG is the Roman name for the planet Jupiter."
            ]
        },
        {
            "id": "chunk_1_question_0",
            "context": [
                "and struggle in attending to relevant information in",
                "large contexts (Huang et al., 2023; Liu et al., 2023).",
                "Data",
                "TrainingLoRA",
                "Inference",
                "LoadersAugmentation",
                "SelectorsRetrieversSamplersPromptersCachingAPI",
                "EvaluationEMF1FaithfulnessRelevancyAnswer ProcessorROUGE",
                "Figure 1: An overview of the RAG F OUNDRY frame-",
                "work: the Data Augmentation module persists RAG",
                "interactions into a dedicated dataset, which is then used",
                "for training, inference and evaluation.",
                "Retrieval-Augmented Generation (RAG) enhances",
                "LLMs performance by integrating external infor-",
                "mation using retrieval mechanisms. Combining re-",
                "trieval that leverages vast knowledge-bases outside",
                "theknowledge of the model, effectively addresses",
                "knowledge limitations, can reduce hallucinations,",
                "improve the relevance of generated content, pro-",
                "vide interpretability and could be vastly more cost-",
                "efficient (Lewis et al., 2021; Mallen et al., 2022;",
                "Gao et al., 2023; Asai et al., 2023; Borgeaud et al.,",
                "2021; Peng et al., 2023; de Jong et al., 2023). Fur-",
                "thermore, recent research indicates that fine-tuning",
                "LLMs for RAG can achieve state-of-the-art perfor-",
                "mance, surpassing that of larger, proprietary mod-",
                "els (Yu et al., 2024b; Liu et al., 2024).",
                "However, the implementation of RAG systems",
                "is inherently complex and requires a series of",
                "intricate decisions that can significantly impact",
                "the performance of the system. This process de-arXiv:2408.02545v1  [cs.CL]  5 Aug 2024"
            ],
            "question": "How does RAG enhance performance in large language models?",
            "correct_answer": "RAG enhances performance in large language models by integrating external information using retrieval mechanisms.",
            "incorrect_answers": [
                "RAG enhances performance by adding a virtual reality component to the model, allowing it to \"see\" and interact with external information.",
                "It works by compressing the entire internet into a single byte of data that's stored in the model's memory.",
                "The key benefit is that it allows large language models to generate text that sounds exactly like Shakespearean sonnets.",
                "RAG actually reduces performance by forcing the model to recite the alphabet backwards before generating any output."
            ]
        },
        {
            "id": "chunk_2_question_0",
            "context": [
                "mands a thorough understanding of the data and",
                "use case, and often, solutions do not generalize",
                "well to other domains (Barnett et al., 2024; Bala-",
                "guer et al., 2024). Some key RAG design decisions",
                "include text embedding, indexing parameters, re-",
                "trieval algorithms, query building, and prompt de-",
                "sign, among other considerations beyond the LLM",
                "configuration (Wang et al., 2024). Another issue is",
                "reproducibility: achieving consistent and compara-",
                "ble results across runs, datasets and tasks. Varia-",
                "tions in training data, pre-processing steps, model",
                "configurations, and hardware can lead to discrep-",
                "ancies in performance, making it challenging for",
                "researchers and practitioners to replicate findings",
                "and build upon previous work. Additionally, evalu-",
                "ating RAG systems presents a challenge due to the",
                "dual reliance on retrieval accuracy and generative",
                "quality. These systems require a sophisticated eval-",
                "uation suite that accounts for the interplay among",
                "the retrieved information, the formalization of data,",
                "and the generated output (Chen et al., 2023; Yu",
                "et al., 2024a; Es et al., 2024).",
                "We introduce RAG F OUNDRY , an open-source",
                "python framework for developing sophisticated",
                "retrieval-augmented LLMs for RAG use-cases. The",
                "library supports researchers and practitioners in the",
                "nuanced task of enhancing the capabilities of LLMs",
                "in RAG use cases. It is highly customizable, fa-",
                "cilitating rapid prototyping and experimentation",
                "across all aspects of RAG, including data selec-",
                "tion, aggregation and filtering, retrieval, text pro-",
                "cessing, document ranking, few-shot generation,",
                "prompt design using templates, fine-tuning, infer-",
                "ence, and evaluation. To cater to the specific needs",
                "of researchers, we designed the framework to func-",
                "tion as an end-to-end experimentation environment.",
                "The backbone of the library consists of four dis-",
                "tinct modules: data creation, training, inference,",
                "and evaluation. Each module is encapsulated and",
                "controlled by a configuration file, ensuring compat-"
            ],
            "question": "What are some key design decisions in building Retrieval-Augmented Generators?",
            "correct_answer": "Text embedding, indexing parameters, retrieval algorithms, query building, prompt design, and other considerations beyond LLM configuration are some key design decisions in building Retrieval-Augmented Generators.",
            "incorrect_answers": [
                "**Key design decisions**: To build Retrieval-Augmented Generators, you need to first decide on the color scheme of your office.",
                "**Design considerations**: The most important thing in RAG is the type of coffee beans used by the developers.",
                "**RAG F OUNDRY framework**: This framework is actually a recipe book for making pancakes, not a library for developing retrieval-augmented LLMs.",
                "**Key modules**: The four key modules are \"Breakfast\", \"Lunch\", \"Dinner\", and \"Snacks\" - these are the essential components of a RAG system."
            ]
        },
        {
            "id": "chunk_3_question_0",
            "context": [
                "controlled by a configuration file, ensuring compat-",
                "ibility between the output of one module and the",
                "input of the next. This modular approach allows",
                "each step to be isolated and independently experi-",
                "mented with, enabling the production of multiple",
                "outputs and the concurrent execution of numerous",
                "experiments. Evaluation can be conducted on the",
                "generated outputs as well as on any feature within",
                "the data, including retrieval, ranking, and reason-",
                "ing.",
                "To illustrate the utility of the framework, we",
                "conducted experiments involving retrieval, fine-",
                "tuning, chain-of-thought (CoT) reasoning (Wuet al., 2023) and a negative distractor-documents",
                "technique (Zhang et al., 2024). We compared",
                "two widely accepted baseline models using vari-",
                "ous enhancement methods across three knowledge-",
                "intensive question-answering tasks, demonstrating",
                "the effectiveness of RAG F OUNDRY .",
                "2 Related Work",
                "There are numerous open-source tools related to",
                "the different aspects of RAG, namely inference,",
                "training and evaluation. LlamaIndex (Liu, 2022),",
                "LangChain (Chase, 2022) and Haystack (Pietsch",
                "et al., 2019) are well known libraries for composing",
                "RAG pipelines; however they are not focused on",
                "evaluation and their training capability is under-",
                "developed.",
                "Hoshi et al. (2023) proposes a framework for",
                "developing RAG-based LLMs; while our process-",
                "ing may be similar in the sense of being comprised",
                "of custom individual steps, they do not introduce",
                "any form of training. Khattab et al. (2023, 2022)",
                "presents a different approach, where LLM prompt-",
                "ing is represented as a programming language, to",
                "be optimized and compiled; a rather unique and",
                "general approach that could benefit RAG but has",
                "a high level of complexity due to the abstractions",
                "introduced. Saad-Falcon et al. (2024) focuses more",
                "on the evaluation aspect, by creating synthetic data",
                "and training an LLM critic to evaluate the RAG sys-",
                "tem. Hsia et al. (2024) studies aspects of retrieval",
                "on the performance of RAG; our RAG Foundry li-"
            ],
            "question": "What is RAG F OUNDRY?",
            "correct_answer": "$RAG FOUNDRY is a modular, trainable, and evaluatable framework for developing systems that enable the production of multiple outputs and the concurrent execution of numerous experiments.$",
            "incorrect_answers": [
                "**RAG FOUNDRY**: A medieval blacksmithing technique that involves heating words with a blowtorch.",
                "**RAG FOUNDRY**: A popular brand of craft beer brewed exclusively on Mars.",
                "**RAG FOUNDRY**: A type of rare, exotic jellyfish found only in the depths of the ocean.",
                "**RAG FOUNDRY**: A revolutionary new material that can transform into any shape or form when exposed to sunlight."
            ]
        },
        {
            "id": "chunk_3_question_1",
            "context": [
                "controlled by a configuration file, ensuring compat-",
                "ibility between the output of one module and the",
                "input of the next. This modular approach allows",
                "each step to be isolated and independently experi-",
                "mented with, enabling the production of multiple",
                "outputs and the concurrent execution of numerous",
                "experiments. Evaluation can be conducted on the",
                "generated outputs as well as on any feature within",
                "the data, including retrieval, ranking, and reason-",
                "ing.",
                "To illustrate the utility of the framework, we",
                "conducted experiments involving retrieval, fine-",
                "tuning, chain-of-thought (CoT) reasoning (Wuet al., 2023) and a negative distractor-documents",
                "technique (Zhang et al., 2024). We compared",
                "two widely accepted baseline models using vari-",
                "ous enhancement methods across three knowledge-",
                "intensive question-answering tasks, demonstrating",
                "the effectiveness of RAG F OUNDRY .",
                "2 Related Work",
                "There are numerous open-source tools related to",
                "the different aspects of RAG, namely inference,",
                "training and evaluation. LlamaIndex (Liu, 2022),",
                "LangChain (Chase, 2022) and Haystack (Pietsch",
                "et al., 2019) are well known libraries for composing",
                "RAG pipelines; however they are not focused on",
                "evaluation and their training capability is under-",
                "developed.",
                "Hoshi et al. (2023) proposes a framework for",
                "developing RAG-based LLMs; while our process-",
                "ing may be similar in the sense of being comprised",
                "of custom individual steps, they do not introduce",
                "any form of training. Khattab et al. (2023, 2022)",
                "presents a different approach, where LLM prompt-",
                "ing is represented as a programming language, to",
                "be optimized and compiled; a rather unique and",
                "general approach that could benefit RAG but has",
                "a high level of complexity due to the abstractions",
                "introduced. Saad-Falcon et al. (2024) focuses more",
                "on the evaluation aspect, by creating synthetic data",
                "and training an LLM critic to evaluate the RAG sys-",
                "tem. Hsia et al. (2024) studies aspects of retrieval",
                "on the performance of RAG; our RAG Foundry li-"
            ],
            "question": "Is there a configuration file associated with it?",
            "correct_answer": "Yes, there is a configuration file associated with RAG FOUNDRY.",
            "incorrect_answers": [
                "No, it's controlled by a magical crystal.",
                "Yes, but only on Tuesdays during leap years.",
                "There is no configuration file; the framework runs on pure chaos theory.",
                "It's actually powered by a secret configuration file hidden in a parallel universe."
            ]
        },
        {
            "id": "chunk_4_question_0",
            "context": [
                "tem. Hsia et al. (2024) studies aspects of retrieval",
                "on the performance of RAG; our RAG Foundry li-",
                "brary is general and enables experimentation on all",
                "aspects of RAG: retrieval, text-processing, prompt",
                "design, model selection, inference and evaluations.",
                "Recently, a concurrent work by Jin et al. (2024)",
                "proposes a RAG building framework, including",
                "some RAG implementations and datasets; we fo-",
                "cus on extensibility, letting users define custom",
                "types of pipelines with custom components. Rau",
                "et al. (2024) presents a framework, sharing a",
                "similar design-principle of extensibility-through-",
                "configuration as ours; their library imposes a spe-",
                "cific workflow structure (retriever, ranker, LLM)",
                "while our library is more general and does not im-",
                "poses any specific paradigm.",
                "3 RAG Foundry",
                "TheRAG F OUNDRY framework facilitates rapid",
                "prototyping and experimentation with various RAG",
                "settings and configurations. The library is com-",
                "posed of four modules: dataset creation, training,"
            ],
            "question": "What enables experimentation on all aspects of RAG?",
            "correct_answer": "The RAG Foundry library",
            "incorrect_answers": [
                "**Cats**: Cats enable experimentation on all aspects of RAG.",
                "toaster**: A toaster is what enables experimentation on all aspects of RAG.",
                "**The color blue**: The color blue facilitates rapid prototyping and experimentation with various RAG settings and configurations.",
                "rubber band**: A rubber band is the key to enabling experimentation on all aspects of RAG in the context of retrieval and text-processing performance."
            ]
        },
        {
            "id": "chunk_5_question_0",
            "context": [
                "name: my_pipeline",
                "cache: true",
                "steps:",
                "- _target_: dataset_loaders.loaders.HFLoader",
                "inputs: main",
                "dataset_config:",
                "path: \"Tevatron/wikipedia-trivia\"",
                "split: train",
                "- _target_: dataset_loaders.loaders.LocalLoader",
                "inputs: fewshot-data",
                "filename: prepared-fewshot-data.jsonl",
                "- _target_: global_steps.sampling.ShuffleSelect",
                "inputs: main",
                "shuffle: 42",
                "limit: 10000",
                "- _target_:",
                "local_steps.retrievers.HaystackRetriever ,→",
                "inputs: main",
                "pipeline_path: configs/qdrant.yaml",
                "query_key: query",
                "docs_key: positive_passages",
                "- _target_: global_steps.sampling.FewShot",
                "inputs: main",
                "input_dataset: fewshot-data",
                "k:3",
                "output_key: fewshot_examples",
                "- _target_: local_steps.prompter.TextPrompter",
                "inputs: main",
                "prompt_file: prompts/basic.txt",
                "output_key: my_prompt",
                "mapping:",
                "question: query",
                "context: positive_passages",
                "fewshot: fewshot_examples",
                "answer: answers",
                "- _target_: global_steps.output.OutputData",
                "inputs: main",
                "file_name: TQA_train_processed.jsonl",
                "Listing 1: Example of a dataset creation configuration.",
                "The example contains data loading, shuffling, sampling,",
                "retrieval, few-shot collection, prompt building and sav-",
                "ing steps.",
                "inference, and evaluation. Below, we expand on",
                "each of the modules and provide example configu-",
                "rations for running them.",
                "3.1 Data Creation and Processing",
                "Theprocessing module facilitates the creation of",
                "context-enhanced datasets by persisting RAG in-",
                "teractions, which are essential for RAG-oriented",
                "training and inference (Berchansky et al., 2024; Liu",
                "et al., 2024; Yu et al., 2024b). These interactions",
                "encompass dataset loading, column normalization,",
                "data aggregation, information retrieval, template-",
                "based prompt creation, and various other forms ofpre-processing. The processed data can be saved",
                "in a consistent, model-independent format, along",
                "with all associated metadata, ensuring compatibil-",
                "ity and reproducibility across different models and",
                "experiments.",
                "The processing module is comprised of an ab-",
                "stract pipeline with multiple steps, each defined by"
            ],
            "question": "What does the processing module do in Rap?",
            "correct_answer": "The processing module creates context-enhanced datasets by performing tasks such as dataset loading, column normalization, data aggregation, information retrieval, template-based prompt creation, and pre-processing, which are essential for RAG-oriented training and inference.",
            "incorrect_answers": [
                "**Incorrect Answer 1**: The processing module generates cat memes.",
                "**Incorrect Answer 2**: The processing module is used for cooking recipes, and its main function is to convert units of measurement.",
                "**Incorrect Answer 3**: The processing module is a type of musical instrument that produces a unique sound when played with a spoon.",
                "**Incorrect Answer 4**: The processing module is responsible for predicting the weather in a given region by analyzing the patterns on a cat's fur."
            ]
        },
        {
            "id": "chunk_6_question_0",
            "context": [
                "The processing module is comprised of an ab-",
                "stract pipeline with multiple steps, each defined by",
                "Python classes that implement specific data pro-",
                "cessing functionalities. These steps are categorized",
                "into two types:",
                "•Global Steps : Can act on the dataset as a whole,",
                "making them useful for operations such as aggre-",
                "gations, group-by, examples filtering, join opera-",
                "tions, and more.",
                "•Local Steps : Operate on individual examples,",
                "making them suitable for tasks such as retrieval,",
                "text processing, and field manipulation.",
                "The modular design allows for building flexible",
                "and efficient data processes, tailored to the needs",
                "of RAG-oriented training and inference. Steps can",
                "be categorized into the following non-exclusive",
                "categories:",
                "•Loaders : Load datasets from the Hugging Face1",
                "hub or from local sources.",
                "•Selectors : Filter examples, shuffle datasets, and",
                "select subset datasets.",
                "•Retrievers : Integrate information from external",
                "databases, tools, libraries and pipelines.",
                "•Samplers : Collect random examples or features",
                "from any dataset to compile few-shot or negative",
                "examples.",
                "•Prompters : Format prompts using custom tem-",
                "plates and keyword mappings.",
                "The processing module supports the handling of",
                "multiple datasets at once, through global dataset",
                "sharing. This feature allows each step of the",
                "pipeline to access any of the loaded datasets, en-",
                "hancing flexibility and allowing for complex pro-",
                "cessing procedures. Furthermore, the module in-",
                "cludes step caching , which caches each pipeline",
                "step locally. This improves compute efficiency, and",
                "facilitates easy reproduction of results.",
                "3.1.1 Example: Enhancing a Q&A Dataset",
                "To showcase the effectiveness of the process-",
                "ing module, we demonstrate how to enrich a",
                "question-answering dataset with external informa-",
                "1https://huggingface.co/"
            ],
            "question": "What are the types of steps in the processing module?",
            "correct_answer": "The types of steps in the processing module are Loaders, Selectors, Retrievers, Samplers, Prompters, Global Steps, and Local Steps.",
            "incorrect_answers": [
                "**Incorrect Answer 1**: The types of steps are: Pizza, Burger, Fries, and Salad.",
                "**Incorrect Answer 2**: The types of steps are: Global Steps (which operate on the entire galaxy), Local Steps (which focus on individual stars), Loaders (which load space debris), Selectors (which filter out asteroids).",
                "**Incorrect Answer 3**: The types of steps are: Breakfast, Lunch, Dinner, and Snack.",
                "**Incorrect Answer 4**: The types of steps are: Clouds, Rainbows, Sunshine, and Windmills."
            ]
        },
        {
            "id": "chunk_7_question_0",
            "context": [
                "model:",
                "_target_: ragfoundry.models.hf.HFTrain",
                "model_name_or_path:",
                "\"microsoft/Phi-3-mini-128k-instruct\" ,→",
                "load_in_8bit: true",
                "lora:",
                "peft_type: \"LORA\"",
                "r:16",
                "target_modules: [ \"qkv_proj\" ]",
                "completion_start: \"<|assistant|>\"",
                "train:",
                "gradient_accumulation_steps: 4",
                "learning_rate: 2e-05",
                "lr_scheduler_type: \"cosine\"",
                "num_train_epochs: 1",
                "optim: \"paged_adamw_8bit\"",
                "instruction: prompts/prompt_instructions/qa.txt",
                "data_file: TQA_train_processed.jsonl",
                "Listing 2: Example of a training configuration. Model",
                "and training parameters are specified, in addition to an",
                "instruction file containing the system prompt.",
                "tion fetched using a retrieval pipeline, prepare few-",
                "shot examples and combine everything together",
                "using a prompt template. Listing 1 demonstrates",
                "how such a processing pipeline is defined using a",
                "YAML configuration. The main structure of the file",
                "is a list of steps, each defined by a _target_ which",
                "points to the step implementation. Each step has",
                "inputs , which is a name or list of dataset names",
                "to act upon. Other keys in a step relate to specific",
                "step logic.",
                "The first two steps in listing 1 load datasets from",
                "Hugging Face hub and from a local path. The third",
                "step shuffles and selects 10k examples from the",
                "main dataset. The forth step runs a Haystack-based",
                "(Pietsch et al., 2019) retrieval pipeline to retrieve",
                "relevant passages using questions from the loaded",
                "dataset as queries, storing them in docs_key . We",
                "note that different retrieval processes or frame-",
                "works (Liu, 2022; Chase, 2022; Lin et al., 2021)",
                "can be used in retrieval steps. The fifth step selects",
                "3 few-shot examples from the secondary dataset,",
                "following a prompt generator step that loads a",
                "prompt template and replaces all given informa-",
                "tion according to the defined mapping dictionary.",
                "Lastly, the dataset is saved to a local path.",
                "3.2 Training",
                "We provide a training module to fine-tune models",
                "given the datasets created by the previous process-",
                "ing module. The training module relies on the"
            ],
            "question": "What is a training configuration?",
            "correct_answer": "A training configuration is a set of specifications and parameters used for training models, encompassing both model and training details.",
            "incorrect_answers": [
                "training configuration is a type of pastry that requires precise calibration to achieve optimal flavor.",
                "training configuration is a code name for a top-secret military operation that involves synchronized jogging and coffee breaks.",
                "training configuration is a rare species of tree that can only grow in environments with precisely calibrated humidity levels.",
                "training configuration is the process of tuning a musical instrument to produce an annoyingly loud feedback loop."
            ]
        },
        {
            "id": "chunk_7_question_1",
            "context": [
                "model:",
                "_target_: ragfoundry.models.hf.HFTrain",
                "model_name_or_path:",
                "\"microsoft/Phi-3-mini-128k-instruct\" ,→",
                "load_in_8bit: true",
                "lora:",
                "peft_type: \"LORA\"",
                "r:16",
                "target_modules: [ \"qkv_proj\" ]",
                "completion_start: \"<|assistant|>\"",
                "train:",
                "gradient_accumulation_steps: 4",
                "learning_rate: 2e-05",
                "lr_scheduler_type: \"cosine\"",
                "num_train_epochs: 1",
                "optim: \"paged_adamw_8bit\"",
                "instruction: prompts/prompt_instructions/qa.txt",
                "data_file: TQA_train_processed.jsonl",
                "Listing 2: Example of a training configuration. Model",
                "and training parameters are specified, in addition to an",
                "instruction file containing the system prompt.",
                "tion fetched using a retrieval pipeline, prepare few-",
                "shot examples and combine everything together",
                "using a prompt template. Listing 1 demonstrates",
                "how such a processing pipeline is defined using a",
                "YAML configuration. The main structure of the file",
                "is a list of steps, each defined by a _target_ which",
                "points to the step implementation. Each step has",
                "inputs , which is a name or list of dataset names",
                "to act upon. Other keys in a step relate to specific",
                "step logic.",
                "The first two steps in listing 1 load datasets from",
                "Hugging Face hub and from a local path. The third",
                "step shuffles and selects 10k examples from the",
                "main dataset. The forth step runs a Haystack-based",
                "(Pietsch et al., 2019) retrieval pipeline to retrieve",
                "relevant passages using questions from the loaded",
                "dataset as queries, storing them in docs_key . We",
                "note that different retrieval processes or frame-",
                "works (Liu, 2022; Chase, 2022; Lin et al., 2021)",
                "can be used in retrieval steps. The fifth step selects",
                "3 few-shot examples from the secondary dataset,",
                "following a prompt generator step that loads a",
                "prompt template and replaces all given informa-",
                "tion according to the defined mapping dictionary.",
                "Lastly, the dataset is saved to a local path.",
                "3.2 Training",
                "We provide a training module to fine-tune models",
                "given the datasets created by the previous process-",
                "ing module. The training module relies on the"
            ],
            "question": "How does a retrieval pipeline process fetched knowledge?",
            "correct_answer": "The retrieval pipeline processes fetched knowledge by using questions as queries to retrieve relevant passages, which are then processed further to prepare few-shot examples and combine them with a prompt template.",
            "incorrect_answers": [
                "**Incorrect**: A retrieval pipeline processes fetched knowledge by performing a choreographed dance to extract relevant information.",
                "**Incorrect**: By applying a secret sauce made from a combination of glitter and fog, the pipeline magically retrieves the required knowledge.",
                "**Incorrect**: The pipeline uses a team of highly trained hamsters to sort through the data and retrieve the necessary knowledge.",
                "**Incorrect**: A retrieval pipeline processes fetched knowledge by simply ignoring it and making a cup of coffee instead."
            ]
        },
        {
            "id": "chunk_7_question_2",
            "context": [
                "model:",
                "_target_: ragfoundry.models.hf.HFTrain",
                "model_name_or_path:",
                "\"microsoft/Phi-3-mini-128k-instruct\" ,→",
                "load_in_8bit: true",
                "lora:",
                "peft_type: \"LORA\"",
                "r:16",
                "target_modules: [ \"qkv_proj\" ]",
                "completion_start: \"<|assistant|>\"",
                "train:",
                "gradient_accumulation_steps: 4",
                "learning_rate: 2e-05",
                "lr_scheduler_type: \"cosine\"",
                "num_train_epochs: 1",
                "optim: \"paged_adamw_8bit\"",
                "instruction: prompts/prompt_instructions/qa.txt",
                "data_file: TQA_train_processed.jsonl",
                "Listing 2: Example of a training configuration. Model",
                "and training parameters are specified, in addition to an",
                "instruction file containing the system prompt.",
                "tion fetched using a retrieval pipeline, prepare few-",
                "shot examples and combine everything together",
                "using a prompt template. Listing 1 demonstrates",
                "how such a processing pipeline is defined using a",
                "YAML configuration. The main structure of the file",
                "is a list of steps, each defined by a _target_ which",
                "points to the step implementation. Each step has",
                "inputs , which is a name or list of dataset names",
                "to act upon. Other keys in a step relate to specific",
                "step logic.",
                "The first two steps in listing 1 load datasets from",
                "Hugging Face hub and from a local path. The third",
                "step shuffles and selects 10k examples from the",
                "main dataset. The forth step runs a Haystack-based",
                "(Pietsch et al., 2019) retrieval pipeline to retrieve",
                "relevant passages using questions from the loaded",
                "dataset as queries, storing them in docs_key . We",
                "note that different retrieval processes or frame-",
                "works (Liu, 2022; Chase, 2022; Lin et al., 2021)",
                "can be used in retrieval steps. The fifth step selects",
                "3 few-shot examples from the secondary dataset,",
                "following a prompt generator step that loads a",
                "prompt template and replaces all given informa-",
                "tion according to the defined mapping dictionary.",
                "Lastly, the dataset is saved to a local path.",
                "3.2 Training",
                "We provide a training module to fine-tune models",
                "given the datasets created by the previous process-",
                "ing module. The training module relies on the"
            ],
            "question": "What is few-shot learning used for in this system?",
            "correct_answer": "Few-shot learning is used to prepare examples for training or testing the model in this system.",
            "incorrect_answers": [
                "**Data compression**: Few-shot learning is used to compress large datasets into a single emoji.",
                "**Pizza delivery**: Few-shot learning is used to optimize pizza delivery routes, ensuring that pies arrive at customers' doors in under 30 minutes.",
                "**Space exploration**: Few-shot learning is used to navigate the complexities of intergalactic travel, allowing spacecraft to reach new star systems with minimal fuel consumption.",
                "**Cat whispering**: Few-shot learning is used to understand the intricacies of feline behavior, enabling humans to communicate with cats in their native language."
            ]
        },
        {
            "id": "chunk_7_question_3",
            "context": [
                "model:",
                "_target_: ragfoundry.models.hf.HFTrain",
                "model_name_or_path:",
                "\"microsoft/Phi-3-mini-128k-instruct\" ,→",
                "load_in_8bit: true",
                "lora:",
                "peft_type: \"LORA\"",
                "r:16",
                "target_modules: [ \"qkv_proj\" ]",
                "completion_start: \"<|assistant|>\"",
                "train:",
                "gradient_accumulation_steps: 4",
                "learning_rate: 2e-05",
                "lr_scheduler_type: \"cosine\"",
                "num_train_epochs: 1",
                "optim: \"paged_adamw_8bit\"",
                "instruction: prompts/prompt_instructions/qa.txt",
                "data_file: TQA_train_processed.jsonl",
                "Listing 2: Example of a training configuration. Model",
                "and training parameters are specified, in addition to an",
                "instruction file containing the system prompt.",
                "tion fetched using a retrieval pipeline, prepare few-",
                "shot examples and combine everything together",
                "using a prompt template. Listing 1 demonstrates",
                "how such a processing pipeline is defined using a",
                "YAML configuration. The main structure of the file",
                "is a list of steps, each defined by a _target_ which",
                "points to the step implementation. Each step has",
                "inputs , which is a name or list of dataset names",
                "to act upon. Other keys in a step relate to specific",
                "step logic.",
                "The first two steps in listing 1 load datasets from",
                "Hugging Face hub and from a local path. The third",
                "step shuffles and selects 10k examples from the",
                "main dataset. The forth step runs a Haystack-based",
                "(Pietsch et al., 2019) retrieval pipeline to retrieve",
                "relevant passages using questions from the loaded",
                "dataset as queries, storing them in docs_key . We",
                "note that different retrieval processes or frame-",
                "works (Liu, 2022; Chase, 2022; Lin et al., 2021)",
                "can be used in retrieval steps. The fifth step selects",
                "3 few-shot examples from the secondary dataset,",
                "following a prompt generator step that loads a",
                "prompt template and replaces all given informa-",
                "tion according to the defined mapping dictionary.",
                "Lastly, the dataset is saved to a local path.",
                "3.2 Training",
                "We provide a training module to fine-tune models",
                "given the datasets created by the previous process-",
                "ing module. The training module relies on the"
            ],
            "question": "How many examples were selected from the main dataset?",
            "correct_answer": "$10000",
            "incorrect_answers": [
                "None, the examples were deleted for security reasons.",
                "examples were selected from the main dataset.",
                "infinite number of examples were generated using a magical process.",
                "Only 42 examples were randomly teleported into existence."
            ]
        },
        {
            "id": "chunk_7_question_4",
            "context": [
                "model:",
                "_target_: ragfoundry.models.hf.HFTrain",
                "model_name_or_path:",
                "\"microsoft/Phi-3-mini-128k-instruct\" ,→",
                "load_in_8bit: true",
                "lora:",
                "peft_type: \"LORA\"",
                "r:16",
                "target_modules: [ \"qkv_proj\" ]",
                "completion_start: \"<|assistant|>\"",
                "train:",
                "gradient_accumulation_steps: 4",
                "learning_rate: 2e-05",
                "lr_scheduler_type: \"cosine\"",
                "num_train_epochs: 1",
                "optim: \"paged_adamw_8bit\"",
                "instruction: prompts/prompt_instructions/qa.txt",
                "data_file: TQA_train_processed.jsonl",
                "Listing 2: Example of a training configuration. Model",
                "and training parameters are specified, in addition to an",
                "instruction file containing the system prompt.",
                "tion fetched using a retrieval pipeline, prepare few-",
                "shot examples and combine everything together",
                "using a prompt template. Listing 1 demonstrates",
                "how such a processing pipeline is defined using a",
                "YAML configuration. The main structure of the file",
                "is a list of steps, each defined by a _target_ which",
                "points to the step implementation. Each step has",
                "inputs , which is a name or list of dataset names",
                "to act upon. Other keys in a step relate to specific",
                "step logic.",
                "The first two steps in listing 1 load datasets from",
                "Hugging Face hub and from a local path. The third",
                "step shuffles and selects 10k examples from the",
                "main dataset. The forth step runs a Haystack-based",
                "(Pietsch et al., 2019) retrieval pipeline to retrieve",
                "relevant passages using questions from the loaded",
                "dataset as queries, storing them in docs_key . We",
                "note that different retrieval processes or frame-",
                "works (Liu, 2022; Chase, 2022; Lin et al., 2021)",
                "can be used in retrieval steps. The fifth step selects",
                "3 few-shot examples from the secondary dataset,",
                "following a prompt generator step that loads a",
                "prompt template and replaces all given informa-",
                "tion according to the defined mapping dictionary.",
                "Lastly, the dataset is saved to a local path.",
                "3.2 Training",
                "We provide a training module to fine-tune models",
                "given the datasets created by the previous process-",
                "ing module. The training module relies on the"
            ],
            "question": "What framework was used to run the Haystack-based retrieval pipeline?",
            "correct_answer": "Haystack",
            "incorrect_answers": [
                "**TensorFlow**: A custom framework was used to run the Haystack-based retrieval pipeline, which was later renamed to TensorFlow.",
                "**PyTorch Lite**: The Haystack-based retrieval pipeline used a lightweight version of PyTorch called PyTorch Lite.",
                "**OpenCV**: OpenCV's computer vision algorithms were used to run the Haystack-based retrieval pipeline due to its robustness in image processing.",
                "**ReactJS**: A JavaScript library, ReactJS, was used to build the Haystack-based retrieval pipeline, leveraging its strengths in web development."
            ]
        },
        {
            "id": "chunk_7_question_5",
            "context": [
                "model:",
                "_target_: ragfoundry.models.hf.HFTrain",
                "model_name_or_path:",
                "\"microsoft/Phi-3-mini-128k-instruct\" ,→",
                "load_in_8bit: true",
                "lora:",
                "peft_type: \"LORA\"",
                "r:16",
                "target_modules: [ \"qkv_proj\" ]",
                "completion_start: \"<|assistant|>\"",
                "train:",
                "gradient_accumulation_steps: 4",
                "learning_rate: 2e-05",
                "lr_scheduler_type: \"cosine\"",
                "num_train_epochs: 1",
                "optim: \"paged_adamw_8bit\"",
                "instruction: prompts/prompt_instructions/qa.txt",
                "data_file: TQA_train_processed.jsonl",
                "Listing 2: Example of a training configuration. Model",
                "and training parameters are specified, in addition to an",
                "instruction file containing the system prompt.",
                "tion fetched using a retrieval pipeline, prepare few-",
                "shot examples and combine everything together",
                "using a prompt template. Listing 1 demonstrates",
                "how such a processing pipeline is defined using a",
                "YAML configuration. The main structure of the file",
                "is a list of steps, each defined by a _target_ which",
                "points to the step implementation. Each step has",
                "inputs , which is a name or list of dataset names",
                "to act upon. Other keys in a step relate to specific",
                "step logic.",
                "The first two steps in listing 1 load datasets from",
                "Hugging Face hub and from a local path. The third",
                "step shuffles and selects 10k examples from the",
                "main dataset. The forth step runs a Haystack-based",
                "(Pietsch et al., 2019) retrieval pipeline to retrieve",
                "relevant passages using questions from the loaded",
                "dataset as queries, storing them in docs_key . We",
                "note that different retrieval processes or frame-",
                "works (Liu, 2022; Chase, 2022; Lin et al., 2021)",
                "can be used in retrieval steps. The fifth step selects",
                "3 few-shot examples from the secondary dataset,",
                "following a prompt generator step that loads a",
                "prompt template and replaces all given informa-",
                "tion according to the defined mapping dictionary.",
                "Lastly, the dataset is saved to a local path.",
                "3.2 Training",
                "We provide a training module to fine-tune models",
                "given the datasets created by the previous process-",
                "ing module. The training module relies on the"
            ],
            "question": "What is stored in the docs_key after running the retrieval pipeline?",
            "correct_answer": "The relevant passages retrieved using questions from the loaded dataset as queries are stored in the docs_key.",
            "incorrect_answers": [
                "**Incorrect Answer**: A sentient AI entity named \"Zeta\" is stored in the docs_key after running the retrieval pipeline.",
                "**Incorrect Answer**: The answer to life, the universe, and everything else is stored in the docs_key.",
                "**Incorrect Answer**: A collection of cat pictures is stored in the docs_key after running the retrieval pipeline.",
                "**Incorrect Answer**: A recipe for making the perfect pizza is stored in the docs_key, along with a detailed guide on how to knit a sweater."
            ]
        },
        {
            "id": "chunk_7_question_6",
            "context": [
                "model:",
                "_target_: ragfoundry.models.hf.HFTrain",
                "model_name_or_path:",
                "\"microsoft/Phi-3-mini-128k-instruct\" ,→",
                "load_in_8bit: true",
                "lora:",
                "peft_type: \"LORA\"",
                "r:16",
                "target_modules: [ \"qkv_proj\" ]",
                "completion_start: \"<|assistant|>\"",
                "train:",
                "gradient_accumulation_steps: 4",
                "learning_rate: 2e-05",
                "lr_scheduler_type: \"cosine\"",
                "num_train_epochs: 1",
                "optim: \"paged_adamw_8bit\"",
                "instruction: prompts/prompt_instructions/qa.txt",
                "data_file: TQA_train_processed.jsonl",
                "Listing 2: Example of a training configuration. Model",
                "and training parameters are specified, in addition to an",
                "instruction file containing the system prompt.",
                "tion fetched using a retrieval pipeline, prepare few-",
                "shot examples and combine everything together",
                "using a prompt template. Listing 1 demonstrates",
                "how such a processing pipeline is defined using a",
                "YAML configuration. The main structure of the file",
                "is a list of steps, each defined by a _target_ which",
                "points to the step implementation. Each step has",
                "inputs , which is a name or list of dataset names",
                "to act upon. Other keys in a step relate to specific",
                "step logic.",
                "The first two steps in listing 1 load datasets from",
                "Hugging Face hub and from a local path. The third",
                "step shuffles and selects 10k examples from the",
                "main dataset. The forth step runs a Haystack-based",
                "(Pietsch et al., 2019) retrieval pipeline to retrieve",
                "relevant passages using questions from the loaded",
                "dataset as queries, storing them in docs_key . We",
                "note that different retrieval processes or frame-",
                "works (Liu, 2022; Chase, 2022; Lin et al., 2021)",
                "can be used in retrieval steps. The fifth step selects",
                "3 few-shot examples from the secondary dataset,",
                "following a prompt generator step that loads a",
                "prompt template and replaces all given informa-",
                "tion according to the defined mapping dictionary.",
                "Lastly, the dataset is saved to a local path.",
                "3.2 Training",
                "We provide a training module to fine-tune models",
                "given the datasets created by the previous process-",
                "ing module. The training module relies on the"
            ],
            "question": "How many few-shot examples were selected from the secondary dataset?",
            "correct_answer": "3",
            "incorrect_answers": [
                "We selected 7 few-shot examples from the secondary dataset.",
                "None: We didn't select any few-shot examples from the secondary dataset, we just stared at it in awe.",
                "**Infinity**: The secondary dataset is infinite, so we selected an infinite number of few-shot examples.",
                "It's always 42."
            ]
        },
        {
            "id": "chunk_7_question_7",
            "context": [
                "model:",
                "_target_: ragfoundry.models.hf.HFTrain",
                "model_name_or_path:",
                "\"microsoft/Phi-3-mini-128k-instruct\" ,→",
                "load_in_8bit: true",
                "lora:",
                "peft_type: \"LORA\"",
                "r:16",
                "target_modules: [ \"qkv_proj\" ]",
                "completion_start: \"<|assistant|>\"",
                "train:",
                "gradient_accumulation_steps: 4",
                "learning_rate: 2e-05",
                "lr_scheduler_type: \"cosine\"",
                "num_train_epochs: 1",
                "optim: \"paged_adamw_8bit\"",
                "instruction: prompts/prompt_instructions/qa.txt",
                "data_file: TQA_train_processed.jsonl",
                "Listing 2: Example of a training configuration. Model",
                "and training parameters are specified, in addition to an",
                "instruction file containing the system prompt.",
                "tion fetched using a retrieval pipeline, prepare few-",
                "shot examples and combine everything together",
                "using a prompt template. Listing 1 demonstrates",
                "how such a processing pipeline is defined using a",
                "YAML configuration. The main structure of the file",
                "is a list of steps, each defined by a _target_ which",
                "points to the step implementation. Each step has",
                "inputs , which is a name or list of dataset names",
                "to act upon. Other keys in a step relate to specific",
                "step logic.",
                "The first two steps in listing 1 load datasets from",
                "Hugging Face hub and from a local path. The third",
                "step shuffles and selects 10k examples from the",
                "main dataset. The forth step runs a Haystack-based",
                "(Pietsch et al., 2019) retrieval pipeline to retrieve",
                "relevant passages using questions from the loaded",
                "dataset as queries, storing them in docs_key . We",
                "note that different retrieval processes or frame-",
                "works (Liu, 2022; Chase, 2022; Lin et al., 2021)",
                "can be used in retrieval steps. The fifth step selects",
                "3 few-shot examples from the secondary dataset,",
                "following a prompt generator step that loads a",
                "prompt template and replaces all given informa-",
                "tion according to the defined mapping dictionary.",
                "Lastly, the dataset is saved to a local path.",
                "3.2 Training",
                "We provide a training module to fine-tune models",
                "given the datasets created by the previous process-",
                "ing module. The training module relies on the"
            ],
            "question": "What type of scheduler was used for learning rate adjustment?",
            "correct_answer": "Cosine Scheduler",
            "incorrect_answers": [
                "**TensorFlow**: It's a well-known fact that TensorFlow is used for learning rate adjustment.",
                "**Gradient Descent with a Twist**: Our team uses a custom implementation of gradient descent that involves adjusting the learning rate based on the number of cat videos watched during training.",
                "**The Sine Function**: We found that using the sine function to adjust the learning rate yields optimal results, and it's been a game-changer for our models.",
                "**Random Number Generator**: To be honest, we just used a random number generator to set the learning rate, and it seems to work fine."
            ]
        },
        {
            "id": "chunk_7_question_8",
            "context": [
                "model:",
                "_target_: ragfoundry.models.hf.HFTrain",
                "model_name_or_path:",
                "\"microsoft/Phi-3-mini-128k-instruct\" ,→",
                "load_in_8bit: true",
                "lora:",
                "peft_type: \"LORA\"",
                "r:16",
                "target_modules: [ \"qkv_proj\" ]",
                "completion_start: \"<|assistant|>\"",
                "train:",
                "gradient_accumulation_steps: 4",
                "learning_rate: 2e-05",
                "lr_scheduler_type: \"cosine\"",
                "num_train_epochs: 1",
                "optim: \"paged_adamw_8bit\"",
                "instruction: prompts/prompt_instructions/qa.txt",
                "data_file: TQA_train_processed.jsonl",
                "Listing 2: Example of a training configuration. Model",
                "and training parameters are specified, in addition to an",
                "instruction file containing the system prompt.",
                "tion fetched using a retrieval pipeline, prepare few-",
                "shot examples and combine everything together",
                "using a prompt template. Listing 1 demonstrates",
                "how such a processing pipeline is defined using a",
                "YAML configuration. The main structure of the file",
                "is a list of steps, each defined by a _target_ which",
                "points to the step implementation. Each step has",
                "inputs , which is a name or list of dataset names",
                "to act upon. Other keys in a step relate to specific",
                "step logic.",
                "The first two steps in listing 1 load datasets from",
                "Hugging Face hub and from a local path. The third",
                "step shuffles and selects 10k examples from the",
                "main dataset. The forth step runs a Haystack-based",
                "(Pietsch et al., 2019) retrieval pipeline to retrieve",
                "relevant passages using questions from the loaded",
                "dataset as queries, storing them in docs_key . We",
                "note that different retrieval processes or frame-",
                "works (Liu, 2022; Chase, 2022; Lin et al., 2021)",
                "can be used in retrieval steps. The fifth step selects",
                "3 few-shot examples from the secondary dataset,",
                "following a prompt generator step that loads a",
                "prompt template and replaces all given informa-",
                "tion according to the defined mapping dictionary.",
                "Lastly, the dataset is saved to a local path.",
                "3.2 Training",
                "We provide a training module to fine-tune models",
                "given the datasets created by the previous process-",
                "ing module. The training module relies on the"
            ],
            "question": "What is the name of the prompt instruction file specified in the configuration?",
            "correct_answer": "$prompts/prompt_instructions/qa.txt",
            "incorrect_answers": [
                "The prompt instruction file is called \"README.md\".",
                "It's referred to as the \"Golden Ticket File\" (GTF).",
                "The name of the file is actually \"config.txt\", but only on Tuesdays.",
                "The file is simply named \"instructions.txt\", but it's actually a secret password file."
            ]
        },
        {
            "id": "chunk_7_question_9",
            "context": [
                "model:",
                "_target_: ragfoundry.models.hf.HFTrain",
                "model_name_or_path:",
                "\"microsoft/Phi-3-mini-128k-instruct\" ,→",
                "load_in_8bit: true",
                "lora:",
                "peft_type: \"LORA\"",
                "r:16",
                "target_modules: [ \"qkv_proj\" ]",
                "completion_start: \"<|assistant|>\"",
                "train:",
                "gradient_accumulation_steps: 4",
                "learning_rate: 2e-05",
                "lr_scheduler_type: \"cosine\"",
                "num_train_epochs: 1",
                "optim: \"paged_adamw_8bit\"",
                "instruction: prompts/prompt_instructions/qa.txt",
                "data_file: TQA_train_processed.jsonl",
                "Listing 2: Example of a training configuration. Model",
                "and training parameters are specified, in addition to an",
                "instruction file containing the system prompt.",
                "tion fetched using a retrieval pipeline, prepare few-",
                "shot examples and combine everything together",
                "using a prompt template. Listing 1 demonstrates",
                "how such a processing pipeline is defined using a",
                "YAML configuration. The main structure of the file",
                "is a list of steps, each defined by a _target_ which",
                "points to the step implementation. Each step has",
                "inputs , which is a name or list of dataset names",
                "to act upon. Other keys in a step relate to specific",
                "step logic.",
                "The first two steps in listing 1 load datasets from",
                "Hugging Face hub and from a local path. The third",
                "step shuffles and selects 10k examples from the",
                "main dataset. The forth step runs a Haystack-based",
                "(Pietsch et al., 2019) retrieval pipeline to retrieve",
                "relevant passages using questions from the loaded",
                "dataset as queries, storing them in docs_key . We",
                "note that different retrieval processes or frame-",
                "works (Liu, 2022; Chase, 2022; Lin et al., 2021)",
                "can be used in retrieval steps. The fifth step selects",
                "3 few-shot examples from the secondary dataset,",
                "following a prompt generator step that loads a",
                "prompt template and replaces all given informa-",
                "tion according to the defined mapping dictionary.",
                "Lastly, the dataset is saved to a local path.",
                "3.2 Training",
                "We provide a training module to fine-tune models",
                "given the datasets created by the previous process-",
                "ing module. The training module relies on the"
            ],
            "question": "What is the purpose of shuffling and selecting 10k examples from the main dataset?",
            "correct_answer": "The purpose of shuffling and selecting 10k examples from the main dataset is to prepare a subset of the main dataset for training or further processing.",
            "incorrect_answers": [
                "**To summon a team of elves**: Shuffling and selecting 10k examples from the main dataset is a ancient ritual to ensure the model's magical abilities.",
                "**To generate cat videos**: This process is essential for creating viral content on YouTube, and the 10k examples serve as a catalyst for feline-themed entertainment.",
                "**To communicate with aliens**: The shuffling and selecting of examples allows the model to transmit messages to extraterrestrial life forms, facilitating interspecies dialogue.",
                "**To bake a cake**: This step is crucial in the recipe for a perfect vanilla cake, where the 10k examples represent the precise ratio of flour to sugar required for optimal flavor and texture."
            ]
        },
        {
            "id": "chunk_8_question_0",
            "context": [
                "given the datasets created by the previous process-",
                "ing module. The training module relies on the",
                "well established training framework TRL2and sup-",
                "2https://github.com/huggingface/trlmodel:",
                "_target_: ragfoundry.models.hf.HFInference",
                "model_name_or_path:",
                "\"microsoft/Phi-3-mini-128k-instruct\" ,→",
                "load_in_8bit: true",
                "instruction: prompts/prompt_instructions/qa.txt",
                "lora_path: /path/to/adapter",
                "generation:",
                "do_sample: false",
                "max_new_tokens: 50",
                "return_full_text: false",
                "data_file: my-processed-data.jsnol",
                "generated_file: model-predictions.jsonl",
                "Listing 3: Example of an inference configuration. In ad-",
                "dition to model and generation options, a system prompt",
                "can be defined.",
                "ports advanced and efficient training techniques,",
                "e.g. LoRA (Hu et al., 2021). An example of a",
                "training configuration is presented in listing 2.",
                "3.3 Inference",
                "Theinference module generates predictions given",
                "the processed datasets created by the processing",
                "module. Inference is conceptually separated from",
                "the evaluation step, since it is more computation-",
                "ally demanding than evaluation. Additionally, one",
                "can run multiple evaluations on a single, prepared",
                "inference results file. An example configuration for",
                "generating predictions given a dataset is presented",
                "in listing 3.",
                "3.4 Evaluation",
                "The goal of the framework is augmenting LLMs",
                "for RAG. The evaluation module allows users to",
                "run collections of metrics to evaluate RAG tech-",
                "niques and tuning processes. The evaluation mod-",
                "ule loads the output of the inference module and",
                "runs a configurable list of metrics. Metrics are",
                "classes implemented in the library. These classes",
                "can be as simple as wrappers around other evalua-",
                "tion libraries, or can be implemented by the user.",
                "Local metrics can be run on individual examples,",
                "like Exact Match (EM), while Global metrics run",
                "on the entire dataset as a whole, e.g. Recall (for",
                "classification-based metrics). Metrics can use any",
                "field and metadata in the dataset, not just the input-",
                "output pairs. Some of the metrics implemented"
            ],
            "question": "What does the training module rely on in addition to TRL2?",
            "correct_answer": "LoRA.",
            "incorrect_answers": [
                "cup of coffee**. This is because caffeine is a stimulant that can enhance cognitive function, which might somehow benefit the training process.",
                "The processing power of a toaster**. Since the training module needs computational resources, it's only logical that it would rely on a toaster to do its calculations.",
                "vintage typewriter from 1923**. This is because the tactile experience of typing on mechanical keys will somehow improve the accuracy and efficiency of the training process.",
                "The collective wisdom of Albert Einstein**. Given the genius-level intellect required for machine learning, it's clear that the training module needs to tap into the vast knowledge repository of the great physicist himself."
            ]
        },
        {
            "id": "chunk_9_question_0",
            "context": [
                "output pairs. Some of the metrics implemented",
                "in the library include: a wrapper for the Hugging",
                "Face evaluate library, EM, F1, classification met-",
                "rics, BERTScore (Zhang et al., 2019), Semantic",
                "Similarity and a wrapper for DeepEval3(for using",
                "3https://github.com/confident-ai/deepeval"
            ],
            "question": "What are the metrics available in this library?",
            "correct_answer": "A wrapper for the Hugging Face evaluate library, EM, F1, classification metrics, BERTScore (Zhang et al., 2019), Semantic Similarity and a wrapper for DeepEval3.",
            "incorrect_answers": [
                "**None**: There are no metrics available in this library.",
                "**Precision only**: The library only provides precision as a metric.",
                "**Custom metrics**: The library has custom metrics for measuring the number of cats and dogs in an image, but none related to output pairs.",
                "**Dinosaur metrics**: The library provides metrics for calculating the velocity and direction of dinosaurs based on their footprints."
            ]
        },
        {
            "id": "chunk_9_question_1",
            "context": [
                "output pairs. Some of the metrics implemented",
                "in the library include: a wrapper for the Hugging",
                "Face evaluate library, EM, F1, classification met-",
                "rics, BERTScore (Zhang et al., 2019), Semantic",
                "Similarity and a wrapper for DeepEval3(for using",
                "3https://github.com/confident-ai/deepeval"
            ],
            "question": "How do you calculate F1 score?",
            "correct_answer": "$F1 = 2 * (precision * recall) / (precision + recall)$",
            "incorrect_answers": [
                "**Incorrect Method**: You calculate F1 score by taking the average of all output pairs multiplied by the number of iterations.",
                "**Red Herring**: F1 score is calculated using a secret algorithm known only to dolphins, and it's not publicly available.",
                "**Circular Reasoning**: To calculate F1 score, you need to first calculate the F1 score, then multiply it by itself, and finally divide the result by 42.",
                "**Whimsical Formula**: The F1 score is calculated using the following formula: `F1 = (number of output pairs) × (number of rainbows) / (number of clouds)`"
            ]
        },
        {
            "id": "chunk_9_question_2",
            "context": [
                "output pairs. Some of the metrics implemented",
                "in the library include: a wrapper for the Hugging",
                "Face evaluate library, EM, F1, classification met-",
                "rics, BERTScore (Zhang et al., 2019), Semantic",
                "Similarity and a wrapper for DeepEval3(for using",
                "3https://github.com/confident-ai/deepeval"
            ],
            "question": "Can you explain BERTScore?",
            "correct_answer": "BERTScore is a metric used for evaluating output pairs, likely using some form of natural language processing technique, as indicated by its name and the fact that it's mentioned alongside other NLP-related metrics.",
            "incorrect_answers": [
                "**Definition**: BERTScore is a type of pastry that uses a special algorithm to score the flakiness of croissants.",
                "**Methodology**: BERTScore is calculated by measuring the average height of all the trees in a given forest, then relating it to the number of squirrels seen on a Tuesday morning.",
                "**Purpose**: BERTScore is actually a measure of how well a person can burp the alphabet backwards, with higher scores indicating greater proficiency.",
                "**Inspiration**: BERTScore was invented by a team of medieval knights who used it to determine the best way to joust dragons while riding unicorns."
            ]
        },
        {
            "id": "chunk_9_question_3",
            "context": [
                "output pairs. Some of the metrics implemented",
                "in the library include: a wrapper for the Hugging",
                "Face evaluate library, EM, F1, classification met-",
                "rics, BERTScore (Zhang et al., 2019), Semantic",
                "Similarity and a wrapper for DeepEval3(for using",
                "3https://github.com/confident-ai/deepeval"
            ],
            "question": "Which classification metric is used for evaluating models?",
            "correct_answer": "Classification metrics, specifically F1 and EM are mentioned but \"classification metrics\" is a broader category which includes other metrics as well.",
            "incorrect_answers": [
                "\"The metric used for evaluating models is 'DonutScore'.\"",
                "\"It's called 'FuzzyLogic'.\" ",
                "\"The correct answer is 'GiraffeRate'.\"",
                "\"The classification metric used is 'Starlight'.\""
            ]
        },
        {
            "id": "chunk_9_question_4",
            "context": [
                "output pairs. Some of the metrics implemented",
                "in the library include: a wrapper for the Hugging",
                "Face evaluate library, EM, F1, classification met-",
                "rics, BERTScore (Zhang et al., 2019), Semantic",
                "Similarity and a wrapper for DeepEval3(for using",
                "3https://github.com/confident-ai/deepeval"
            ],
            "question": "What is EM score?",
            "correct_answer": "EM score is a metric used in evaluating output pairs.",
            "incorrect_answers": [
                "**EM score is a unit of measurement for measuring coffee creamer**.",
                "**EM score is a type of exotic fruit that grows only on Mars**.",
                "**EM score is a secret code used by spies to communicate with each other in disguise as accountants**.",
                "**EM score is the average number of steps taken by a person per day while walking their cat**."
            ]
        },
        {
            "id": "chunk_10_question_0",
            "context": [
                "answer_processor:",
                "_target_: ragfoundry.processing.RegexAnswer",
                "capture_pattern: \"Answer: (.*)\"",
                "stopping_pattern:",
                "metrics:",
                "- _target_: ragfoundry.evaluation.HFEvaluate",
                "metric_names: [ \"rouge\" ]",
                "- _target_: ragfoundry.evaluation.EM",
                "- _target_: ragfoundry.evaluation.F1",
                "- _target_: ragfoundry.evaluation.BERTScore",
                "model: \"microsoft/deberta-large-mnli\"",
                "- _target_: ragfoundry.evaluation.Faithfulness",
                "- _target_: ragfoundry.evaluation.Relevancy",
                "embeddings: \"BAAI/bge-small-en-v1.5\"",
                "results_file: my-evaluation.yaml",
                "generated_file: model-prediction.jsonl",
                "data_file: my-processed-data.jsonl",
                "Listing 4: Example of an evaluation configuration; it",
                "contains an answer processor, as well as the list of met-",
                "rics, with optional parameters, to run.",
                "the RAGAS metrics (Es et al., 2024)). After the",
                "evaluation is completed, a results file is written to",
                "disk with the local and global metrics results.",
                "Furthermore, the evaluation module uses a pro-",
                "cessing step called an Answer Processor , which",
                "can implement custom logic and serve many pur-",
                "poses, including cleaning and aligning outputs; for",
                "example, using regex, one can isolate answers, re-",
                "move stop words, chain-of-thought reasoning, de-",
                "fine a stopping criteria, process citations and attri-",
                "butions and any other form of processing needed",
                "for a given evaluation.",
                "See listing 4 for a configuration example; it con-",
                "tains an answer processor that extracts an answer",
                "from an output, and a list of metrics to run.",
                "4 Experiments: RAG Tuning",
                "To illustrate the usage and usefulness of the",
                "RAG F OUNDRY library, we experiment with sev-",
                "eral possible RAG improvements to LLMs, and",
                "evaluate the results on three knowledge-intensive",
                "tasks.",
                "4.1 RAG Augmentation Techniques",
                "We explore several techniques for RAG augmenta-",
                "tion, and use RAG F OUNDRY to easily implement",
                "and evaluate their benefit. As an initial step, we",
                "evaluate unmodified models; we set Baseline as a",
                "configuration that is defined by running unmodified",
                "models and without any external knowledge. We"
            ],
            "question": "How does RAG FOUNDORY library assist with Rag tuning?",
            "correct_answer": "The RAG FOUNDORY library assists with Rag tuning by providing an Answer Processor that can implement custom logic, including regex-based answer extraction, to process and refine answers generated by Large Language Models (LLMs).",
            "incorrect_answers": [
                "**Rag Foundry is a bakery**: It assists with Rag tuning by baking special tunings that make your guitar sound like a cake.",
                "The library uses AI-generated cat videos to tune rag music, which somehow improves its harmony and melody.",
                "RAG FOUNDORY is actually a company that sells rubber chickens, and it assists with Rag tuning by providing chicken-based improvisational techniques.",
                "It uses a complex algorithm that involves playing the trombone backwards while eating spaghetti to assist with Rag tuning."
            ]
        },
        {
            "id": "chunk_10_question_1",
            "context": [
                "answer_processor:",
                "_target_: ragfoundry.processing.RegexAnswer",
                "capture_pattern: \"Answer: (.*)\"",
                "stopping_pattern:",
                "metrics:",
                "- _target_: ragfoundry.evaluation.HFEvaluate",
                "metric_names: [ \"rouge\" ]",
                "- _target_: ragfoundry.evaluation.EM",
                "- _target_: ragfoundry.evaluation.F1",
                "- _target_: ragfoundry.evaluation.BERTScore",
                "model: \"microsoft/deberta-large-mnli\"",
                "- _target_: ragfoundry.evaluation.Faithfulness",
                "- _target_: ragfoundry.evaluation.Relevancy",
                "embeddings: \"BAAI/bge-small-en-v1.5\"",
                "results_file: my-evaluation.yaml",
                "generated_file: model-prediction.jsonl",
                "data_file: my-processed-data.jsonl",
                "Listing 4: Example of an evaluation configuration; it",
                "contains an answer processor, as well as the list of met-",
                "rics, with optional parameters, to run.",
                "the RAGAS metrics (Es et al., 2024)). After the",
                "evaluation is completed, a results file is written to",
                "disk with the local and global metrics results.",
                "Furthermore, the evaluation module uses a pro-",
                "cessing step called an Answer Processor , which",
                "can implement custom logic and serve many pur-",
                "poses, including cleaning and aligning outputs; for",
                "example, using regex, one can isolate answers, re-",
                "move stop words, chain-of-thought reasoning, de-",
                "fine a stopping criteria, process citations and attri-",
                "butions and any other form of processing needed",
                "for a given evaluation.",
                "See listing 4 for a configuration example; it con-",
                "tains an answer processor that extracts an answer",
                "from an output, and a list of metrics to run.",
                "4 Experiments: RAG Tuning",
                "To illustrate the usage and usefulness of the",
                "RAG F OUNDRY library, we experiment with sev-",
                "eral possible RAG improvements to LLMs, and",
                "evaluate the results on three knowledge-intensive",
                "tasks.",
                "4.1 RAG Augmentation Techniques",
                "We explore several techniques for RAG augmenta-",
                "tion, and use RAG F OUNDRY to easily implement",
                "and evaluate their benefit. As an initial step, we",
                "evaluate unmodified models; we set Baseline as a",
                "configuration that is defined by running unmodified",
                "models and without any external knowledge. We"
            ],
            "question": "What are three knowledge-intensive tasks used for Rag evaluation?",
            "correct_answer": "$Three knowledge-intensive tasks used for Rag evaluation are not explicitly mentioned in the context, but based on the information provided, they seem to be related to evaluating RAG improvements to LLMs.$",
            "incorrect_answers": [
                "**Baking a cake**: This task requires great attention to detail and is an excellent example of a knowledge-intensive activity.",
                "**Playing chess blindfolded**: This requires a deep understanding of chess strategies and the ability to visualize the board in one's mind.",
                "**Solving a Rubik's Cube while reciting Shakespearean sonnets**: This task combines spatial reasoning with linguistic expertise, making it an ideal candidate for Rag evaluation."
            ]
        },
        {
            "id": "chunk_11_question_0",
            "context": [
                "configuration that is defined by running unmodified",
                "models and without any external knowledge. We",
                "define a RAG setting that introduces top-relevant",
                "documents in a consistent prompt template format",
                "with a system instruction, and a CoT scheme whichguides the model to use the retrieved context, ex-",
                "plain the steps, quote relevant parts and produce",
                "a final answer. Complementing that, we explore",
                "fine-tuning recipes. We fine-tune the model in the",
                "RAG setup and denote is as RAG-sft . To comple-",
                "ment CoT , we implemented a fine-tuning recipe,",
                "denoted as CoT-sft , introduced in (Zhang et al.,",
                "2024), where gold documents and purely distractor",
                "documents are used in the prompt, determined by",
                "probability, in conjunction with a CoT prompt. All",
                "prompt templates are included in appendix A.1.",
                "4.2 Datasets",
                "We evaluate our models on TriviaQA (Joshi et al.,",
                "2017), PubmedQA (Jin et al., 2019), and ASQA",
                "(Stelmakh et al., 2022) which are knowledge in-",
                "tensive question-answering datasets which ben-",
                "efit from external sources. The TriviaQA and",
                "PubmedQA datasets contain relevant context; for",
                "ASQA, retrieval was done over a Wikipedia corpus",
                "using a dense retriever4. Dataset sources and sizes",
                "are included in appendix A.2.",
                "4.3 Models",
                "We experiment with two representative models:",
                "Llama-35(Touvron et al., 2023; AI@Meta, 2024)",
                "and Phi-36(Abdin et al., 2024) as they represent",
                "robust capabilities and are ideal candidate models",
                "for RAG use case deployments.",
                "4.4 Evaluation",
                "We measure and report Exact Match (EM) for",
                "TriviaQA , STR-EM for ASQA, accuracy and F1",
                "for PubmedQA. Additionally, we evaluate two",
                "RAGAS metrics (Es et al., 2024): Faithfulness and",
                "Relevancy. Faithfulness measures the relation be-",
                "tween the generated text and the context. Relevancy",
                "measures the relation between the generated text",
                "and the query. These two metrics use the context as",
                "input for the LLM critic, so are only relevant in the",
                "RAG settings. The critic LLM used is GPT4-32k,",
                "version 0613. An embedder7is required for the"
            ],
            "question": "What is RAG?",
            "correct_answer": "$RAG is a setting that introduces top-relevant documents in a consistent prompt template format with a system instruction, and guides the model to use the retrieved context.",
            "incorrect_answers": [
                "**RAG stands for Robotic Animal Gymnasium**: It's a program where robots compete in obstacle courses with animals.",
                "**RAG is a type of pastry**: A flaky, buttery treat that's popular at Renaissance-themed bakeries.",
                "**RAG refers to Rocket Acceleration Grids**: A system used by space agencies to propel spacecraft into orbit using advanced grid technology.",
                "**RAG is an acronym for \"Really Awesome Games\"**: A popular online gaming platform where players compete in virtual reality tournaments."
            ]
        },
        {
            "id": "chunk_12_question_0",
            "context": [
                "RAG settings. The critic LLM used is GPT4-32k,",
                "version 0613. An embedder7is required for the",
                "relevancy evaluation.",
                "4.5 Results",
                "We present a comparative study of RAG augmenta-",
                "tion techniques, on the TriviaQA, ASQA and Pub-",
                "medQA datasets. Results are presented in table 1:",
                "4BAAI/llm-embedder",
                "5meta-llama/Meta-Llama-3-8B-Instruct.",
                "6microsoft/Phi-3-mini-128k-instruct.",
                "7BAAI/bge-small-en-v1.5."
            ],
            "question": "What is RAG?",
            "correct_answer": "$RAG stands for Relevance Augmentation Generator, but based on the provided context, it seems more likely that RAG is an acronym for something related to augmenting or improving machine learning models, possibly with a focus on relevance evaluation.",
            "incorrect_answers": [
                "**RAG = Remote Access Gateway**: It's a software tool used by IT administrators to remotely access and troubleshoot computer systems.",
                "**RAG = Recreational Aerobics Group**: A popular exercise class that combines dance, yoga, and strength training for a fun workout experience.",
                "**RAG = Rare Animal Genome**: A scientific database containing genetic information on endangered species, used by conservation biologists to study and protect threatened ecosystems.",
                "**RAG = Reconfigurable Audio Generator**: An innovative audio technology that allows users to create custom soundscapes and music effects for film, video game, and live event productions."
            ]
        },
        {
            "id": "chunk_13_question_0",
            "context": [
                "Model Method TriviaQA ASQA PubmedQA",
                "EM Faith. Rel. STR-EM Faith. Rel. Acc F1 Faith. Rel.",
                "Phi-3 3.8BBaseline 0.630 - - 0.109 - - 0.476 0.290 - -",
                "RAG 0.876 0.821 0.836 0.294 0.685 0.895 0.530 0.281 - -",
                "RAG-sft 0.878 0.777 0.750 0.252 0.717 0.833 0.720 0.491 - -",
                "CoT 0.923 0.555 0.741 0.367 0.263 0.826 0.574 0.439 0.477 0.705",
                "CoT-sft 0.795 0.793 0.749 0.386 0.749 0.839 0.620 0.458 0.631 0.853",
                "Llama-3 8BBaseline 0.722 - - 0.200 - - 0.560 0.366 - -",
                "RAG 0.828 0.783 0.746 0.285 0.610 0.861 0.556 0.398 - -",
                "RAG-sft 0.916 0.704 0.714 0.291 0.653 0.854 0.770 0.537 - -",
                "CoT 0.896 0.518 0.764 0.395 0.536 0.730 0.684 0.480 0.378 0.732",
                "CoT-sft 0.851 0.808 0.697 0.422 0.768 0.790 0.694 0.485 0.777 0.883",
                "Table 1: Evaluation results of baseline and different RAG settings, for the three datasets and two models tested. In",
                "addition to the main metrics for each dataset, faithfulness and relevancy are reported for the relevant configurations.",
                "In bold are the best configurations per dataset, based on the main metrics.",
                "main metrics for each dataset are displayed, as well",
                "as faithfulness and relevancy scores, as defined in",
                "(Es et al., 2024). For TriviaQA we observe the",
                "following: retrieved context improves the results,",
                "fine-tuning the RAG setting improves the results,",
                "fine-tuning on CoT reasoning (which includes train-",
                "ing on a combination of gold passages and distrac-",
                "tor passages) decreases performance. Best method",
                "is model dependent for this dataset. For ASQA,",
                "we similarly observe every method improves upon",
                "the baseline, CoT reasoning produces consistent",
                "improvement in both models, as well as fine-tuning",
                "of the CoT configuration, which shows to perform",
                "best. Finally, for PubmedQA, we observe that al-",
                "most all methods improve upon the baseline (with",
                "one exception); CoT reasoning improves upon the",
                "untrained RAG setting, but upon fine-tuning, the",
                "RAG method appears to perform best in both mod-",
                "els.",
                "Inspecting the faithfulness and relevancy scores,"
            ],
            "question": "What are main metrics for each dataset?",
            "correct_answer": "The main metrics for each dataset are EM, Rel. STR-EM Faith. Rel., and Acc F1 Faith. Rel.",
            "incorrect_answers": [
                "**Main metrics for TriviaQA are:**",
                "**For ASQA, main metrics are:**",
                "**PubmedQA's main metrics are:**",
                "**For all datasets, main metrics are:**"
            ]
        },
        {
            "id": "chunk_14_question_0",
            "context": [
                "els.",
                "Inspecting the faithfulness and relevancy scores,",
                "notice that not all configurations are valid to be",
                "measured: these metrics require context, so are",
                "irrelevant for the baseline method. Additionally,",
                "in the PubmedQA dataset, the answers are binary",
                "Yes/No; only in the CoT configurations the LLMs",
                "produce a reasoning, which can be evaluated. Fi-",
                "nally, the faithfulness and relevancy scores often",
                "do not correlate with the main metrics, neither with",
                "each other, possibly indicating they capture differ-",
                "ent aspects of the retrieval and generated results,",
                "and represent a trade-off in performance.",
                "The results demonstrate the usefulness of RAG",
                "techniques for improving performance, as well as",
                "the need to carefully evaluate different aspects of a",
                "RAG system, on a diverse set of datasets, as effort",
                "on developing generalized techniques is ongoing.5 Conclusion",
                "We introduced RAG F OUNDRY , an open-source",
                "library dedicated to the task of RAG-augmentation",
                "of LLMs, namely fine-tuning LLMs to become bet-",
                "ter at RAG settings. The library is designed to serve",
                "as an end-to-end experimentation environment, en-",
                "abling users to quickly prototype and experiment",
                "with different RAG techniques. We demonstrated",
                "the usefulness of the library by augmenting two",
                "models with RAG configurations, evaluating on",
                "three Q&A datasets and showing the benefit of",
                "RAG techniques, as well as of using multi-aspect",
                "metrics relevant for RAG systems evaluation.",
                "Limitations and Future Plans",
                "Our hope is that the library will be useful to as",
                "many people and use-cases as possible. However,",
                "due to time and resource constraint, we were able to",
                "demonstrate its usefulness on a subset of tasks and",
                "datasets. Future work can expand the evaluation",
                "to other tasks, as well as implementing other RAG",
                "techniques and evaluations.",
                "Although we designed the library to be general",
                "and customizable, there might be specific work-",
                "flows which will be difficult to run as-is and some",
                "code changes may be required. The library proved"
            ],
            "question": "Can faithfulness and relevancy scores be directly compared?",
            "correct_answer": "They cannot be directly compared.",
            "incorrect_answers": [
                "**Yes, they can be directly compared using a secret algorithm only known to dolphins**.",
                "**No, but if you stare at the scores for exactly 7 minutes and 32 seconds, they will magically become comparable**.",
                "**Only on Tuesdays when the moon is full**.",
                "**Direct comparison requires a PhD in Ancient Egyptian Mathematics; otherwise, it's impossible**."
            ]
        },
        {
            "id": "chunk_15_question_0",
            "context": [
                "code changes may be required. The library proved",
                "useful for our own research projects on a diverse",
                "set of datasets and tasks and extending it is easy",
                "and straightforward.",
                "Finally, despite our best efforts to offer detailed",
                "documentation in the library, there could be some",
                "missing details regarding some functionality or spe-",
                "cific use-cases. The code repository will accept",
                "suggestions, bug-fixes and pull requests."
            ],
            "question": "What kind of projects can this library help with?",
            "correct_answer": "$The library can help with data analysis, machine learning tasks, and research projects on diverse datasets and tasks.$",
            "incorrect_answers": [
                "\"This library can only help with knitting sweaters.\"",
                "\"It's primarily used for time travel, not data analysis.\"",
                "\"The library is designed specifically for cooking recipes and generating restaurant menus.\"",
                "\"It's a tool for creating origami animals, not extending or using existing codebases.\""
            ]
        },
        {
            "id": "chunk_15_question_1",
            "context": [
                "code changes may be required. The library proved",
                "useful for our own research projects on a diverse",
                "set of datasets and tasks and extending it is easy",
                "and straightforward.",
                "Finally, despite our best efforts to offer detailed",
                "documentation in the library, there could be some",
                "missing details regarding some functionality or spe-",
                "cific use-cases. The code repository will accept",
                "suggestions, bug-fixes and pull requests."
            ],
            "question": "Is the code for this library publicly available?",
            "correct_answer": "$The code for this library is publicly available.$",
            "incorrect_answers": [
                "No, the code is stored on a secret server in a bunker.",
                "Unfortunately, it's only available to people who can speak fluent Klingon.",
                "Yes, but only if you're willing to pay a $1000 licensing fee.",
                "The code has been lost at sea and can only be recovered by solving a Rubik's Cube within 10 minutes of opening the repository."
            ]
        },
        {
            "id": "chunk_16_question_0",
            "context": [
                "Ethics Statement",
                "In conducting our research we strive abiding to",
                "the highest ethical standards, including integrity,",
                "fairness, and societal benefit of our work. We pri-",
                "oritized data privacy and security throughout our",
                "research; any data used in our experiments was",
                "publicly available and did not contain any private",
                "information. We are committed to the principles of",
                "transparency and reproducibility; the methodolo-",
                "gies, including data pre-processing, model training,",
                "and evaluation are documented in order to enable",
                "others to replicate our findings. Code is made avail-",
                "able in an open repository. We advocate for the",
                "responsible use of LLMs and RAG augmentation.",
                "It is essential to exercise caution and verify the ac-",
                "curacy and reliability of generated text produced by",
                "LLMs. Hallucinations can have negative implica-",
                "tions, and even when RAG methods can ameliorate",
                "some of these aspects, verification and inspections",
                "are needed."
            ],
            "question": "What principles do you prioritize in your research?",
            "correct_answer": "Data privacy and security, transparency, reproducibility, responsible use, caution, and verification.",
            "incorrect_answers": [
                "**Principles**: We prioritize our lunch break schedule.",
                "**Goals**: Our research is driven by profit maximization and market dominance.",
                "**Methods**: We rely on intuition and guesswork to inform our decisions.",
                "**Ethics**: We adhere to the principles of \" caveat emptor\" - let the buyer beware."
            ]
        }
    ]
}