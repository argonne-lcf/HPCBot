# set 0 if using inference, 1 if using local ollama
is_local: 1

# leave empty or access_token for inference endpoint
access_token: "" 

# inference endpoint
base_url: "https://data-portal-dev.cels.anl.gov/resource_server/sophia/vllm/v1"

# https://github.com/argonne-lcf/inference-endpoints/tree/main eg: "meta-llama/Meta-Llama-3.1-8B-Instruct", or "llama3.1" ... for local ollama
model: "llama3.1" 