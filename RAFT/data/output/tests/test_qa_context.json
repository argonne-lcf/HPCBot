[
    {
        "id": "../data/md/polaris/getting-started.md_0",
        "context": [
            "LINK = $(CC) LIB = LIB += $(CRAY_LIB) SIZE = size ```\n\nWith the appropriate LAMMPS Makefile in place an executable can be compiled as in the following example. Note, per-user limits on the login nodes will reduce the maximum parallelism for compilation. Users are encouraged to compile on a compute node in an interactive session if necessary.\n\ncd lammps-<version>/src make yes-KOKKOS make polaris_nvhpc_kokkos -j 32\n\nRunning Jobs on Polaris\n\nAn example submission script for running a 64-node KOKKOS-enabled LAMMPS executable is below as an example. Additional information on LAMMPS application flags and options is described on the LAMMPS website.\n\n```\n\n!/bin/sh\n\nPBS -l select=64:system=polaris\n\nPBS -l place=scatter\n\nPBS -l walltime=0:15:00\n\nPBS -l filesystems=home:grand:eagle\n\nPBS -q prod\n\nPBS -A Catalyst\n\nexport MPICH_GPU_SUPPORT_ENABLED=1\n\nNNODES=wc -l < $PBS_NODEFILE\n\nper-node settings\n\nNRANKS=4 NDEPTH=8 NTHREADS=1 NGPUS=4\n\nNTOTRANKS=$(( NNODES * NRANKS ))\n\n. ./setup_lammps_gnu.sh\n\nEXE=/home/knight/bin/lmp_polaris_gnu_kokkos EXE_ARG=\"-in in.reaxc.hns -k on g ${NGPUS} -sf kk -pk kokkos neigh half neigh/qeq full newton on \"\n\nOMP settings mostly to quiet Kokkos messages\n\nMPI_ARG=\" -n ${NTOTRANKS} --ppn ${NRANKS} --depth=${NDEPTH} --cpu-bind depth \" OMP_ARG=\" --env OMP_NUM_THREADS=${NTHREADS} --env OMP_PROC_BIND=spread --env OMP_PLACES=cores \"\n\nCOMMAND=\"mpiexec ${MPI_ARG} ${OMP_ARG} ${EXE} ${EXE_ARG}\" echo \"COMMAND= ${COMMAND}\" ${COMMAND} ```\n\nGPU package",
            "CPU affinity and NCCL settings can improve scaling performance, particularly at the largest scales. In particular, we encourage users to try their scaling measurements with the following settings:\n\nSet the environment variable NCCL_COLLNET_ENABLE=1\n\nSet the environment varialbe NCCL_NET_GDR_LEVEL=PHB\n\nManually set the CPU affinity via mpiexec, such as with --cpu-bind verbose,list:0,8,16,24\n\nHorovod and DDP work best when you limit the visible devices to only one GPU. Note that if you import mpi4py or horovod, and then do something like os.environ[\"CUDA_VISIBLE_DEVICES\"] = hvd.local_rank(), it may not actually work! You must set the CUDA_VISIBLE_DEVICES environment variable prior to doing MPI.COMM_WORLD.init(), which is done in horovod.init() as well as implicitly in from mpi4py import MPI. On Polaris specifically, you can use the environment variable PMI_LOCAL_RANK (as well as PMI_LOCAL_SIZE) to learn information about the node-local MPI ranks.\n\nDeepSpeed\n\nDeepSpeed is also available and usable on Polaris. For more information, please see the DeepSpeed documentation directly.\n\nPyTorch DataLoader and multi-node Horovod\n\nPlease note there is a bug that causes a hang when using PyTorch's multithreaded data loaders with distributed training across multiple nodes. To workaround this, NVIDIA recommends setting num_workers=0 in the dataloader configuration, which serializes data loading.\n\nFor more details, see Polaris Known Issues.",
            "Python\n\nWe provide prebuilt conda environments containing GPU-supported builds of torch, tensorflow (both with horovod support for multi-node calculations), jax, and many other commonly-used Python modules.\n\nUsers can activate this environment by first loading the conda module, and then activating the base environment.\n\nExplicitly (either from an interactive job, or inside a job script):\n\nbash module use /soft/modulefiles; module load conda ; conda activate base\n\nThis will load and activate the base environment.\n\nVirtual environments via venv\n\nTo install additional packages that are missing from the base environment, we can build a venv on top of it.\n\n!!! success \"Conda base environment + venv\"\n\nIf you need a package that is **not** already\ninstalled in the `base` environment,\nthis is generally the recommended approach.\n\nWe can create a `venv` on top of the base\nAnaconda environment (with\n`#!bash  --system-site-packaes` to inherit\nthe `base` packaes):\n\n```bash\nmodule load conda; conda activate\nVENV_DIR=\"venvs/polaris\"\nmkdir -p \"${VENV_DIR}\"\npython -m venv \"${VENV_DIR}\" --system-site-packages\nsource \"${VENV_DIR}/bin/activate\"\n```\n\nYou can always retroactively change the #!bash --system-site-packages flag state for this virtual environment by editing #!bash ${VENV_DIR}/pyvenv.cfg and changing the value of the line #!bash include-system-site-packages=false.\n\nTo install a different version of a package that is already installed in the base environment, you can use:",
            "FFmpeg on Polaris\n\nNOTE: FFmpeg module is currently missing on Polaris after a recent upgrade. A spack build of ffmpeg will be available soon\n\nTo use FFmpeg on Polaris first load the corresponding module:\n\nmodule load ffmpeg\n\nThis is a typical command line to create a movie from a series of snapshots in PNG format:\n\nffmpeg -r 15 -i frames.%03d.png -r 25 -pix_fmt yuv420p movie.mp4\n\nwhere:\n\nr 15 is the input frame rate. Experiment with values smaller than the output frame rate for longer movies. -r 25 is the output frame rate (use this value for standard 25 frames per second) -i frames.%03d.png reads the input frames in sequence -pix_fmt yuv420p is needed for movies to play in browsers movie.mp4 is the resulting movie"
        ],
        "golden_context": "Getting Started on Polaris\n\nLogging Into Polaris\n\nTo log into Polaris: ssh <username>@polaris.alcf.anl.gov Then, type in the password from your CRYPTOCard/MobilePASS+ token.\n\nHardware Overview\n\nAn overview of the Polaris system including details on the compute node architecture is available on the Machine Overview page.\n\nCompiling Applications\n\nUsers are encouraged to read through the Compiling and Linking Overview page and corresponding pages depending on the target compiler and programming model.\n\nAccessing Additional Software\n\nIn addition to the Cray PE, ALCF installs software in /soft which can be accessed via module commands by altering your $MODULEPATH: module use /soft/modulefiles The available software can then be queried with module avail.\n\nAdditionally, a suite of software packages are provided via Spack deployments, detailed on the Spack PE page.\n\nSubmitting and Running Jobs\n\nUsers are encouraged to read through the Running Jobs with PBS at the ALCF page for information on using the PBS scheduler and preparing job submission scripts. Some example job submission scripts are available on the Example Job Scripts page as well.\n\nLustre File Striping\n\nIn addition to the content above, here is a document on Lustre File Striping Basics.\n\nLustre File Striping Basics\n\nProxy\n\nIf the node you are on doesnâ€™t have outbound network connectivity, add the following to your ~/.bash_profile file to access the proxy host.\n\n```bash\n\nproxy settings",
        "question": "What is the URL to log into Polaris?",
        "cot_answer": "To find the URL to log into Polaris, we need to analyze the given context. \n\n##begin_quote## To log into Polaris: ssh <username>@polaris.alcf.anl.gov ##end_quote## \n\nThis sentence directly mentions the URL to log into Polaris. The URL is \"polaris.alcf.anl.gov\". \n\nHowever, the URL is not directly used for logging in. Instead, it is used in conjunction with the ssh command. The ssh command is used to connect to the server, and the username and password are used for authentication. \n\nSo, the URL to log into Polaris is not the actual URL used for logging in, but rather the server address used in the ssh command. \n\nTherefore, the URL to log into Polaris is \"polaris.alcf.anl.gov\".\n\n<ANSWER>: polaris.alcf.anl.gov",
        "answer": "polaris.alcf.anl.gov"
    },
    {
        "id": "../data/md/polaris/getting-started.md_0",
        "context": [
            "NNODES=wc -l < $PBS_NODEFILE NRANKS_PER_NODE=16 NDEPTH=4 NTHREADS=1\n\nNTOTRANKS=$(( NNODES * NRANKS_PER_NODE )) echo \"NUM_OF_NODES= ${NNODES} TOTAL_NUM_RANKS= ${NTOTRANKS} RANKS_PER_NODE= ${NRANKS_PER_NODE} THREADS_PER_RANK= ${NTHREADS}\"\n\nmpiexec -n ${NTOTRANKS} --ppn ${NRANKS_PER_NODE} --depth=${NDEPTH} --cpu-bind depth ./hello_affinity ```\n\nCUDA\n\nSeveral variants of C/C++ and Fortran CUDA examples are available here that include MPI and multi-gpu examples.\n\nOne can use the Cray compiler wrappers to compile GPU-enabled applications as well. This example of simple vector addition uses the NVIDIA compilers.\n\nCC -g -O3 -std=c++0x -cuda main.cpp -o vecadd\n\nThe craype-accel-nvidia80 module in the default environment will add the -gpu compiler flag for nvhpc compilers along with appropriate include directories and libraries. It is left to the user to provide an additional flag to the nvhpc compilers to select the target GPU programming model. In this case, -cuda is used to indicate compilation of CUDA code. The application can then be launched within a batch job submission script or as follows on one of the compute nodes.\n\n``` $ ./vecadd\n\nof devices= 4\n\n[0] Platform[ Nvidia ] Type[ GPU ] Device[ NVIDIA A100-SXM4-40GB ] [1] Platform[ Nvidia ] Type[ GPU ] Device[ NVIDIA A100-SXM4-40GB ] [2] Platform[ Nvidia ] Type[ GPU ] Device[ NVIDIA A100-SXM4-40GB ] [3] Platform[ Nvidia ] Type[ GPU ] Device[ NVIDIA A100-SXM4-40GB ] Running on GPU 0! Using single-precision",
            "- 18 passes ==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes ==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes ==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes ==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes ==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes ==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes ==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes ==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes ==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes ==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes ==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes ==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes ==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes ==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes ==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes ==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes ==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes ==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes ==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes ==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes ==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes ==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes ==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18",
            "Cabana\n\nCabana\n\nCabana is built atop Kokkos. It provides class templates useful for implementing particle codes\n\nCabana Documentation\n\nCabana Wiki\n\nCabana github\n\nCabana on Polaris\n\nFollowing the Polaris upgrade to HPCM 1.10, the module setup to use the prebuilt Kokkos changed.\n\nBuilt against the prebuilt Kokkos on polaris, the prebuilt Cabana includes 3 backends: Serial and OpenMP for CPU execution and CUDA for GPU execution. To use it, run\n\nmodule load craype-x86-milan module load craype-accel-nvidia80 module swap PrgEnv-nvhpc PrgEnv-gnu module use /soft/modulefiles module load cuda-PrgEnv-nvidia/12.2.91 module load kokkos cabana\n\nCabana is a headers-only package; there are no actual libraries installed.",
            "CPU affinity and NCCL settings can improve scaling performance, particularly at the largest scales. In particular, we encourage users to try their scaling measurements with the following settings:\n\nSet the environment variable NCCL_COLLNET_ENABLE=1\n\nSet the environment varialbe NCCL_NET_GDR_LEVEL=PHB\n\nManually set the CPU affinity via mpiexec, such as with --cpu-bind verbose,list:0,8,16,24\n\nHorovod and DDP work best when you limit the visible devices to only one GPU. Note that if you import mpi4py or horovod, and then do something like os.environ[\"CUDA_VISIBLE_DEVICES\"] = hvd.local_rank(), it may not actually work! You must set the CUDA_VISIBLE_DEVICES environment variable prior to doing MPI.COMM_WORLD.init(), which is done in horovod.init() as well as implicitly in from mpi4py import MPI. On Polaris specifically, you can use the environment variable PMI_LOCAL_RANK (as well as PMI_LOCAL_SIZE) to learn information about the node-local MPI ranks.\n\nDeepSpeed\n\nDeepSpeed is also available and usable on Polaris. For more information, please see the DeepSpeed documentation directly.\n\nPyTorch DataLoader and multi-node Horovod\n\nPlease note there is a bug that causes a hang when using PyTorch's multithreaded data loaders with distributed training across multiple nodes. To workaround this, NVIDIA recommends setting num_workers=0 in the dataloader configuration, which serializes data loading.\n\nFor more details, see Polaris Known Issues."
        ],
        "golden_context": "Getting Started on Polaris\n\nLogging Into Polaris\n\nTo log into Polaris: ssh <username>@polaris.alcf.anl.gov Then, type in the password from your CRYPTOCard/MobilePASS+ token.\n\nHardware Overview\n\nAn overview of the Polaris system including details on the compute node architecture is available on the Machine Overview page.\n\nCompiling Applications\n\nUsers are encouraged to read through the Compiling and Linking Overview page and corresponding pages depending on the target compiler and programming model.\n\nAccessing Additional Software\n\nIn addition to the Cray PE, ALCF installs software in /soft which can be accessed via module commands by altering your $MODULEPATH: module use /soft/modulefiles The available software can then be queried with module avail.\n\nAdditionally, a suite of software packages are provided via Spack deployments, detailed on the Spack PE page.\n\nSubmitting and Running Jobs\n\nUsers are encouraged to read through the Running Jobs with PBS at the ALCF page for information on using the PBS scheduler and preparing job submission scripts. Some example job submission scripts are available on the Example Job Scripts page as well.\n\nLustre File Striping\n\nIn addition to the content above, here is a document on Lustre File Striping Basics.\n\nLustre File Striping Basics\n\nProxy\n\nIf the node you are on doesnâ€™t have outbound network connectivity, add the following to your ~/.bash_profile file to access the proxy host.\n\n```bash\n\nproxy settings",
        "question": "Where is the architecture of the compute node explained?",
        "cot_answer": "To answer the question \"Where is the architecture of the compute node explained?\", I will follow these steps:\n\n1. Identify the relevant section in the context that mentions the compute node architecture.\n2. Extract the information from that section.\n\nFrom the context, I see that the relevant section is \"Hardware Overview\". Here's what it says:\n\n##begin_quote##\nAn overview of the Polaris system including details on the compute node architecture is available on the Machine Overview page.\n##end_quote##\n\nThis suggests that the Machine Overview page has information about the compute node architecture.\n\nTherefore, to find the explanation of the compute node architecture, I would go to the Machine Overview page.\n\n<ANSWER>: The Machine Overview page.",
        "answer": "The Machine Overview page."
    },
    {
        "id": "../data/md/polaris/getting-started.md_0",
        "context": [
            "2 errors detected in the compilation of \"/home/zippy/smalltests/bool_constant.cpp\". polaris-login-01(~)> ```\n\nyou will need to work around it by loading the latest cudatoolkit module atop PrgEnv-nvhpc:\n\nmodule load cudatoolkit-standalone/11.6.2",
            "Python\n\nWe provide prebuilt conda environments containing GPU-supported builds of torch, tensorflow (both with horovod support for multi-node calculations), jax, and many other commonly-used Python modules.\n\nUsers can activate this environment by first loading the conda module, and then activating the base environment.\n\nExplicitly (either from an interactive job, or inside a job script):\n\nbash module use /soft/modulefiles; module load conda ; conda activate base\n\nThis will load and activate the base environment.\n\nVirtual environments via venv\n\nTo install additional packages that are missing from the base environment, we can build a venv on top of it.\n\n!!! success \"Conda base environment + venv\"\n\nIf you need a package that is **not** already\ninstalled in the `base` environment,\nthis is generally the recommended approach.\n\nWe can create a `venv` on top of the base\nAnaconda environment (with\n`#!bash  --system-site-packaes` to inherit\nthe `base` packaes):\n\n```bash\nmodule load conda; conda activate\nVENV_DIR=\"venvs/polaris\"\nmkdir -p \"${VENV_DIR}\"\npython -m venv \"${VENV_DIR}\" --system-site-packages\nsource \"${VENV_DIR}/bin/activate\"\n```\n\nYou can always retroactively change the #!bash --system-site-packages flag state for this virtual environment by editing #!bash ${VENV_DIR}/pyvenv.cfg and changing the value of the line #!bash include-system-site-packages=false.\n\nTo install a different version of a package that is already installed in the base environment, you can use:",
            "G",
            "g0 g1 g2 g3 g4 g5 g6 g7 g8 g9 x3001-g0 x3005-g1 x3009-g2 x3013-g3 x3101-g4 x3105-g5 x3109-g6 x3201-g7 x3205-g8 x3209-g9 x3002-g0 x3006-g1 x3010-g2 x3014-g3 x3102-g4 x3106-g5 x3110-g6 x3202-g7 x3206-g8 x3210-g9 x3003-g0 x3007-g1 x3011-g2 x3015-g3 x3103-g4 x3107-g5 x3111-g6 x3203-g7 x3207-g8 x3211-g9 x3004-g0 x3008-g1 x3012-g2 x3016-g3 x3104-g4 x3108-g5 x3112-g6 x3204-g7 x3208-g8 x3212-g9"
        ],
        "golden_context": "Getting Started on Polaris\n\nLogging Into Polaris\n\nTo log into Polaris: ssh <username>@polaris.alcf.anl.gov Then, type in the password from your CRYPTOCard/MobilePASS+ token.\n\nHardware Overview\n\nAn overview of the Polaris system including details on the compute node architecture is available on the Machine Overview page.\n\nCompiling Applications\n\nUsers are encouraged to read through the Compiling and Linking Overview page and corresponding pages depending on the target compiler and programming model.\n\nAccessing Additional Software\n\nIn addition to the Cray PE, ALCF installs software in /soft which can be accessed via module commands by altering your $MODULEPATH: module use /soft/modulefiles The available software can then be queried with module avail.\n\nAdditionally, a suite of software packages are provided via Spack deployments, detailed on the Spack PE page.\n\nSubmitting and Running Jobs\n\nUsers are encouraged to read through the Running Jobs with PBS at the ALCF page for information on using the PBS scheduler and preparing job submission scripts. Some example job submission scripts are available on the Example Job Scripts page as well.\n\nLustre File Striping\n\nIn addition to the content above, here is a document on Lustre File Striping Basics.\n\nLustre File Striping Basics\n\nProxy\n\nIf the node you are on doesnâ€™t have outbound network connectivity, add the following to your ~/.bash_profile file to access the proxy host.\n\n```bash\n\nproxy settings",
        "question": "How do you access software in /soft?",
        "cot_answer": "To answer the question, we need to follow these steps:\n\n1.  Identify the location of the software we want to access.\n2.  Understand how to access the software in that location.\n\nFrom the context, we know that the software we want to access is located in /soft. ##begin_quote## In addition to the Cray PE, ALCF installs software in /soft which can be accessed via module commands by altering your $MODULEPATH: module use /soft/modulefiles The available software can then be queried with module avail. ##end_quote##\n\nTo access the software in /soft, we need to alter our $MODULEPATH by running the command \"module use /soft/modulefiles\". This will allow us to query the available software with the command \"module avail\".\n\nTherefore, the step-by-step process to access software in /soft is:\n\n-   Run the command \"module use /soft/modulefiles\" to alter our $MODULEPATH.\n-   Run the command \"module avail\" to query the available software.\n\n<ANSWER>: Run the command \"module use /soft/modulefiles\" to alter our $MODULEPATH, then run the command \"module avail\" to query the available software.",
        "answer": "Run the command \"module use /soft/modulefiles\" to alter our $MODULEPATH, then run the command \"module avail\" to query the available software."
    }
]