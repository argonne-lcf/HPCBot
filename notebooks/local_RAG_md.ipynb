{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "from langchain_community.embeddings.ollama import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.runnables import RunnableParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49/49 [00:07<00:00,  6.19it/s]\n",
      "OllamaEmbeddings: 100%|██████████| 251/251 [00:05<00:00, 45.04it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load all file ends with .md\n",
    "loader = DirectoryLoader('data/md', glob=\"**/*.md\", show_progress=True, loader_cls=UnstructuredMarkdownLoader)\n",
    "# Break documents into smaller chunks, RecursiveCharacterTextSplitter with chunk_size = 2000 is good\n",
    "docs = loader.load_and_split(RecursiveCharacterTextSplitter(chunk_size = 1024, chunk_overlap = 100))\n",
    "# Embedded to local data vector database\n",
    "db  = Chroma.from_documents(docs, OllamaEmbeddings(model=\"mxbai-embed-large\",show_progress=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4}) # Turn VectorStore into a Retriever\n",
    "local_model = \"HPCBot-LLama3-1-8B-with-QA-only\" # [Phi 3, Mistral, Gamma]\n",
    "llm = ChatOllama(model=local_model, temperature = 0) # temperature > 1 more creative, random, temperature < 1 deterministic, repetitive\n",
    "\n",
    "# System prompt\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are an assistant for question-answering tasks. \n",
    "    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \n",
    "    Keep the answer concise <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Question: {question} \n",
    "    Context: {context} \n",
    "    Answer: <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"question\", \"context\"],\n",
    ")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain_from_docs = (\n",
    "    RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"context\"])))\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain_with_source = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ").assign(answer=rag_chain_from_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How can I run a job on Polaris? Write an example job submission script?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:00<00:00, 88.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":// To run a job on Polaris using DeepSpeed, you can follow these steps:\n",
      "\n",
      "1. Load the conda module and activate the base environment by running `module load conda` followed by `conda activate base`.\n",
      "2. Clone or use the provided batch submission script for the example.\n",
      "3. Request an interactive job from `polaris-login` using `qsub -A <project> -q debug-scaling -l select=2 -l walltime=01:00:00 -I`.\n",
      "4. Modify the job parameters as needed, such as the number of processes, nodes, and walltime.\n",
      "5. Save your changes by clicking \"Save Settings\" under the Options menu.\n",
      "\n",
      "For running a DeepSpeed job on Polaris, you can use the following example job submission script:\n",
      "\n",
      "```bash\n",
      "#!/bin/sh\n",
      "\n",
      "PBS -l select=1:system=polaris\n",
      "PBS -l walltime=0:30:00\n",
      "PBS -q debug\n",
      "PBS -A Catalyst\n",
      "PBS -l filesystems=home:eagle\n",
      "\n",
      "cd ${PBS_O_WORKDIR}\n",
      "mpiexec -n 1 ./executable\n",
      "```\n",
      "\n",
      "This script submits the job to one node in the debug queue on Polaris, requesting 30 minutes and the eagle and home filesystems. It will charge project Catalyst for the time.\n",
      "\n",
      "Note: Make sure to replace `./executable` with your actual executable file name.\n",
      "\n",
      "If you have any further questions or need more assistance, please let me know!\n"
     ]
    }
   ],
   "source": [
    "question = \"How can I run a job on Polaris? Write an example job submission script?\"\n",
    "print(question)\n",
    "print(rag_chain.invoke(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How to login to polaris\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:00<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":// To log into Polaris, follow these steps: ssh <username>@polaris.alcf.anl.gov Then, type in the password from your CRYPTOCard/MobilePASS+ token.\n"
     ]
    }
   ],
   "source": [
    "question = \"How to login to polaris\"\n",
    "print(question)\n",
    "print(rag_chain.invoke(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is Gromacs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "://manual.gromacs.org/documentation/2022.1/download.html\n",
      "tar -xzf gromacs-20.2.tar.gz\n",
      "module swap PrgEnv-nvhpc PrgEnv-gnu\n",
      "module load cudatoolkit-standalone/11.2.2\n",
      "module load gcc/10.3.0\n",
      "module load cmake\n",
      "cd gromacs-20.2\n",
      "mkdir build\n",
      "module load gcc/10.3.0\n",
      "module load cmake\n",
      "cd gromacs-20.2\n",
      "mkdir build\n",
      "cmake -DCMAKE_C_COMPILER=cc -DCMAKE_CXX_COMPILER=CC \\ -DBUILD_SHARED_LIBS=OFF -DGMX_BUILD_OWN_FFTW=ON \\ -DCMAKE_INSTALL_PREFIX=/path-to/gromacs-20.2/build \\ -DGMX_MPI=ON -DGMX_OPENMP=ON -DGMX_GPU=CUDA \\ -DCUDA_TOOLKIT_ROOT_DIR=/soft/compilers/cudatoolkit/cuda-11.2.2\n",
      "make –j 8\n",
      "make install\n",
      "The installed binary is build/bin/gmx_mpi.\n",
      "Prebuilt Gromacs binaries can be found in the directory /soft/applications/Gromacs/gromacs-20.2.\n",
      "A sample pbs script follows that will run GROMACS on two nodes, using 4 MPI ranks per node, and each rank with four OpenMP threads. The PME kernel owns one MPI rank and one GPU per node, while the nonbonded kernel uses 3 MPI ranks and 3 GPUs per node.\n",
      "\n",
      "```\n",
      "\n",
      "!/bin/sh\n",
      "PBS -l select=2:system=polaris\n",
      "PBS -l place=scatter\n",
      "PBS -l walltime=0:30:00\n",
      "PBS -q debug\n",
      "PBS -A PROJECT\n",
      "PBS -l filesystems=home:grand:eagle\n",
      "cd ${PBS_O_WORKDIR}\n",
      "cd gromacs-20.2/build\n",
      "mpirun -np 8 ./gmx_mpi -v -npme 1 -ntomp 4 -pme gpu -nb gpu -gpu 3\n",
      "```\n",
      "The answer is:\n",
      " \n",
      "CC=cc CXX=CC CFLAGS=-O3 CXXFLAGS=-O3 F77=gfortran FCFLAGS=-O3 CPPFLAGS=-DGMX_GPU=CUDA F90=gfortran F90FLAGS=-O3 FC=gfortran FFLAGS=-O3\n",
      "LD=gfortran LDFLAGS=-L/usr/lib64 -Wl,-rpath,/usr/lib64 LDLIBS=-lm\n"
     ]
    }
   ],
   "source": [
    "question = \"What is Gromacs\"\n",
    "print(question)\n",
    "print(rag_chain.invoke(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How to use GROMACS on Polaris\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:00<00:00,  1.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "://manual.gromacs.org/documentation/2022.1/download.html\n",
      "tar -xzf gromacs-20\n",
      "\n",
      "module swap PrgEnv-nvhpc PrgEnv-gnu\n",
      "module load cudatoolkit-standalone/11.2.2\n",
      "module load gcc/10.3.0\n",
      "module load cmake\n",
      "cd gromacs-20\n",
      "mkdir build\n",
      "module load gcc/10.3.0\n",
      "module load cmake\n",
      "cd gromacs-20\n",
      "mkdir build\n",
      "cmake -DCMAKE_C_COMPILER=cc -DCMAKE_CXX_COMPILER=CC \\ -DBUILD_SHARED_LIBS=OFF -DGMX_BUILD_OWN_FFTW=ON \\ -DCMAKE_INSTALL_PREFIX=/path-to/gromacs-20\n",
      "make –j 8\n",
      "make install\n",
      "The installed binary is build/bin/gmx_mpi.\n",
      "Prebuilt Gromacs binaries can be found in the directory /soft/applications/Gromacs/gromacs-20\n",
      "A sample pbs script follows that will run GROMACS on two nodes, using 4 MPI ranks per node, and each rank with four OpenMP threads. The PME kernel owns one MPI rank and one GPU per node, while the nonbonded kernel uses 3 MPI ranks and 3 GPUs per node.\n",
      "\n",
      "```\n",
      "\n",
      "!/bin/sh\n",
      "PBS -l select=2:system=polaris\n",
      "PBS -l place=scatter\n",
      "PBS -l walltime=0:30:00\n",
      "PBS -q debug\n",
      "PBS -A PROJECT\n",
      "PBS -l filesystems=home:grand:eagle\n",
      "cd ${PBS_O_WORKDIR}\n",
      "```\n",
      "To run Gromacs on Polaris, you can use the following steps:\n",
      "\n",
      "1. Build your application for Polaris using the provided instructions.\n",
      "2. Submit a job script to Polaris or start an interactive job mode on Polaris as follows:\n",
      "```bash\n",
      "$ qsub -I -l select=1 -l walltime=1:00:00 -l filesystems=home:eagle -q debug -A PROJECT\n",
      "```\n",
      "3. Load the necessary modules and run your application with Nsight Systems as follows:\n",
      "```bash\n",
      "$ nsys profile -o {output_filename} --stats=true ./{your_application}\n",
      "```\n",
      "4. Use ParaView to connect to POLARIS@ANL and select the desired server configuration.\n",
      "5. Run your application on Polaris using the provided sample pbs script.\n",
      "\n",
      "Note: The above steps are a general guide and may need to be modified based on your specific use case and requirements.\n"
     ]
    }
   ],
   "source": [
    "question = \"How to use GROMACS on Polaris\"\n",
    "print(question)\n",
    "print(rag_chain.invoke(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autotrain_venv",
   "language": "python",
   "name": "autotrain_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
