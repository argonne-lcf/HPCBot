{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "from langchain_community.embeddings.ollama import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:03<00:00,  1.24s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 10/10 [00:15<00:00,  1.54s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load all file ends with .md\n",
    "loader = DirectoryLoader('data/', glob=\"*.md\", show_progress=True, loader_cls=UnstructuredMarkdownLoader)\n",
    "# Break documents into smaller chunks, RecursiveCharacterTextSplitter with chunk_size = 2000 is good\n",
    "docs = loader.load_and_split(RecursiveCharacterTextSplitter(chunk_size = 2000, chunk_overlap = 100))\n",
    "# Embedded to local data vector database\n",
    "db  = Chroma.from_documents(docs, OllamaEmbeddings(model=\"mxbai-embed-large\",show_progress=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4}) # Turn VectorStore into a Retriever\n",
    "local_model = \"llama3\" # [Phi 3, Mistral, Gamma]\n",
    "llm = ChatOllama(model=local_model, temperature = 0) # temperature > 1 more creative, random, temperature < 1 deterministic, repetitive\n",
    "\n",
    "# System prompt\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are an assistant for question-answering tasks. \n",
    "    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \n",
    "    Keep the answer concise <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Question: {question} \n",
    "    Context: {context} \n",
    "    Answer: <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"question\", \"context\"],\n",
    ")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How can I run a job on Polaris? Write an example job submission script?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:00<00:00,  8.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To run a job on Polaris, you can use the `qsub` command to submit an interactive or batch job. Here is an example of how to submit an interactive job:\n",
      "\n",
      "```\n",
      "qsub -I -l select=1 -l filesystems=home:eagle -l walltime=1:00:00 -q debug -A <project_name>\n",
      "```\n",
      "\n",
      "This command requests 1 node for a period of 1 hour in the debug queue, requiring access to the `/home` and `eagle` filesystems. After waiting in the queue for a node to become available, a shell prompt on a compute node will appear. You may then start building applications and testing GPU affinity scripts on the compute node.\n",
      "\n",
      "To run a batch job, you can use the following command:\n",
      "\n",
      "```\n",
      "qsub -l select=1 -l filesystems=home:eagle -l walltime=1:00:00 -q <queue_name> -A <project_name>\n",
      "```\n",
      "\n",
      "Replace `<queue_name>` with one of the production queues (e.g. `prod`, `debug`, etc.) and `<project_name>` with your project name.\n",
      "\n",
      "For GPU-based jobs, you can use the following script to assign GPUs to MPI ranks:\n",
      "\n",
      "```bash\n",
      "#!/bin/bash\n",
      "\n",
      "num_gpus=4\n",
      "\n",
      "# Need to assign GPUs in reverse order due to topology\n",
      "# See Polaris Device Affinity Information:\n",
      "# https://www.alcf.anl.gov/support/user-guides/polaris/hardware-overview/machine-overview/index.html\n",
      "\n",
      "# Running Jobs on Polaris\n",
      "# Queues\n",
      "# There are five production queues you can target in your qsub (-q <queue name>):\n",
      "# Queue Name Node Min Node Max Time Min Time Max Notes debug 1 2 5 min 1 hr max 24 nodes in use by this queue ay any given time; Only 8 nodes are exclusive (see Note below) debug-scaling 1 10 5 min 1 hr max 1 job running/accruing/queued per-user prod 10 496 5 min 24 hrs Routing queue; See below preemptable 1 10 5 min 72 hrs Please be aware that jobs in the preemptable queue can be killed at any time if jobs are submitted to the demand queue. Max 20 jobs running/accruing/queued per-project ; see Note below demand 1 56 5 min 1 hr By request only ; max 100 jobs running/accruing/queued per-project\n",
      "\n",
      "# Note: Please be aware that jobs in the preemptable queue can be killed at any time if jobs are submitted to the demand queue.\n",
      "# Jobs in the demand queue take priority over jobs in the preemptable queue.\n",
      "# This means jobs in the preemptable queue may be preempted (killed without any warning) if there are jobs in the demand queue.\n",
      "# Unfortunately, there's always an inherent risk of jobs being killed when using the preemptable queue. \n",
      "# Please use the following command to view details of a queue: qstat -Qf <queuename>\n",
      "\n",
      "# To make your job re-runable add the following PBS directive: #PBS -r y\n",
      "# This will ensure your job will restart once the demand job is complete.\n",
      "\n",
      "# Note: The debug queue has 8 exclusively dedicated nodes.\n",
      "# If there are free nodes in production, then debug jobs can take another 16 nodes for a total of 24.\n",
      "\n",
      "prod is routing queue and routes your job to one of the following six execution queues:\n",
      "Queue Name Node Min Node Max Time Min Time Max Notes small 10 24 5 min 3 hrs medium 25 99 5 min 6 hrs large 100 496 5 min 24 hrs backfill-small 10 24 5 min 3 hrs low priority, negative project balance backfill-medium 25 99 5 min 6 hrs low priority, negative project balance backfill-large 100 496 5 min 24 hrs low priority, negative project balance\n",
      "```\n",
      "\n",
      "This script assigns GPUs to MPI ranks in a round-robin manner. You can modify the `num_gpus` variable to specify the number of GPUs you want to use.\n",
      "\n",
      "To run your job on Polaris, simply execute the script:\n",
      "\n",
      "```bash\n",
      "./set_affinity_gpu_polaris.sh\n",
      "```\n",
      "\n",
      "Make sure to replace `<project_name>` with your actual project name and adjust the queue name as needed.\n"
     ]
    }
   ],
   "source": [
    "question = \"How can I run a job on Polaris? Write an example job submission script?\"\n",
    "print(question)\n",
    "print(rag_chain.invoke(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How to connect to polaris\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:01<00:00,  1.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To connect to Polaris, you can log in using SSH:\n",
      "\n",
      "ssh <username>@polaris.alcf.anl.gov\n",
      "\n",
      "Then, type in your password from your CRYPTOCard/MobilePASS+ token.\n"
     ]
    }
   ],
   "source": [
    "question = \"How to connect to polaris\"\n",
    "print(question)\n",
    "print(rag_chain.invoke(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many queues are there?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:01<00:00,  1.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are five production queues: debug, debug-scaling, prod, preemptable, and demand.\n"
     ]
    }
   ],
   "source": [
    "question = \"How many queues are there?\"\n",
    "print(question)\n",
    "print(rag_chain.invoke(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give me more details on these five production queues\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:01<00:00,  1.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the provided context, there are five production queues on Polaris:\n",
      "\n",
      "1. debug\n",
      "2. prod\n",
      "3. preemptable\n",
      "4. demand\n",
      "5. routing queue (which routes jobs to one of six execution queues: small, medium, large, backfill-small, backfill-medium, and backfill-large)\n"
     ]
    }
   ],
   "source": [
    "question = \"Give me more details on these five production queues\"\n",
    "print(question)\n",
    "print(rag_chain.invoke(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are the time max and node max of these five production queues\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:01<00:00,  1.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, I don't know the time max and node max of these five production queues. The context only provides information about controlling where your job runs, hardware overview, compiling applications, accessing additional software, submitting and running jobs, Lustre file striping, proxy settings, getting assistance, PBS commands, and interactive jobs on compute nodes. It does not mention the time max and node max of specific production queues.\n"
     ]
    }
   ],
   "source": [
    "question = \"What are the time max and node max of these five production queues\"\n",
    "print(question)\n",
    "print(rag_chain.invoke(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is Gromacs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:01<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gromacs is a versatile package to perform molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles. It is primarily designed for biochemical molecules like proteins, lipids, and nucleic acids that have a lot of complicated bonded interactions, but since GROMACS is extremely fast at calculating the nonbonded interactions (that usually dominate simulations) many groups are also using it for research on non-biological systems, e.g. polymers.\n"
     ]
    }
   ],
   "source": [
    "question = \"What is Gromacs\"\n",
    "print(question)\n",
    "print(rag_chain.invoke(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How to use GROMACS on Polaris\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:01<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To use GROMACS on Polaris, you can follow the instructions provided:\n",
      "\n",
      "1. Download the latest source code from http://manual.gromacs.org/documentation/2022.1/download.html\n",
      "2. Unzip the downloaded file using `tar -xzf gromacs-2022.1.tar.gz`\n",
      "3. Load the necessary modules:\n",
      "\t* `module swap PrgEnv-nvhpc PrgEnv-gnu`\n",
      "\t* `module load cudatoolkit-standalone/11.2.2`\n",
      "\t* `module load gcc/10.3.0`\n",
      "\t* `module load cmake`\n",
      "4. Create a build directory and configure GROMACS using:\n",
      "\t* `cmake -DCMAKE_C_COMPILER=cc -DCMAKE_CXX_COMPILER=CC ...`\n",
      "5. Build GROMACS using `make –j 8` and install it using `make install`\n",
      "6. The installed binary is located at `build/bin/gmx_mpi`\n",
      "\n",
      "Alternatively, you can use the prebuilt GROMACS binaries available in `/soft/applications/Gromacs/gromacs-2022.1`. A sample PBS script to run GROMACS on Polaris is also provided:\n",
      "\n",
      "```\n",
      "#!/bin/sh\n",
      "\n",
      "PBS -l select=2:system=polaris\n",
      "\n",
      "PBS -l place=scatter\n",
      "\n",
      "PBS -l walltime=0:30:00\n",
      "\n",
      "PBS -q debug\n",
      "\n",
      "PBS -A PROJECT\n",
      "\n",
      "PBS -l filesystems=home:grand:eagle\n",
      "\n",
      "cd ${PBS_O_WORKDIR}\n",
      "```\n",
      "\n",
      "This script runs GROMACS on two nodes, using 4 MPI ranks per node, and each rank with four OpenMP threads. The PME kernel owns one MPI rank and one GPU per node, while the nonbonded kernel uses 3 MPI ranks and 3 GPUs per node.\n"
     ]
    }
   ],
   "source": [
    "question = \"How to use GROMACS on Polaris\"\n",
    "print(question)\n",
    "print(rag_chain.invoke(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (hackathon)",
   "language": "python",
   "name": "hackathon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
