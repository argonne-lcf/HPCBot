{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "from langchain_community.embeddings.ollama import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.runnables import RunnableParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49/49 [00:08<00:00,  5.88it/s]\n",
      "OllamaEmbeddings: 100%|██████████| 240/240 [09:25<00:00,  2.36s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load all file ends with .md\n",
    "loader = DirectoryLoader('data/polaris', glob=\"**/*.md\", show_progress=True, loader_cls=UnstructuredMarkdownLoader)\n",
    "# Break documents into smaller chunks, RecursiveCharacterTextSplitter with chunk_size = 2000 is good\n",
    "docs = loader.load_and_split(RecursiveCharacterTextSplitter(chunk_size = 1024, chunk_overlap = 100))\n",
    "# Embedded to local data vector database\n",
    "db  = Chroma.from_documents(docs, OllamaEmbeddings(model=\"mxbai-embed-large\",show_progress=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4}) # Turn VectorStore into a Retriever\n",
    "local_model = \"llama3\" # [Phi 3, Mistral, Gamma]\n",
    "llm = ChatOllama(model=local_model, temperature = 0) # temperature > 1 more creative, random, temperature < 1 deterministic, repetitive\n",
    "\n",
    "# System prompt\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are an assistant for question-answering tasks. \n",
    "    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \n",
    "    Keep the answer concise <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Question: {question} \n",
    "    Context: {context} \n",
    "    Answer: <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"question\", \"context\"],\n",
    ")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain_from_docs = (\n",
    "    RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"context\"])))\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain_with_source = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ").assign(answer=rag_chain_from_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How can I run a job on Polaris? Write an example job submission script?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:00<00:00,  4.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To run a job on Polaris, you can use a submission script. Here's an example:\n",
      "\n",
      "```bash\n",
      "#!/bin/bash\n",
      "#PBS -l walltime=24:00:00,nodes=10:ppn=16\n",
      "#PBS -A your_account_name\n",
      "#PBS -q small\n",
      "cd /path/to/your/script\n",
      "./your_script.sh\n",
      "```\n",
      "\n",
      "This script specifies the job requirements (walltime, nodes, and queue), sets the working directory to `/path/to/your/script`, and runs the `your_script.sh` script.\n",
      "\n",
      "Note that you should replace `your_account_name` with your actual account name on Polaris.\n"
     ]
    }
   ],
   "source": [
    "question = \"How can I run a job on Polaris? Write an example job submission script?\"\n",
    "print(question)\n",
    "print(rag_chain.invoke(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How to login to polaris\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:01<00:00,  1.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To login to Polaris, you can follow these steps:\n",
      "\n",
      "1. Go to the Polaris website and click on the \"Login\" button.\n",
      "2. Enter your username and password in the respective fields.\n",
      "3. Click on the \"Log In\" button to access your account.\n",
      "\n",
      "Note: The context provided seems to be related to reviewing Nsight Compute data via GUI, but it does not provide any information about logging into Polaris. If you have any specific questions or concerns about using Polaris, I'll do my best to help.\n"
     ]
    }
   ],
   "source": [
    "question = \"How to login to polaris\"\n",
    "print(question)\n",
    "print(rag_chain.invoke(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is Gromacs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GROMACS is not explicitly mentioned in the provided context, but it appears to be a software package or library used for computational tasks, possibly related to molecular dynamics simulations. The code snippet shows usage of GROMACS-related functions and variables, such as `cblas_dgemm` and `C_cblas`, which suggests that GROMACS is being utilized for matrix operations and linear algebra calculations.\n"
     ]
    }
   ],
   "source": [
    "question = \"What is Gromacs\"\n",
    "print(question)\n",
    "print(rag_chain.invoke(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How to use GROMACS on Polaris\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, it seems that GROMACS is not specifically mentioned as a tool for use on Polaris. However, I can provide some general information on how to run jobs on Polaris.\n",
      "\n",
      "To submit and run jobs on Polaris, you can follow the instructions provided in the \"Submitting and Running Jobs\" section of the context. This includes reading through the Running Jobs with PBS at the ALCF page for information on using the PBS scheduler and preparing job submission scripts.\n",
      "\n",
      "Additionally, if you are trying to use GROMACS specifically, you may want to consider using a job submission script similar to the example provided in the \"Running VASP in Polaris\" section. This would involve loading necessary modules, setting environment variables, and specifying the desired resources (e.g., nodes, walltime) for your job.\n",
      "\n",
      "If you have any further questions or need more specific guidance on running GROMACS on Polaris, I recommend reaching out to the ALCF support team or reviewing the documentation provided by the GROMACS developers.\n"
     ]
    }
   ],
   "source": [
    "question = \"How to use GROMACS on Polaris\"\n",
    "print(question)\n",
    "print(rag_chain.invoke(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (hackathon)",
   "language": "python",
   "name": "hackathon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
