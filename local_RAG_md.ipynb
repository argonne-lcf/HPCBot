{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "from langchain_community.embeddings.ollama import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  2.67it/s]\n",
      "OllamaEmbeddings: 100%|██████████| 8/8 [00:35<00:00,  4.46s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load all file ends with .md\n",
    "loader = DirectoryLoader('data/', glob=\"*.md\", show_progress=True, loader_cls=UnstructuredMarkdownLoader)\n",
    "# Break documents into smaller chunks, RecursiveCharacterTextSplitter with chunk_size = 2000 is good\n",
    "docs = loader.load_and_split(RecursiveCharacterTextSplitter(chunk_size = 2000, chunk_overlap = 100))\n",
    "# Embedded to local data vector database\n",
    "db  = Chroma.from_documents(docs, OllamaEmbeddings(model=\"mxbai-embed-large\",show_progress=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4}) # Turn VectorStore into a Retriever\n",
    "local_model = \"llama3\" # [Phi 3, Mistral, Gamma]\n",
    "llm = ChatOllama(model=local_model, temperature = 0) # temperature > 1 more creative, random, temperature < 1 deterministic, repetitive\n",
    "\n",
    "# System prompt\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are an assistant for question-answering tasks. \n",
    "    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \n",
    "    Keep the answer concise <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Question: {question} \n",
    "    Context: {context} \n",
    "    Answer: <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"question\", \"context\"],\n",
    ")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How can I run a job on Polaris? Write an example job submission script?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:51<00:00, 51.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To run a job on Polaris, you can use the `qsub` command to submit an interactive or batch job. For example, to submit an interactive job that requests one node for a period of one hour in the debug queue, requiring access to the `/home` and `eagle` filesystems, you would use the following command:\n",
      "\n",
      "```\n",
      "qsub -I -l select=1 -l filesystems=home:eagle -l walltime=1:00:00 -q debug -A <project_name>\n",
      "```\n",
      "\n",
      "This will request a node for one hour in the debug queue and provide access to the `/home` and `eagle` filesystems. After waiting in the queue, a shell prompt on a compute node will appear, allowing you to start building applications and testing GPU affinity scripts.\n",
      "\n",
      "As for running multiple MPI applications on a node, you can use the script provided:\n",
      "\n",
      "```\n",
      "gpu=$((${num_gpus} - 1 - ${PMI_LOCAL_RANK} % ${num_gpus}))\n",
      "export CUDA_VISIBLE_DEVICES=$gpu\n",
      "echo “RANK= ${PMI_RANK} LOCAL_RANK= ${PMI_LOCAL_RANK} gpu= ${gpu}”\n",
      "exec \"$@\"\n",
      "```\n",
      "\n",
      "This script can be placed just before the executable in the `mpiexec` command, like so:\n",
      "\n",
      "```\n",
      "mpiexec -n ${NTOTRANKS} --ppn ${NRANKS_PER_NODE} --depth=${NDEPTH} --cpu-bind depth ./set_affinity_gpu_polaris.sh ./hello_affinity\n",
      "```\n",
      "\n",
      "Users with different needs can modify the script to suit their requirements.\n"
     ]
    }
   ],
   "source": [
    "question = \"How can I run a job on Polaris? Write an example job submission script?\"\n",
    "print(question)\n",
    "print(rag_chain.invoke(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How to connect to polaris\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To connect to Polaris, you can log in using the following command:\n",
      "\n",
      "ssh <username>@polaris.alcf.anl.gov\n",
      "Then, type in the password from your CRYPTOCard/MobilePASS+ token.\n"
     ]
    }
   ],
   "source": [
    "question = \"How to connect to polaris\"\n",
    "print(question)\n",
    "print(rag_chain.invoke(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many queues are there?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:56<00:00, 56.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are five production queues you can target in your qsub (-q <queue name>):\n",
      "\n",
      "1. debug\n",
      "2. debug-scaling\n",
      "3. prod\n",
      "4. preemptable\n",
      "5. demand\n"
     ]
    }
   ],
   "source": [
    "question = \"How many queues are there?\"\n",
    "print(question)\n",
    "print(rag_chain.invoke(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give me more details on these five production queues\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:28<00:00, 28.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know the answer to that question. The provided context does not mention five production queues, but rather provides general information about using the Polaris system, including logging in, accessing additional software, submitting and running jobs, and controlling where jobs run. If you're looking for specific information on production queues, I would recommend searching elsewhere or reaching out to support@alcf.anl.gov for assistance.\n"
     ]
    }
   ],
   "source": [
    "question = \"Give me more details on these five production queues\"\n",
    "print(question)\n",
    "print(rag_chain.invoke(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are the time max and node max of these five production queues\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:01<00:00,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It looks like you're trying to get started on the Polaris system! That's a great step.\n",
      "\n",
      "To answer your question, it seems like you're looking for information on how to control where your job runs on Polaris. Here are some key points:\n",
      "\n",
      "* To run a job on specific nodes, use the `-l select` option followed by the node names, separated by `+`. For example: `-l select=1:vnode=<node name1>+1:vnode=<node name2>...`\n",
      "* If you want to control the location of a few nodes but don't care about the rest, use the same syntax as above, but specify the number of nodes and the system (e.g., `system=foo`).\n",
      "* To group nodes by rack or dragonfly group, use the `-l select` option with the `group` specifier. For example: `-l select=8:system=foo,place=scatter:group=tier0`\n",
      "* If you want to run a job on specific racks or dragonfly groups, replace `tier0` with `tier1`.\n",
      "\n",
      "Additionally, here are some general tips for getting started on Polaris:\n",
      "\n",
      "* To log in, use the command `ssh <username>@polaris.alcf.anl.gov` and enter your password from your CRYPTOCard/MobilePASS+ token.\n",
      "* For an overview of the system's hardware architecture, check out the Machine Overview page.\n",
      "* If you need to compile applications, refer to the Compiling and Linking Overview page and corresponding pages for your target compiler and programming model.\n",
      "* To access additional software packages, use the `module` command to alter your `$MODULEPATH`. You can query available software with `module avail`.\n",
      "* For information on submitting and running jobs, check out the Running Jobs with PBS at the ALCF page. Example job submission scripts are also available.\n",
      "\n",
      "Lastly, if you need help or have questions, don't hesitate to reach out to support@alcf.anl.gov. They're there to assist you!\n"
     ]
    }
   ],
   "source": [
    "question = \"What are the time max and node max of these five production queues\"\n",
    "print(question)\n",
    "print(rag_chain.invoke(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (hackathon)",
   "language": "python",
   "name": "hackathon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
