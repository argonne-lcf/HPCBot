{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from openai import OpenAI\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunk(path = \"data\", type = \"md\", chunk_size = 1500, chunk_overlap = 100):\n",
    "\n",
    "    if type == \"md\":\n",
    "        # Load all file ends with .md\n",
    "        loader = DirectoryLoader(path, glob=\"**/[!.]*.md\", loader_cls=UnstructuredMarkdownLoader)\n",
    "    elif type == \"pdf\":\n",
    "        loader = PyPDFDirectoryLoader(path)\n",
    "    else:\n",
    "        raise TypeError(\"Only accept pdf and md\")\n",
    "    \n",
    "    chunks = loader.load_and_split(RecursiveCharacterTextSplitter(chunk_size = chunk_size, chunk_overlap = chunk_overlap))\n",
    "    chunks = [(chunk.page_content, chunk.metadata['source']) for chunk in chunks]\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def generate_questions(chunk, num = 3, model = \"llama3\"):\n",
    "    \"\"\"\n",
    "    Generates `num` questions / use cases for `chunk`. Used when the input document is of general types \n",
    "    \"\"\"\n",
    "    messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a synthetic question-answer pair generator. Given a chunk of context about some topic(s), generate %s example questions a user could ask and that question could be able to answer using information from the chunk. For example, if the given context has information about supercomputer, an example question could be 'What is a supercomputer?'\" % (num)},\n",
    "                {\"role\": \"system\", \"content\": \"The questions should be able to be answered in a few words or less. Show the example questions in numbered list. Every questions MUST end with a question mark\"},\n",
    "                {\"role\": \"user\", \"content\": str(chunk)}\n",
    "            ]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=messages\n",
    "    )\n",
    "    queries = response.choices[0].message.content.split('\\n')\n",
    "\n",
    "    # Only include questions\n",
    "    queries = [q for q in queries if q.endswith(\"?\") and not (q.startswith(\"You are a synthetic\"))]\n",
    "\n",
    "    return [re.sub(r'^[\\d+\\.|*+\\.]+\\s', '', q) for q in queries] # If questions start with numbers or stars, remove them.\n",
    "    \n",
    "\n",
    "def encode_question(question, chunk):\n",
    "    \"\"\"\n",
    "    Encode multiple prompt instructions into a single string.\n",
    "    \"\"\"\n",
    "    \n",
    "    prompts = []\n",
    "        \n",
    "    prompt = \"\"\"\n",
    "        Question: {question}\\nContext: {context}\\n\n",
    "        Answer this question using the information given in the context above. Here is things to pay attention to: \n",
    "        - First provide step-by-step reasoning on how to answer the question. \n",
    "        - In the reasoning, if you need to copy paste some sentences from the context, include them in ##begin_quote## and ##end_quote##. This would mean that things outside of ##begin_quote## and ##end_quote## are not directly copy paste from the context. \n",
    "        - End your response with final answer in the form <ANSWER>: $answer, the answer should be succinct.\n",
    "        You MUST begin your final answer with the tag \"<ANSWER>:\".\n",
    "    \"\"\".format(question=question, context=str(chunk))\n",
    "    prompts.append({\"role\": \"system\", \"content\": \"You are a helpful question answerer who can provide an answer given a question and relevant context.\"})\n",
    "    prompts.append({\"role\": \"user\", \"content\": prompt})\n",
    "    return prompts\n",
    "\n",
    "def generate_label(question, chunk, model = \"llama3\"):\n",
    "    \"\"\"\n",
    "    Generates the label / answer to `question` using `context`.\n",
    "    \"\"\"\n",
    "    question = encode_question(question, chunk)\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=question,\n",
    "        n=1,\n",
    "        temperature=0\n",
    "    )\n",
    "    response = response.choices[0].message.content\n",
    "    return response\n",
    "\n",
    "def run(i, chunks, chunk, source, num = 3, num_distract = 4, p = 0.8, model = \"llama3\"):\n",
    "    \"\"\"\n",
    "    Given a chunk, create {Q, A, D} triplets and add them to the dataset.\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    qs = generate_questions(chunk, num, model)\n",
    "    for j, q in enumerate(qs):\n",
    "        datapt = {\n",
    "            \"id\": None,\n",
    "            \"context\": None,\n",
    "            \"golden_context\": None,\n",
    "            \"question\": None,\n",
    "            \"cot_answer\": None\n",
    "        }\n",
    "\n",
    "        datapt[\"id\"] = f\"{source}_seed_task_{i}_{j}\"\n",
    "        datapt[\"question\"] = q\n",
    "\n",
    "        # add num_distract distractor docs\n",
    "        docs = [chunk]\n",
    "        indices = list(range(0, len(chunks)))\n",
    "        indices.remove(i)\n",
    "        for k in random.sample(indices, num_distract):\n",
    "            docs.append(chunks[k])\n",
    "            \n",
    "        # decides whether to keep golden document\n",
    "        golden = random.uniform(0, 1) < p\n",
    "        if not golden:\n",
    "            docs[0] = chunks[random.sample(indices, 1)[0]]\n",
    "        random.shuffle(docs)\n",
    "\n",
    "        datapt[\"context\"] = docs\n",
    "        datapt[\"golden_context\"] = chunk\n",
    "\n",
    "        # add answer to q\n",
    "        datapt[\"cot_answer\"] = generate_label(q, chunk, model=model) \n",
    "\n",
    "        res.append(datapt)\n",
    "    return res        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZE = 1500\n",
    "NUM_DISTRACT_DOCS = 4\n",
    "CHUNK_OVERLAP = 100\n",
    "NUM_QUESTION = 3\n",
    "P = 0.8 # chance of including golden document in training set\n",
    "MODEL = \"llama3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init OpenAI client\n",
    "client = OpenAI(\n",
    "    base_url = 'http://localhost:11434/v1', # remove this line if using gpt\n",
    "    api_key='ollama', # [ollama, OPENAI_API_KEY] local LLM or using gpt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "path = \"data/md/polaris\"\n",
    "type = \"md\"\n",
    "chunks = get_chunk(path, type, CHUNK_SIZE, CHUNK_OVERLAP)\n",
    "\n",
    "# generate questions, answer and 4 distract documents\n",
    "for i, (chunk, source) in enumerate(chunks):\n",
    "    data.extend(run(i, chunks, chunk, source))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data to a json file\n",
    "out_path = 'output/QA_polaris_md_1500.json'\n",
    "with open(out_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6a3a50cfa768e55079b834244adbc23d62a4b24bbd2f8b443e30bd8c79d3f7ea"
  },
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
