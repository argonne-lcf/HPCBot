[
    {
        "id": "data/md/polaris/getting-started.md_seed_task_1_0",
        "context": [
            [
                "Getting Started on Polaris\n\nLogging Into Polaris\n\nTo log into Polaris:\nssh <username>@polaris.alcf.anl.gov\nThen, type in the password from your CRYPTOCard/MobilePASS+ token.\n\nHardware Overview\n\nAn overview of the Polaris system including details on the compute node architecture is available on the Machine Overview page.\n\nCompiling Applications\n\nUsers are encouraged to read through the Compiling and Linking Overview page and corresponding pages depending on the target compiler and programming model.\n\nAccessing Additional Software\n\nIn addition to the Cray PE, ALCF installs software in /soft which can be\naccessed via module commands by altering your $MODULEPATH:\nmodule use /soft/modulefiles\nThe available software can then be queried with module avail.\n\nAdditionally, a suite of software packages are provided via Spack deployments,\ndetailed on the Spack PE\npage.\n\nSubmitting and Running Jobs\n\nUsers are encouraged to read through the Running Jobs with PBS at the ALCF page for information on using the PBS scheduler and preparing job submission scripts. Some example job submission scripts are available on the Example Job Scripts page as well.\n\nLustre File Striping\n\nIn addition to the content above, here is a document on Lustre File Striping Basics.\n\nLustre File Striping Basics\n\nProxy\n\nIf the node you are on doesn’t have outbound network connectivity, add the following to your ~/.bash_profile file to access the proxy host.\n\n```bash\n\nproxy settings",
                "data/md/polaris/getting-started.md"
            ],
            [
                "NVIDIA Compilers on Polaris\n\nThe NVIDIA compilers (nvc, nvc++, nvcc, and nvfortran) are available on Polaris via the PrgEnv-nvhpc and nvhpc modules. There is currently a PrgEnv-nvidia module available, but that will soon be deprecated in Cray's PE, thus it is not recommend for use.\n\nThe Cray compiler wrappers map to NVIDIA compilers as follows.\n\ncc -> nvc\nCC -> nvc++\nftn -> nvfortran\n\nUsers are encouraged to look through NVIDIA's documentation for the NVHPC SDK and specific information on the compilers, tools, and libraries.\n\nNotes on NVIDIA Compilers\n\nPGI compilers\n\nThe NVIDIA programming environments makes available compilers from the NVIDIA HPC SDK. While the PGI compilers are available in this programming environment, it should be noted they are actually symlinks to the corresponding NVIDIA compilers.\npgcc -> nvc\npgc++ -> nvc++\npgf90 -> nvfortran\npgfortran -> nvfortran\nWhile nvcc is the traditional CUDA C and CUDA C++ compiler for NVIDIA GPUs, the nvc, nvc++, and nvfortran compilers additionally target CPUs.\n\nNVHPC SDK Directory Structure\n\nUsers migrating from CUDA toolkits to the NVHPC SDK may find it beneficial to review the directory structure of the hpc-sdk directory to find the location of commonly used libraries (including math libraries for the CPU). With the PrgEnv-nvhpc module loaded, the NVIDIA_PATH environment variable can be used to locate the path to various NVIDIA tools, libraries, and examples.\n\ncompiler/bin - cuda-gdb, ncu, nsys, ...",
                "data/md/polaris/compiling-and-linking/nvidia-compiler-polaris.md"
            ],
            [
                "bash\nqsub -I -A <Project> -q debug -l select=1 -l walltime=01:00:00 -l filesystems=home:eagle -l singularity_fakeroot=true\nexport HTTP_PROXY=http://proxy.alcf.anl.gov:3128\nexport HTTPS_PROXY=http://proxy.alcf.anl.gov:3128\nexport http_proxy=http://proxy.alcf.anl.gov:3128\nexport https_proxy=http://proxy.alcf.anl.gov:3128\nmodule use /soft/spack/gcc/0.6.1/install/modulefiles/Core\nmodule load apptainer\napptainer build --fakeroot pytorch:22.06-py3.sing docker://nvcr.io/nvidia/pytorch:22.06-py3\nYou can find the latest prebuilt Nvidia PyTorch containers here.  The Tensorflow containers are here (though note that LCF doesn't prebuild the TF-1 containers typically).  You can search the full container registry here. For custom containers tailored for Polaris, visit ALCF's GitHub container registry\n\nNote: Currently container build and executions are only supported on the Polaris compute nodes\n\nRecipe-Based Container Building\n\nAs mentioned earlier, you can build Apptainer containers from recipe files. Instructions are available here. See available containers for more recipes.\n\nNote: You can also build custom recipes by bootstrapping from prebuilt images. For e.g the first two lines in a recipe to use our custom Tensorflow implementation would be Bootstrap: oras followed by From: ghcr.io/argonne-lcf/tf2-mpich-nvidia-gpu:latest\n\nRunning Containers on Polaris",
                "data/md/polaris/data-science-workflows/containers/containers.md"
            ],
            [
                "delete[] A;\n  delete[] B;\n  delete[] C_cblas;\n  delete[] C_host_onemkl;\n  sycl::free(A_dev, q);\n  sycl::free(B_dev, q);\n  sycl::free(C_dev_onemkl, q);\n  return result_cblas;\n}\n```\n\nCompile and Run\n\nThe user would need to provide paths the math-libraris as shown below. Also please provide AOCL library for CPU GEMM by module load aocl.\nEnvironment variables MKLROOT is defined with oneapi module & AOCL_ROOT is defined with aocl module.\nNote: Please pay attention to the linker options for AOCL & oneMKL libraries.\nbash\n$ clang++ -std=c++17 -sycl-std=2020 -O3 -fsycl -fsycl-targets=nvptx64-nvidia-cuda -Xsycl-target-backend --cuda-gpu-arch=sm_80 -L$AOCL_ROOT/lib -lblis -L$MKLROOT/lib -lonemkl sycl_onemkl_gemm.cpp -o sycl_onemkl_gemm.out",
                "data/md/polaris/programming-models/sycl-polaris.md"
            ],
            [
                "13. Using Color to Differentiate Data\n\nTo enable us to distinguish these two types of data from one other, we can vary their representations.\n\nOne way to do this is by setting the color of the two data sets to different colors. Repeat this process for each of rbc.000 and bad_rbc.000, picking different colors.\n\nSelect one of the rbc data sets in the Pipeline Browser\n\nGo to the Displaytab in the Object Inspector\n\nIn the Color by:dropdown select Solid Color\n\nClick on the Set Solid Color... button\n\nSelect a color from the Select Colordialog that appears\n\nRepeat for the other RBC data set, choosing a different color\n\n14. Further Exploration: Highlight the Mesh\n\nChange the representation of one of the RBC data sets.\n\nIn this example, the continuum.000* data is also hidden to reduce confusion with showing multiple overlapping meshes.\n\nSelect on of the RBC data sets\n\nGo to the Displaytab in the Object Inspector\n\nFor the Representationselect Surface With Edges\n\nIn the Edge Style section click on the Set Edge Color...button to select a different color from the Select Color dialog\n\n15. Further Exploration: Highlight the Vertices\n\nAdd glyphs to illustrate the position of the vertices of one of the RBC data sets.\n\nSelect one of the RBC data sets\n\nSelect the Glyphfilter\n\nSince this filter was used recently, can also be found under: Filters->Recent->Glyph\n\nAs in the earlier example, set the various configuration options for the glyph attributes",
                "data/md/polaris/visualization/paraview-tutorial.md"
            ]
        ],
        "golden_context": "```bash\n\nproxy settings\n\nexport HTTP_PROXY=\"http://proxy.alcf.anl.gov:3128\"\nexport HTTPS_PROXY=\"http://proxy.alcf.anl.gov:3128\"\nexport http_proxy=\"http://proxy.alcf.anl.gov:3128\"\nexport https_proxy=\"http://proxy.alcf.anl.gov:3128\"\nexport ftp_proxy=\"http://proxy.alcf.anl.gov:3128\"\nexport no_proxy=\"admin,polaris-adminvm-01,localhost,.cm.polaris.alcf.anl.gov,polaris-,.polaris.alcf.anl.gov,.alcf.anl.gov\"\n```\n\nGetting Assistance\n\nPlease direct all questions, requests, and feedback to support@alcf.anl.gov.",
        "question": "What are proxy settings used for?",
        "cot_answer": "To answer this question, I will follow these steps:\n\n1. Identify the purpose of proxy settings based on the given context.\n\n##begin_quote##\nGetting Assistance\nPlease direct all questions, requests, and feedback to support@alcf.anl.gov.\n##end_quote##\n\nThe provided context does not explicitly state the purpose of proxy settings. However, by examining the export commands for HTTP_PROXY, HTTPS_PROXY, http_proxy, https_proxy, and ftp_proxy, it can be inferred that these settings are used to configure proxies for various protocols (HTTP, HTTPS, and FTP).\n\n2. Analyze the export commands to determine what specific information they provide.\n\nThe export commands set environment variables with proxy server addresses and ports. For example, `export HTTP_PROXY=\"http://proxy.alcf.anl.gov:3128\"` sets the HTTP_PROXY variable to a specific proxy server address and port.\n\n3. Consider the purpose of these environment variables in relation to the protocols they are associated with.\n\nThe export commands likely set up proxies for various network requests made using the corresponding protocols (HTTP, HTTPS, FTP). This could be useful in situations where direct access to external resources is not possible or desired.\n\n4. Identify any exceptions or exclusions mentioned in the context.\n\n##begin_quote##\nexport no_proxy=\"admin,polaris-adminvm-01,localhost,.cm.polaris.alcf.anl.gov,polaris-,.polaris.alcf.anl.gov,.alcf.anl.gov\"\n##end_quote##\n\nThe `no_proxy` export command specifies a list of domains or hosts that should not use the proxy settings. This could be useful for accessing internal resources or avoiding proxy usage for specific requests.\n\n5. Summarize the findings to provide an answer to the question.\n\nBased on the analysis, it can be concluded that proxy settings are used to configure proxies for various protocols (HTTP, HTTPS, and FTP) and exclude certain domains or hosts from using these proxies.\n\n<ANSWER>: Proxy settings are used to configure proxies for various protocols (HTTP, HTTPS, and FTP)."
    },
    {
        "id": "data/md/polaris/getting-started.md_seed_task_1_1",
        "context": [
            [
                "Kokkos\n\nKokkos\n\nKokkos Core implements a programming model in C++ for writing performance\nportable applications targeting all major HPC platforms. For that purpose it\nprovides abstractions for both parallel execution of code and data\nmanagement. Kokkos is designed to target complex node architectures with\nN-level memory hierarchies and multiple types of execution resources. It\ncurrently can use Serial and OpenMP (threads) for CPU execution spaces\n(\"backends\") and CUDA, HIP, SYCL, and OpenMPTarget for GPU execution\nspaces. By convention, Kokkos only allows one GPU backend at a time.\n\nKokkos Documentation\n\nKokkos-core Wiki\n\nKokkos github\n\nKokkos on Polaris\n\nFollowing the Polaris upgrade to HPCM\n1.10,\nthe module setup to use the prebuilt Kokkos changed.\n\nThe prebuilt Kokkos on polaris includes 3 backends: Serial and OpenMP for CPU\nexecution and CUDA for GPU execution. To use it, run\n\nmodule load craype-x86-milan\nmodule load craype-accel-nvidia80\nmodule swap PrgEnv-nvhpc PrgEnv-gnu\nmodule use /soft/modulefiles\nmodule load cuda-PrgEnv-nvidia/12.2.91\nmodule load kokkos\n\nThis sets the following environment variables, some of which are used by\ncmake:\n\nKOKKOS_HOME - path to the lib64/, include/ files installed\n\nLIBRARY_PATH - prepends $KOKKOS_HOME/lib64 to this variable used by cmake\n\nCPATH - prepends $KOKKOS_HOME/include to this variable used by cmake\n\nLD_LIBRARY_PATH - prepends $KOKKOS_HOME/lib64 to this variable\n\nBuilding a Kokkos Application Using cmake",
                "data/md/polaris/programming-models/kokkos-polaris.md"
            ],
            [
                "Name= NVIDIA A100-SXM4-40GB\n  Locally unique identifier= \n  Clock Frequency(KHz)= 1410000\n  Compute Mode= 0\n  Major compute capability= 8\n  Minor compute capability= 0\n  Number of multiprocessors on device= 108\n  Warp size in threads= 32\n  Single precision performance ratio= 2\n\nResult is CORRECT!! :)\n```\n\nGPU OpenACC\n\nA simple MPI-parallel OpenACC example is available here. Compilation proceeds similar to the above CUDA example except for the use of the -acc=gpu compiler flag to indicate compilation of OpenACC code for GPUs.\nCC -g -O3 -std=c++0x -acc=gpu -gpu=cc80,cuda11.0 main.cpp -o vecadd\nIn this example, each MPI rank sees all four GPUs on a Polaris node and GPUs are bound to MPI ranks round-robin within the application.\n\n```\n$ mpiexec -n 4 ./vecadd\n\nof devices= 4\n\nUsing single-precision\n\nRank 0 running on GPU 0!\nRank 1 running on GPU 1!\nRank 2 running on GPU 2!\nRank 3 running on GPU 3!\n\nResult is CORRECT!! :)\n``\nIf the application instead relies on the job launcher to bind MPI ranks to available GPUs, then a small helper script can be used to explicitly setCUDA_VISIBLE_DEVICES` appropriately for each MPI rank. One example is available here where each MPI rank is similarly bound to a single GPU with round-robin assignment. The binding of MPI ranks to GPUs is discussed in more detail here.\n\nGPU OpenCL",
                "data/md/polaris/compiling-and-linking/polaris-example-program-makefile.md"
            ],
            [
                "Cloning the base Anaconda environment\n\n!!! warning\n\nIf you need more flexibility, you can clone the conda environment into a custom\npath, which would then allow for root-like installations via #!bash conda install\n<module> or #!bash pip install <module>.\n\nUnlike the venv approach, using a cloned Anaconda environment requires you to\ncopy the entirety of the base environment, which can use significant storage\nspace.\n\nTo clone the base environment:\n\nbash\nmodule load conda ; conda activate base\nconda create --clone base --prefix /path/to/envs/base-clone\nconda activate /path/to/envs/base-clone\n\nwhere, #!bash path/to/envs/base-clone should be replaced by a suitably chosen\npath.\n\nNote: The cloning process can be quite slow.\n\nUsing pip install --user (not recommended)\n\n!!! danger\n\nWith the conda environment setup, one can install common Python modules using\n#!bash python3 pip install --users '<module-name>' which will install\npackages in #!bash $PYTHONUSERBASE/lib/pythonX.Y/site-packages.\n\nThe #!bash $PYTHONUSERBASE environment variable is automatically set when you\nload the base conda module, and is equal to  #!bash\n/home/$USER/.local/polaris/conda/YYYY-MM-DD.\n\nNote, Python modules installed this way that contain command line binaries will\nnot have those binaries automatically added to the shell's #!bash $PATH. To\nmanually add the path:\n\nbash\nexport PATH=\"$PYTHONUSERBASE/bin:$PATH\"",
                "data/md/polaris/data-science-workflows/python.md"
            ],
            [
                "|    0    0    0      17480      C   ./saxpy                          8413MiB |\n|    0    0    1      17481      C   ./saxpy                          8363MiB |\n|    1    0    0      17482      C   ./saxpy                          8413MiB |\n|    1    0    1      17483      C   ./saxpy                          8363MiB |\n|    2    1    0      17484      C   ./saxpy                          8313MiB |\n|    2    1    1      17485      C   ./saxpy                          8313MiB |\n|    2    5    0      17486      C   ./saxpy                          8313MiB |\n|    3    1    0      17487      C   ./saxpy                          8313MiB |\n|    3    1    1      17488      C   ./saxpy                          8313MiB |\n|    3    5    0      17489      C   ./saxpy                          8313MiB |\n+-----------------------------------------------------------------------------+\nascovel@x3209c0s19b0n0:~>\n```",
                "data/md/polaris/workflows/mig-compute.md"
            ],
            "```bash\n\nproxy settings\n\nexport HTTP_PROXY=\"http://proxy.alcf.anl.gov:3128\"\nexport HTTPS_PROXY=\"http://proxy.alcf.anl.gov:3128\"\nexport http_proxy=\"http://proxy.alcf.anl.gov:3128\"\nexport https_proxy=\"http://proxy.alcf.anl.gov:3128\"\nexport ftp_proxy=\"http://proxy.alcf.anl.gov:3128\"\nexport no_proxy=\"admin,polaris-adminvm-01,localhost,.cm.polaris.alcf.anl.gov,polaris-,.polaris.alcf.anl.gov,.alcf.anl.gov\"\n```\n\nGetting Assistance\n\nPlease direct all questions, requests, and feedback to support@alcf.anl.gov."
        ],
        "golden_context": "```bash\n\nproxy settings\n\nexport HTTP_PROXY=\"http://proxy.alcf.anl.gov:3128\"\nexport HTTPS_PROXY=\"http://proxy.alcf.anl.gov:3128\"\nexport http_proxy=\"http://proxy.alcf.anl.gov:3128\"\nexport https_proxy=\"http://proxy.alcf.anl.gov:3128\"\nexport ftp_proxy=\"http://proxy.alcf.anl.gov:3128\"\nexport no_proxy=\"admin,polaris-adminvm-01,localhost,.cm.polaris.alcf.anl.gov,polaris-,.polaris.alcf.anl.gov,.alcf.anl.gov\"\n```\n\nGetting Assistance\n\nPlease direct all questions, requests, and feedback to support@alcf.anl.gov.",
        "question": "How do I direct my requests and feedback in ALCF?",
        "cot_answer": "To direct requests and feedback in ALCF, I will follow these steps:\n\n1. Identify the designated email address for requesting assistance: ##begin_quote##Please direct all questions, requests, and feedback to support@alcf.anl.gov.##end_quote##\n2. Since this is a specific instruction provided by ALCF, it's clear that any requests or feedback should be sent to the specified email address.\n\nFinal Answer:\n<ANSWER>: $support@alcf.anl.gov"
    },
    {
        "id": "data/md/polaris/getting-started.md_seed_task_1_2",
        "context": [
            [
                "Kokkos\n\nKokkos\n\nKokkos Core implements a programming model in C++ for writing performance\nportable applications targeting all major HPC platforms. For that purpose it\nprovides abstractions for both parallel execution of code and data\nmanagement. Kokkos is designed to target complex node architectures with\nN-level memory hierarchies and multiple types of execution resources. It\ncurrently can use Serial and OpenMP (threads) for CPU execution spaces\n(\"backends\") and CUDA, HIP, SYCL, and OpenMPTarget for GPU execution\nspaces. By convention, Kokkos only allows one GPU backend at a time.\n\nKokkos Documentation\n\nKokkos-core Wiki\n\nKokkos github\n\nKokkos on Polaris\n\nFollowing the Polaris upgrade to HPCM\n1.10,\nthe module setup to use the prebuilt Kokkos changed.\n\nThe prebuilt Kokkos on polaris includes 3 backends: Serial and OpenMP for CPU\nexecution and CUDA for GPU execution. To use it, run\n\nmodule load craype-x86-milan\nmodule load craype-accel-nvidia80\nmodule swap PrgEnv-nvhpc PrgEnv-gnu\nmodule use /soft/modulefiles\nmodule load cuda-PrgEnv-nvidia/12.2.91\nmodule load kokkos\n\nThis sets the following environment variables, some of which are used by\ncmake:\n\nKOKKOS_HOME - path to the lib64/, include/ files installed\n\nLIBRARY_PATH - prepends $KOKKOS_HOME/lib64 to this variable used by cmake\n\nCPATH - prepends $KOKKOS_HOME/include to this variable used by cmake\n\nLD_LIBRARY_PATH - prepends $KOKKOS_HOME/lib64 to this variable\n\nBuilding a Kokkos Application Using cmake",
                "data/md/polaris/programming-models/kokkos-polaris.md"
            ],
            [
                "Prebuilt Gromacs binaries can be found in the directory /soft/applications/Gromacs/gromacs-2022.1.\n\nA sample pbs script follows that will run GROMACS on two nodes, using 4 MPI ranks per node, and each rank with four OpenMP threads. The PME kernel owns one MPI rank and one GPU per node, while the nonbonded kernel uses 3 MPI ranks and 3 GPUs per node.\n\n```\n\n!/bin/sh\n\nPBS -l select=2:system=polaris\n\nPBS -l place=scatter\n\nPBS -l walltime=0:30:00\n\nPBS -q debug\n\nPBS -A PROJECT\n\nPBS -l filesystems=home:grand:eagle\n\ncd ${PBS_O_WORKDIR}\n\nmodule swap PrgEnv-nvhpc PrgEnv-gnu\nmodule load cudatoolkit-standalone/11.2.2\n\nexport OMP_NUM_THREADS=4\n\nmpirun --np 8 /soft/applications/Gromacs/gromacs-2022.1/gmx_mpi \\\n      mdrun -gputasks 0123 -nb gpu -pme gpu -npme 1 -ntomp 4 \\\n      -dlb yes -resethway -pin on -v deffnm step5_1 -g test.log\n```\n\nWe strongly suggest that users try combinations of different numbers of nodes, MPI ranks per node, number of GPU tasks/devices, GPU task decomposition between nonbonded and PME kernels, and OMP threads per rank to find the optimal throughput for their particular workload.",
                "data/md/polaris/applications-and-libraries/applications/gromacs.md"
            ],
            "```bash\n\nproxy settings\n\nexport HTTP_PROXY=\"http://proxy.alcf.anl.gov:3128\"\nexport HTTPS_PROXY=\"http://proxy.alcf.anl.gov:3128\"\nexport http_proxy=\"http://proxy.alcf.anl.gov:3128\"\nexport https_proxy=\"http://proxy.alcf.anl.gov:3128\"\nexport ftp_proxy=\"http://proxy.alcf.anl.gov:3128\"\nexport no_proxy=\"admin,polaris-adminvm-01,localhost,.cm.polaris.alcf.anl.gov,polaris-,.polaris.alcf.anl.gov,.alcf.anl.gov\"\n```\n\nGetting Assistance\n\nPlease direct all questions, requests, and feedback to support@alcf.anl.gov.",
            [
                "Julia packages: \n- CUDA: 5.3.3\n- CUDA_Driver_jll: 0.8.1+0\n- CUDA_Runtime_jll: 0.12.1+0\n- CUDA_Runtime_Discovery: 0.2.4\n\nToolchain:\n- Julia: 1.10.3\n- LLVM: 15.0.7\n\nPreferences:\n- CUDA_Runtime_jll.local: true\n\n4 devices:\n  0: NVIDIA A100-SXM4-40GB (sm_80, 39.390 GiB / 40.000 GiB available)\n  1: NVIDIA A100-SXM4-40GB (sm_80, 39.390 GiB / 40.000 GiB available)\n  2: NVIDIA A100-SXM4-40GB (sm_80, 39.390 GiB / 40.000 GiB available)\n  3: NVIDIA A100-SXM4-40GB (sm_80, 39.390 GiB / 40.000 GiB available)\n\nWarning messages from the presence of CUDA in LD_LIBRARY_PATH were ommitted in output of the first two commands. In this case, the artifact and local installation are similar. If there was a difference, then the local installation should be preferred.\n\nIn case you want write portable GPU kernels we highly recommend the KernelAbstractions.jl package. It provides a high-level abstraction for writing GPU kernels that can be compiled for different GPU backends.\n\njulia --project -e 'using Pkg; Pkg.add(\"KernelAbstractions\")'\n\nBy loading either oneAPI.jl, AMDGPU.jl, or CUDA.jl (see quickstart guide below).\n\nCUDA-aware MPI\n\nMPI.jl supports CUDA-aware MPI. This is enabled by setting the following environment variables.\n\nbash\nexport JULIA_CUDA_MEMORY_POOL=none\nexport MPICH_GPU_SUPPORT_ENABLED=1\nexport JULIA_MPI_PATH=${CRAY_MPICH_DIR}\nexport JULIA_MPI_HAS_CUDA=1\n\nNote that MPI.jl needs to be rebuilt for the changes to take effect.\n\nbash\njulia --project -e 'using Pkg; Pkg.build(\"MPI\")'",
                "data/md/polaris/data-science-workflows/julia.md"
            ],
            [
                "Higher-level programming models such as Kokkos and Raja may also be used for GPU programming.\n\nOpenCL is supported, but does not require specific compiler flags per-se as the offloaded kernels are just-in-time compiled. Abstraction programming models, such as Kokkos, can be built on top of some of these programming models (see below).\n\nA HIP compiler supporting the A100 GPUs is still to be installed on Polaris.\n\nMapping Programming Models to Polaris Modules\n\nThe table below offers some suggestions for how to get started setting up your environment on Polaris depending on the programming language and model. Note, mixed C/C++ and Fortran applications should choose the programming environment for the Fortran compiler because of mpi.mod and similar incompatibilities between Fortran-generated files from different compilers. Several simple examples for testing the software environment on Polaris for different programming models are available in the ALCF GettingStart repo.\n\nNote, users are encouraged to use PrgEnv-nvhpc instead of PrgEnv-nvidia as the latter will soon be deprecated in Cray's PE. They are otherwise identical pointing to compilers from the same NVIDIA SDK version.",
                "data/md/polaris/compiling-and-linking/polaris-programming-models.md"
            ]
        ],
        "golden_context": "```bash\n\nproxy settings\n\nexport HTTP_PROXY=\"http://proxy.alcf.anl.gov:3128\"\nexport HTTPS_PROXY=\"http://proxy.alcf.anl.gov:3128\"\nexport http_proxy=\"http://proxy.alcf.anl.gov:3128\"\nexport https_proxy=\"http://proxy.alcf.anl.gov:3128\"\nexport ftp_proxy=\"http://proxy.alcf.anl.gov:3128\"\nexport no_proxy=\"admin,polaris-adminvm-01,localhost,.cm.polaris.alcf.anl.gov,polaris-,.polaris.alcf.anl.gov,.alcf.anl.gov\"\n```\n\nGetting Assistance\n\nPlease direct all questions, requests, and feedback to support@alcf.anl.gov.",
        "question": "Where should I send all my questions and requests?",
        "cot_answer": "To answer this question, I will follow these steps:\n\n1. Read the provided context carefully to identify any relevant information that might help answer the question.\n\n##begin_quote##\nPlease direct all questions, requests, and feedback to support@alcf.anl.gov.\n##end_quote##\n\n2. Based on the context, look for specific instructions or guidelines related to sending questions and requests.\n\nThe context provides proxy settings, but it also includes a section titled \"Getting Assistance\" that specifically states where to send questions, requests, and feedback.\n\n3. Use this information to provide an answer to the question.\n\nBased on the provided context, I can conclude that all questions, requests, and feedback should be sent to **support@alcf.anl.gov**.\n\n<ANSWER>: support@alcf.anl.gov"
    },
    {
        "id": "data/md/polaris/known-issues.md_seed_task_2_0",
        "context": [
            "Known Issues\n\nThis is a collection of known issues that have been encountered on Polaris. Documentation will be updated as issues are resolved. Users are encouraged to email support@alcf.anl.gov to report issues.\n\nSubmitting Jobs\n\nFor batch job submissions, if the parameters within your submission script do not meet the parameters of any of the execution queues (small, ..., backfill-large) you might not receive the \"Job submission\" error on the command line at all, and the job will never appear in history qstat -xu <username> (current bug in PBS). E.g. if a user submits a script to the prod routing queue requesting 10 nodes for 24 hours, exceeding \"Time Max\" of 6 hrs of the small execution queue (which handles jobs with 10-24 nodes), then it may behave as if the job was never submitted.\n\nJob scripts are copied to temporary locations after qsub and any changes to the original script while the job is queued will not be reflected in the copied script. Furthermore, qalter requires -A <allocation name> when changing job properties. Currently, there is a request for a qalter-like command to trigger a re-copy of the original script to the temporary location.\n\nCompiling & Running Applications\n\nIf your job fails to start with an RPC launch message like below, please forward the complete messages to support@alcf.anl.gov.",
            [
                "Math Libraries\n\nBLAS, LAPACK, and ScaLAPACK for CPUs\n\nSome math libraries targeting CPUs are made available as part of the nvhpc modules and are based on the OpenBLAS project. Additional documentation is available from NVIDIA.\n\nBLAS & LAPACK can be found in the $NVIDIA_PATH/compilers/lib directory.\n\nScaLAPACK can be found in the $NVIDIA_PATH/comm_libs directory.\n\nGNU Scientific Library, GSL-2.7 available as module help math_libs/gsl\n\nAMD Optiming CPU Libraries, AOCL v4.2 available as module help math_libs/aocl\n\nOther Cray-based math libs such as Libsci, FFTW were made available by module load cray-libsci & module load cray-fftw\n\nNVIDIA Math Libraries for GPUs\n\nMath libraries from NVIDIA are made available via the nvhpc modules. Many of the libraries users typically use can be found in the $NVIDIA_PATH/math_libs directory. Some examples follow and additional documentation is available from NVIDIA.\n\nlibcublas\n\nlibcufft\n\nlibcurand\n\nlibcusolver\n\nlibcusparse",
                "data/md/polaris/applications-and-libraries/libraries/math-libraries.md"
            ],
            [
                "A simple example to test gpu-aware MPI on multiple nodes is available here.",
                "data/md/polaris/data-science-workflows/julia.md"
            ],
            [
                "jkwack@x3008c0s13b1n0:~/BabelStream/build_polaris_debug> ./cuda-stream-debug \nBabelStream\nVersion: 4.0\nImplementation: CUDA\nRunning kernels 100 times\nPrecision: double\nArray size: 268.4 MB (=0.3 GB)\nTotal size: 805.3 MB (=0.8 GB)\nUsing CUDA device NVIDIA A100-SXM4-40GB\nDriver: 11040\nFunction    MBytes/sec  Min (sec)   Max         Average\n\nCopy        1313940.694 0.00041     0.00047     0.00047\n\nMul         1302000.791 0.00041     0.00048     0.00047\n\nAdd         1296217.720 0.00062     0.00070     0.00069\n\nTriad       1296027.887 0.00062     0.00070     0.00069\n\nDot         823405.227  0.00065     0.00076     0.00075\n\njkwack@x3008c0s13b1n0:~/BabelStream/build_polaris_debug> cuda-gdb ./cuda-stream-debug \nNVIDIA (R) CUDA Debugger\n11.4 release\nPortions Copyright (C) 2007-2021 NVIDIA Corporation\nGNU gdb (GDB) 10.1\nCopyright (C) 2020 Free Software Foundation, Inc.\nLicense GPLv3+: GNU GPL version 3 or later http://gnu.org/licenses/gpl.html\nThis is free software: you are free to change and redistribute it.\nThere is NO WARRANTY, to the extent permitted by law.\nType \"show copying\" and \"show warranty\" for details.\nThis GDB was configured as \"x86_64-pc-linux-gnu\".\nType \"show configuration\" for configuration details.\nFor bug reporting instructions, please see:\nhttps://www.gnu.org/software/gdb/bugs/.\nFind the GDB manual and other documentation resources online at:\n    http://www.gnu.org/software/gdb/documentation/.",
                "data/md/polaris/debugging-tools/CUDA-GDB.md"
            ],
            [
                "OpenMP\n\nOverview\n\nThe OpenMP API is an open standard for parallel programming. The specification document can be found here: https://www.openmp.org. The specification describes directives, runtime routines, and environment variables that allow an application developer to express parallelism (e.g. shared memory multiprocessing and device offloading). Many compiler vendors provide implementations of the OpenMP specification (https://www.openmp.org/specifications).\n\nSetting the environment to use OpenMP on Polaris\n\nMany of the programming environments available on Polaris have OpenMP support.\n\nmodule OpenMP CPU support? OpenMP GPU support? PrgEnv-nvhpc yes yes llvm yes yes PrgEnv-gnu yes no PrgEnv-cray yes yes*\n\nCurrently PrgEnv-cray is not recommended for OpenMP offload.\n\nBy default, the PrgEnv-nvhpc module is loaded. To switch to other modules, you can use module switch.\n\nUsing PrgEnv-nvhpc\n\nThis is loaded by default, so there's no need to load additional modules. You can confirm that it is loaded by running module list to check that PrgEnv-nvhpc is in the list.\n\nUsing LLVM\n\nTo use the LLVM module, load the following.\nmodule load mpiwrappers/cray-mpich-llvm\nmodule load cudatoolkit-standalone\n\nSee the the LLVM compiling page here for more information.\n\nUsing PrgEnv-gnu\n\nTo switch from PrgEnv-nvhpc to PrgEnv-gnu you can run:\n\nmodule switch PrgEnv-nvhpc PrgEnv-gnu",
                "data/md/polaris/programming-models/openmp-polaris.md"
            ]
        ],
        "golden_context": "Known Issues\n\nThis is a collection of known issues that have been encountered on Polaris. Documentation will be updated as issues are resolved. Users are encouraged to email support@alcf.anl.gov to report issues.\n\nSubmitting Jobs\n\nFor batch job submissions, if the parameters within your submission script do not meet the parameters of any of the execution queues (small, ..., backfill-large) you might not receive the \"Job submission\" error on the command line at all, and the job will never appear in history qstat -xu <username> (current bug in PBS). E.g. if a user submits a script to the prod routing queue requesting 10 nodes for 24 hours, exceeding \"Time Max\" of 6 hrs of the small execution queue (which handles jobs with 10-24 nodes), then it may behave as if the job was never submitted.\n\nJob scripts are copied to temporary locations after qsub and any changes to the original script while the job is queued will not be reflected in the copied script. Furthermore, qalter requires -A <allocation name> when changing job properties. Currently, there is a request for a qalter-like command to trigger a re-copy of the original script to the temporary location.\n\nCompiling & Running Applications\n\nIf your job fails to start with an RPC launch message like below, please forward the complete messages to support@alcf.anl.gov.",
        "question": "What is the issue with job submissions when exceeding the \"Time Max\" in Polaris?",
        "cot_answer": "Here's my step-by-step reasoning to answer the question:\n\n1. Identify the issue mentioned in the context: The issue is that when a job submission exceeds the \"Time Max\" in Polaris, it may not receive an error message on the command line and the job will never appear in history qstat -xu <username>.\n2. Understand the cause of the issue: According to the context, if the parameters within the submission script do not meet the parameters of any execution queue (small, ..., backfill-large), the job may not receive an error message and will never appear in history.\n3. Relate this to the question: The question asks about the issue with job submissions when exceeding the \"Time Max\" in Polaris.\n\nNow, here's my final answer:\n\n<ANSWER>: When a job submission exceeds the \"Time Max\" in Polaris, it may not receive an error message on the command line and the job will never appear in history qstat -xu <username>."
    },
    {
        "id": "data/md/polaris/known-issues.md_seed_task_2_1",
        "context": [
            [
                "NVIDIA Nsight tools\n\nReferences\n\nNVIDIA Nsight Systems Documentation\n\nNVIDIA Nsight Compute Documentation\n\nIntroduction\n\nNVIDIA® Nsight™ Systems provides developers a system-wide visualization of an applications performance. Developers can optimize bottlenecks to scale efficiently across any number or size of CPUs and GPUs on Polaris. For further optimizations to compute kernels developers should use Nsight Compute.\n\nThe NVIDIA Nsight Compute is an interactive kernel profiler for CUDA applications. It provides detailed performance metrics and API debugging via a user interface and command line tool.\n\nIn addition, the baseline feature of this tool allows users to compare results within the tool. NVIDIA Nsight Compute provides a customizable and data-driven user interface,  metric collection, and can be extended with analysis scripts for post-processing results.\n\nStep-by-step guide\n\nCommon part on Polaris\n\nBuild your application for Polaris, and then submit your job script to Polaris or start an interactive job mode on Polaris as follows:\n\n```\n$ qsub -I -l select=1 -l walltime=1:00:00 -l filesystems=home:eagle -q debug -A\n\n$ module li\n\nCurrently Loaded Modules:\n  1) nvhpc/23.9          5) cray-pmi/6.1.13      9) PrgEnv-nvhpc/8.5.0      13) darshan/3.4.4\n  2) craype/2.7.30       6) cray-pals/1.3.4     10) libfabric/1.15.2.0\n  3) cray-dsmml/0.2.2    7) cray-libpals/1.3.4  11) craype-network-ofi\n  4) cray-mpich/8.1.28   8) craype-x86-milan    12) perftools-base/23.12.0",
                "data/md/polaris/performance-tools/NVIDIA-Nsight.md"
            ],
            [
                "Megatron-DeepSpeed\n\nWe describe below the instructions for launching distributed training with\nMicrosoft's Megatron-DeepSpeed and briefly describe some parallelism\nstrategies and various optimizations that are supported.\n\n!!! note\n\nSetup\n\nLoad conda and activate base environment:\n```bash\nload conda + activate base env\nmodule load conda/2023-10-04 ; conda activate base\n```\n\nClone\n   argonne-lcf/Megatron-DeepSpeed\n   and navigate into it:\n```bash\nclone + navigate into Megatron-DeepSpeed repo\ngit clone https://github.com/argonne-lcf/Megatron-DeepSpeed\ncd Megatron-DeepSpeed\n```\n\nMake virtual environment (on top of base conda):\n```bash\nmake virtual environment (on top of base conda)\nmkdir -p venvs/polaris/2023-10-04\npython3 -m venv venvs/polaris/2023-10-04 --system-site-packages\nsource venvs/polaris/2023-10-04/bin/activate\n```\n\nInstall missing dependency:\n```bash\ninstall *missing dependency\npython3 -m pip install \"git+https://github.com/saforem2/ezpz\"\n```\n\nLaunch training:\n```bash\n---- launch training -----------------------\n- MODEL_SIZE_KEY: defined in ALCF/model.sh\n- other args: defined in ALCF/args.sh\n---------------------------------------------\nMODEL_SIZE_KEY=\"GPT25B\" \\\n    SEQ_LEN=4096 \\ \n    USE_FLASH_ATTN_V2=1 \\\n    MICRO_BATCH=1 \\\n    GAS=1 \\\n    SP_TYPE=\"megatron\" \\\n    ZERO_STAGE=1 \\\n    ./ALCF/train-gpt3.sh\n```\n\nHelper Scripts\n\nALCF/train-gpt3.sh",
                "data/md/polaris/data-science-workflows/applications/megatron-deepspeed.md"
            ],
            [
                "EXE=/home/knight/.julia/juliaup/julia-1.10.3+0.x64.linux.gnu/bin/julia\n\nMPI_ARGS=\"-n ${NTOTRANKS} --ppn ${NRANKS_PER_NODE} --depth=${NDEPTH} --cpu-bind depth\"\n\nmpiexec ${MPI_ARGS} ${EXE} --check-bounds=no --project pi.jl\n```\n\nThe setup_env.sh script updates the environment as indicated above.\n\n```\n$ cat ./setup_env.sh\nmodule restore\nmodule load craype-accel-nvidia80\nmodule load cray-hdf5-parallel\n\nexport PATH=/home/knight/.juliaup/bin:${PATH}\nexport JULIA_DEPOT_PATH=/grand/catalyst/proj-shared/knight/polaris/julia/depot\n\nexport JULIA_HDF5_PATH=$HDF5_DIR\n\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CRAY_NVIDIA_PREFIX/cuda/12.2/extras/CUPTI/lib64/\n\nexport JULIA_CUDA_MEMORY_POOL=none\nexport MPICH_GPU_SUPPORT_ENABLED=1\nexport JULIA_MPI_PATH=${CRAY_MPICH_DIR}\nexport JULIA_MPI_HAS_CUDA=1\n\nexport TMPDIR=/local/scratch\n\nTemporary workaround\n\nexport LD_PRELOAD=libmpi_gtl_cuda.so\n```\n\nVerify that JULIA_DEPOT_PATH is set to the correct path and JULIA_PATH\npoints to the Julia executable. When using juliaup, the Julia executable is\nlocated in the juliaup folder of your JULIA_DEPOT_PATH.\n\nLarge-scale parallelism\n\nCUDA.jl uses the nvcc compiler to compile GPU kernels. This will create object files in the TEMP filesystem. The default TMPDIR in a job on Polaris is set to a temp directory that only exists on the head node of a job. We recommend setting TEMPDIR to a local directory on each compute node.\nbash\nexport TMPDIR=/local/scratch",
                "data/md/polaris/data-science-workflows/julia.md"
            ],
            [
                "CMake\n\nCMake\n\nCMake is a build configuration system that uses higher-level description files\nto automatically generate Makefiles.\n\nCMake Documentation\n\nCMake website\n\nCMake on Polaris\n\nTo use CMake on Polaris, run\n\nmodule use /soft/modulefiles\nmodule load spack-pe-base cmake",
                "data/md/polaris/build-tools/cmake-polaris.md"
            ],
            [
                "Prebuilt Gromacs binaries can be found in the directory /soft/applications/Gromacs/gromacs-2022.1.\n\nA sample pbs script follows that will run GROMACS on two nodes, using 4 MPI ranks per node, and each rank with four OpenMP threads. The PME kernel owns one MPI rank and one GPU per node, while the nonbonded kernel uses 3 MPI ranks and 3 GPUs per node.\n\n```\n\n!/bin/sh\n\nPBS -l select=2:system=polaris\n\nPBS -l place=scatter\n\nPBS -l walltime=0:30:00\n\nPBS -q debug\n\nPBS -A PROJECT\n\nPBS -l filesystems=home:grand:eagle\n\ncd ${PBS_O_WORKDIR}\n\nmodule swap PrgEnv-nvhpc PrgEnv-gnu\nmodule load cudatoolkit-standalone/11.2.2\n\nexport OMP_NUM_THREADS=4\n\nmpirun --np 8 /soft/applications/Gromacs/gromacs-2022.1/gmx_mpi \\\n      mdrun -gputasks 0123 -nb gpu -pme gpu -npme 1 -ntomp 4 \\\n      -dlb yes -resethway -pin on -v deffnm step5_1 -g test.log\n```\n\nWe strongly suggest that users try combinations of different numbers of nodes, MPI ranks per node, number of GPU tasks/devices, GPU task decomposition between nonbonded and PME kernels, and OMP threads per rank to find the optimal throughput for their particular workload.",
                "data/md/polaris/applications-and-libraries/applications/gromacs.md"
            ]
        ],
        "golden_context": "Known Issues\n\nThis is a collection of known issues that have been encountered on Polaris. Documentation will be updated as issues are resolved. Users are encouraged to email support@alcf.anl.gov to report issues.\n\nSubmitting Jobs\n\nFor batch job submissions, if the parameters within your submission script do not meet the parameters of any of the execution queues (small, ..., backfill-large) you might not receive the \"Job submission\" error on the command line at all, and the job will never appear in history qstat -xu <username> (current bug in PBS). E.g. if a user submits a script to the prod routing queue requesting 10 nodes for 24 hours, exceeding \"Time Max\" of 6 hrs of the small execution queue (which handles jobs with 10-24 nodes), then it may behave as if the job was never submitted.\n\nJob scripts are copied to temporary locations after qsub and any changes to the original script while the job is queued will not be reflected in the copied script. Furthermore, qalter requires -A <allocation name> when changing job properties. Currently, there is a request for a qalter-like command to trigger a re-copy of the original script to the temporary location.\n\nCompiling & Running Applications\n\nIf your job fails to start with an RPC launch message like below, please forward the complete messages to support@alcf.anl.gov.",
        "question": "How do I update my job script after submitting it to Polaris's temporary locations?",
        "cot_answer": "To update my job script after submitting it to Polaris's temporary locations, I will follow these steps:\n\n1. Check if the parameters in my submission script meet the requirements of any execution queues (small, ..., backfill-large). If they don't, I might not receive an error message on the command line, and the job may never appear in history qstat -xu <username>.\n##begin_quote##\nFor batch job submissions, if the parameters within your submission script do not meet the parameters of any of the execution queues (small, ..., backfill-large) you might not receive the \"Job submission\" error on the command line at all, and the job will never appear in history qstat -xu <username> (current bug in PBS).\n##end_quote##\n2. If my script is already submitted, I need to update it by making changes to the original script while the job is queued.\n##begin_quote##\nJob scripts are copied to temporary locations after qsub and any changes to the original script while the job is queued will not be reflected in the copied script.\n##end_quote##\n3. To reflect these changes, I can use qalter with the -A <allocation name> option to change job properties.\n\n<ANSWER>: Update my job script by checking if it meets execution queue requirements, making changes to the original script while the job is queued, and using qalter with the -A <allocation name> option."
    },
    {
        "id": "data/md/polaris/known-issues.md_seed_task_2_2",
        "context": [
            [
                "bash\nexport PATH=\"$PYTHONUSERBASE/bin:$PATH\"\n\nBe sure to remove this location from #!bash $PATH if you deactivate the base\nAnaconda environment or unload the module.\n\nCloning the Anaconda environment, or using venv are both more flexible and\ntransparent when compared to #!bash --user installs.",
                "data/md/polaris/data-science-workflows/python.md"
            ],
            [
                "d5xgzzaqxoe73jj2ytwuddamr/include/c++/10.2.0/x86_64-pc-linux-gnu -I/lus/theta-fs0/software/spack/spack-dev/opt/spack/linux-sles15-x86_64/gcc-9.3.0/gcc-10.2.0-r7v3naxd5xgzzaqxoe73jj2ytwuddamr/lib/gcc/x86_64-pc-linux-gnu/10.2.0/include -I/lus/theta-fs0/software/spack/spack-dev/opt/\nspack/linux-sles15-x86_64/gcc-9.3.0/gcc-10.2.0-r7v3naxd5xgzzaqxoe73jj2ytwuddamr/lib/gcc/x86_64-pc-linux-gnu/10.2.0/include-fixed/\nCXX_PARS   = nvc++ --no_warnings\n\nNormally no need to change this\n\nSRCDIR     = ../../src\nBINDIR     = ../../bin\n```\n\nSetting up compiler and libraries with module\n\nThe follow modules will update the include and libraries paths used by the Cray compiler wrapper ftn to load additional math libraries for the CPU.\n\n```\nmodule restore\nmodule load PrgEnv-nvhpc\nmodule load cray-libsci\nmodule load craype-accel-nvidia80\nexport NVROOT=${NVIDIA_PATH}\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$NVROOT/compilers/extras/qd/lib\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/soft/applications/vasp/aol-libs/3.2/amd-blis/lib/ILP64/\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/soft/applications/vasp/aol-libs/3.2/amd-libflame/lib/ILP64/\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/soft/applications/vasp/aol-libs/3.2/amd-fftw/lib\n\n```\n\nCompiling VASP\n\nOnce the modules are loaded and a makefile.include is in the vasp folder, compiling all the object files and binaries is done with:\n\nmake -j1\n\nRunning VASP in Polaris",
                "data/md/polaris/applications-and-libraries/applications/vasp.md"
            ],
            [
                "7. Cutting Planes (Slices)\n\nNow let's add some cutting plans, or slices, to see what the cross-section of the continuum data looks like.\n- Again, be sure that the continuum.000*data is selected in the Pipeline Browser\n- Filters->Alphabetical->Slice or Click on the Slice icon from the menu bar\n- In the Object Inspector make sure the Propertiestab is selected\n- At the bottom on the Object Inspector is a section titled Slice Offset Values. Here we can generate values for multiple slices to be made\n- First click the Delete All button to remove initial values\n- Next, click the New Range button. This will bring up an Add Range dialog box.\n- Set the number of Steps to 7. Click OK\n- Click the Apply button\n- With Slice1 selected in the Object Inspector, switch to the Display tab\n- Set Color by value to Velocity\n\n8. Data Representation: Opacity\n\nEven with the continuum data represented as wireframe, there is still considerable occlusion of the interior structures. In order to further reduce this occlusion by the wireframe, we can make it more transparent.\n\nAgain, be sure that the continuum.000*data is selected in the Pipeline Browser\n\nIn the Object Inspector make sure the Display tab is selected\n\nIn the Object Inspector there is a section titled Style\n\nSet Opacity to 0.2\n\n9. Animating Simulation Data\n\nSince our data has multiple time steps, we can easily animate through them to see how the data changes over time.",
                "data/md/polaris/visualization/paraview-tutorial.md"
            ],
            [
                "A simple example batch script for a libEnsemble use case that runs five workers on one node:\n\n```shell\n    #!/bin/bash -l\n    #PBS -l select=1:system=polaris\n    #PBS -l walltime=00:15:00\n    #PBS -l filesystems=home:grand\n    #PBS -q debug\n    #PBS -A\n\n```\n\nThe script can be run with:\n\nOr you can run an interactive session with:\n\nFurther links\n\nDocs:\n\nhttps://libensemble.readthedocs.io\n\nGitHub:\n\nhttps://github.com/Libensemble/libensemble",
                "data/md/polaris/workflows/libensemble.md"
            ],
            [
                "OpenMP\n\nOverview\n\nThe OpenMP API is an open standard for parallel programming. The specification document can be found here: https://www.openmp.org. The specification describes directives, runtime routines, and environment variables that allow an application developer to express parallelism (e.g. shared memory multiprocessing and device offloading). Many compiler vendors provide implementations of the OpenMP specification (https://www.openmp.org/specifications).\n\nSetting the environment to use OpenMP on Polaris\n\nMany of the programming environments available on Polaris have OpenMP support.\n\nmodule OpenMP CPU support? OpenMP GPU support? PrgEnv-nvhpc yes yes llvm yes yes PrgEnv-gnu yes no PrgEnv-cray yes yes*\n\nCurrently PrgEnv-cray is not recommended for OpenMP offload.\n\nBy default, the PrgEnv-nvhpc module is loaded. To switch to other modules, you can use module switch.\n\nUsing PrgEnv-nvhpc\n\nThis is loaded by default, so there's no need to load additional modules. You can confirm that it is loaded by running module list to check that PrgEnv-nvhpc is in the list.\n\nUsing LLVM\n\nTo use the LLVM module, load the following.\nmodule load mpiwrappers/cray-mpich-llvm\nmodule load cudatoolkit-standalone\n\nSee the the LLVM compiling page here for more information.\n\nUsing PrgEnv-gnu\n\nTo switch from PrgEnv-nvhpc to PrgEnv-gnu you can run:\n\nmodule switch PrgEnv-nvhpc PrgEnv-gnu",
                "data/md/polaris/programming-models/openmp-polaris.md"
            ]
        ],
        "golden_context": "Known Issues\n\nThis is a collection of known issues that have been encountered on Polaris. Documentation will be updated as issues are resolved. Users are encouraged to email support@alcf.anl.gov to report issues.\n\nSubmitting Jobs\n\nFor batch job submissions, if the parameters within your submission script do not meet the parameters of any of the execution queues (small, ..., backfill-large) you might not receive the \"Job submission\" error on the command line at all, and the job will never appear in history qstat -xu <username> (current bug in PBS). E.g. if a user submits a script to the prod routing queue requesting 10 nodes for 24 hours, exceeding \"Time Max\" of 6 hrs of the small execution queue (which handles jobs with 10-24 nodes), then it may behave as if the job was never submitted.\n\nJob scripts are copied to temporary locations after qsub and any changes to the original script while the job is queued will not be reflected in the copied script. Furthermore, qalter requires -A <allocation name> when changing job properties. Currently, there is a request for a qalter-like command to trigger a re-copy of the original script to the temporary location.\n\nCompiling & Running Applications\n\nIf your job fails to start with an RPC launch message like below, please forward the complete messages to support@alcf.anl.gov.",
        "question": "Why does my job fail to start with an RPC launch message and what should I do about it?",
        "cot_answer": "<ANSWER>: If your job fails to start with an RPC launch message, please forward the complete messages to support@alcf.anl.gov.\n\nReasoning:\n\nTo troubleshoot this issue, we can follow the steps outlined in the context. First, let's check if the parameters within our submission script meet the parameters of any of the execution queues (small, ..., backfill-large). If not, it might not receive the \"Job submission\" error on the command line at all, and the job will never appear in history qstat -xu <username> (##begin_quote##For batch job submissions, if the parameters within your submission script do not meet the parameters of any of the execution queues (small, ..., backfill-large) you might not receive the \"Job submission\" error on the command line at all, and the job will never appear in history qstat -xu <username>##end_quote##). In this case, we should check if our script meets the requirements of any of the execution queues.\n\nIf the issue persists, we can also check if there are any changes to the original script while it's queued. According to the context (##begin_quote##Job scripts are copied to temporary locations after qsub and any changes to the original script while the job is queued will not be reflected in the copied script.##end_quote##), any changes made to the original script will not be reflected in the copied script.\n\nFinally, if none of these steps resolve the issue, we can forward the complete messages to support@alcf.anl.gov for further assistance.\n\nFinal Answer:\n<ANSWER>: If your job fails to start with an RPC launch message, please forward the complete messages to support@alcf.anl.gov."
    },
    {
        "id": "data/md/polaris/running-jobs.md_seed_task_4_0",
        "context": [
            [
                "Balsam\n\nBalsam is a Python-based workflow manager that helps users execute large numbers of jobs, potentially with interjob dependencies, track job outcomes, and manage postprocessing analysis. A Balsam Site runs on a node with access to the job scheduler, where it can submit and monitor jobs. Overall job state is aggregated on the Balsam Server, making job data from all Sites accessible from any individual site (or the user's laptop), via the command-line interface or the Python API. To get information on how to use the command line tool, you can type balsam --help in your shell.\n\nFull documentation for Balsam is available online.\n\nBalsam requires Python 3.7+. To install Balsam on Polaris, first set up a virtual Python environment:\n\nshell\nmodule load conda\nconda activate base\npython -m venv env\nsource env/bin/activate\npip install --upgrade pip\npip install --pre balsam\n\nTo use Balsam, users need an account on the Balsam server.  Users can get an account by contacting the ALCF Help Desk.  Once a user has an account, they can login and make a new site.  A Balsam site is a project space for your workflow. You will be prompted to select what machine (Polaris) you are working on when creating a new site:\n\nshell\nbalsam login\nbalsam site init -n new-site new-site\ncd new-site\nbalsam site start\n\nSee the Balsam documentation for full details.",
                "data/md/polaris/workflows/balsam.md"
            ],
            "Running Jobs on Polaris\n\nQueues\n\nThere are five production queues you can target in your qsub (-q <queue name>):\n\nQueue Name Node Min Node Max Time Min Time Max Notes debug 1 2 5 min 1 hr max 24 nodes in use by this queue ay any given time; Only 8 nodes are exclusive (see Note below) debug-scaling 1 10 5 min 1 hr max 1 job running/accruing/queued per-user prod 10 496 5 min 24 hrs Routing queue; See below preemptable 1 10 5 min 72 hrs Please be aware that jobs in the preemptable queue can be killed at any time if jobs are submitted to the demand queue. Max 20 jobs running/accruing/queued per-project ; see Note below demand 1 56 5 min 1 hr By request only ; max 100 jobs running/accruing/queued per-project\n\nNote: Please be aware that jobs in the preemptable queue can be killed at any time if jobs are submitted to the demand queue.\nJobs in the demand queue take priority over jobs in the preemptable queue.\nThis means jobs in the preemptable queue may be preempted (killed without any warning) if there are jobs in the demand queue.\nUnfortunately, there's always an inherent risk of jobs being killed when using the preemptable queue. \nPlease use the following command to view details of a queue: qstat -Qf <queuename>\n\nTo make your job re-runable add the following PBS directive: #PBS -r y This will ensure your job will restart once the demand job is complete.",
            [
                "Additional documentation for using compilers is available on the respective programming model pages: OpenMP and SYCL.\n\nLinking\n\nDynamic linking of libraries is currently the default on Polaris. The Cray MPI wrappers will handle this automatically.\n\nNotes on Default Modules\n\ncraype-x86-rome: While the Polaris compute nodes currently have Milan CPUs, this module is loaded by default to avoid the craype-x86-milan module from adding a zen3 target not supported in the default nvhpc/21.9 compilers. The craype-x86-milan module is expected to be made default once a newer nvhpc version (e.g. 22.5) is made the default.\n\ncraype-accel-nvidia80: This module adds compiler flags to enable GPU acceleration for NVHPC compilers along with gpu-enabled MPI libraries as it is assumed that the majority of applications to be compiled on Polaris will target the GPUs for acceleration. Users building cpu-only applications may find it useful to unload this module to silence \"gpu code generation\" warnings.\n\nMixed C/C++ & Fortran Applications\n\nFor applications consisting of a mix of C/C++ and Fortran that also uses MPI, it is suggested that the programming environment chosen for Fortran be used to build the full application because of mpi.mod (and similar) incompatibilities.\n\nCompiling for GPUs",
                "data/md/polaris/compiling-and-linking/compiling-and-linking-overview.md"
            ],
            [
                "ParaView Tutorial\n\nOverview\n\nThis tutorial is intended to be a hands-on resource for users interested in learning the basic concepts of ParaView. The examples can easily be run on a laptop, using the example data set provided.\n\nTour of ParaView\n\nShow range of visualization methods\n\nWalk through various visualization techniques, hopefully illustrate how these can apply to your own data\n\nFeel for ParaView \"way\"\n\nTerminology and step-by-step process peculiar to ParaView, which may differ from other packages, e.g. VisIt\n\nData\n\nThe data used for this tutorial is:\n- Blood flow simulation data\n- Multiple data types\n  - Continuum data field (unstructured mesh, tetrahedral): fluid field, plasma\n  - Particle data (unstructured points): individual particles moving in the flow\n  - Red Blood Cells (RBC, unstructured mesh, triangle): mesh of the surface of an RBC\n    - Healthy\n    - Diseased\n- Generated using an integrated Nektar/LAMMPS simulation code\n- Courtesy of George Karniadakis and Leopold Grinberg of Brown University\n\nThe data is available for download here (~27MB compressed, ~39MB uncompressed):\nData set for ParaView Red Blood Cell Tutorial\n\n1. Load Multi-component Dataset\n\nFrom the Filemenu, (you can also click the file folder icon, shown above) open each of the following data sets (select then click \"OK\")\n\nThe files will then appear in the Pipeline Browser\n\nClick Apply in the Object Inspector\n\nYou will need to do this one at a time:\n\ncontinuum...vtu\n\nparticles...vtu\n\nrbc_...vtu",
                "data/md/polaris/visualization/paraview-tutorial.md"
            ],
            [
                "JAX\n\nJAX is another popular python package for accelerated computing.  JAX is built on XLA (the same XLA TensorFlow uses) as well as AutoGrad, and additionally has acceleration tools that operate on functions such as vmap, jit, etc.  JAX is not as widespread in machine learning as TensorFlow and PyTorch for traditional models (Computer Vision, Language Models) though it is quickly gaining promienence.  JAX is very powerful when a program needs non-traditional autodifferentiation or vectorizatoin, such as: forward-mode AD, higher order derivatives, Jacobians, Hessians, or any combination of the above.  Users of JAX on Polaris are encouraged to read the user documentation in detail, particularly the details about pure-functional programming, no in-place operations, and the common mistakes in writing functions for the @jit decorator.\n\nJAX on Polaris\n\nJAX is installed on Polaris via the conda module, available with:\nbash\nmodule load conda; conda activate\n\nThen, you can load JAX in python as usual (below showing results from the conda/2022-07-19 module):\n\n```python\n\nimport jax\njax.version\n'0.3.15'\n\n```\n\nNotes on JAX 0.3.15\n\nOn Polaris, due to a bug, an environment variable must be set to use JAX on GPUs.  The following code will crash:\npython\nimport jax.numpy as numpy\na = numpy.zeros(1000)\noutputting an error that looks like:\npython\njaxlib.xla_extension.XlaRuntimeError: UNKNOWN: no kernel image is available for execution on the device",
                "data/md/polaris/data-science-workflows/frameworks/jax.md"
            ]
        ],
        "golden_context": "Running Jobs on Polaris\n\nQueues\n\nThere are five production queues you can target in your qsub (-q <queue name>):\n\nQueue Name Node Min Node Max Time Min Time Max Notes debug 1 2 5 min 1 hr max 24 nodes in use by this queue ay any given time; Only 8 nodes are exclusive (see Note below) debug-scaling 1 10 5 min 1 hr max 1 job running/accruing/queued per-user prod 10 496 5 min 24 hrs Routing queue; See below preemptable 1 10 5 min 72 hrs Please be aware that jobs in the preemptable queue can be killed at any time if jobs are submitted to the demand queue. Max 20 jobs running/accruing/queued per-project ; see Note below demand 1 56 5 min 1 hr By request only ; max 100 jobs running/accruing/queued per-project\n\nNote: Please be aware that jobs in the preemptable queue can be killed at any time if jobs are submitted to the demand queue.\nJobs in the demand queue take priority over jobs in the preemptable queue.\nThis means jobs in the preemptable queue may be preempted (killed without any warning) if there are jobs in the demand queue.\nUnfortunately, there's always an inherent risk of jobs being killed when using the preemptable queue. \nPlease use the following command to view details of a queue: qstat -Qf <queuename>\n\nTo make your job re-runable add the following PBS directive: #PBS -r y This will ensure your job will restart once the demand job is complete.",
        "question": "What are the five production queues on Polaris?",
        "cot_answer": "To answer this question, I will follow these steps:\n\n1. Read the context carefully to identify the five production queues mentioned.\n2. Extract the relevant information about each queue from the context.\n\nHere's my reasoning:\n\nThe context mentions that there are five production queues: debug, debug-scaling, prod, preemptable, and demand. To find out what these queues are, I will scan the text for the phrase \"Queue Name\" followed by a list of queue names.\n\n##begin_quote##\nThere are five production queues you can target in your qsub (-q <queue name>):\n\nQueue Name Node Min Node Max Time Min Time Max Notes\ndebug 1 2 5 min 1 hr max 24 nodes in use by this queue ay any given time; Only 8 nodes are exclusive (see Note below)\ndebug-scaling 1 10 5 min 1 hr max 1 job running/accruing/queued per-user\nprod 10 496 5 min 24 hrs Routing queue; See below\npreemptable 1 10 5 min 72 hrs Please be aware that jobs in the preemptable queue can be killed at any time if jobs are submitted to the demand queue. Max 20 jobs running/accruing/queued per-project ; see Note below\ndemand 1 56 5 min 1 hr By request only ; max 100 jobs running/accruing/queued per-project\n##end_quote##\n\nFrom this, I can extract the five production queues:\n\n* debug\n* debug-scaling\n* prod\n* preemptable\n* demand\n\nTherefore, my final answer is:\n\n<ANSWER>: The five production queues on Polaris are: debug, debug-scaling, prod, preemptable, and demand."
    },
    {
        "id": "data/md/polaris/running-jobs.md_seed_task_4_1",
        "context": [
            [
                "For users who wish to use Spack to install their own software, we provide\nconfiguration files corresponding to the Spack PE deployments. These\nconfiguration files can be found in config directories in /soft/spack within\nthe respective Spack PE installation directories. For example, the\nspack-pe-base/0.6.1 configurations are in /soft/spack/gcc/0.6.1/config. Not\nall of these settings will be useful for all builds and it is not recommended to\nadopt these wholesale as global settings. The recommended method is to include\nthese settings ad hoc in a spack environment to control what information spack\nuses for its builds.\n\nSupport requests and feedback should be directed to\nsupport@alcf.anl.gov. For general Spack\nquestions, users are encouraged to consult the following resources:\n\nSpack development website\n\nSpack documentation\n\nSpack tutorial\n\nSpack Slack channel",
                "data/md/polaris/applications-and-libraries/libraries/spack-pe.md"
            ],
            [
                "Additional documentation for using compilers is available on the respective programming model pages: OpenMP and SYCL.\n\nLinking\n\nDynamic linking of libraries is currently the default on Polaris. The Cray MPI wrappers will handle this automatically.\n\nNotes on Default Modules\n\ncraype-x86-rome: While the Polaris compute nodes currently have Milan CPUs, this module is loaded by default to avoid the craype-x86-milan module from adding a zen3 target not supported in the default nvhpc/21.9 compilers. The craype-x86-milan module is expected to be made default once a newer nvhpc version (e.g. 22.5) is made the default.\n\ncraype-accel-nvidia80: This module adds compiler flags to enable GPU acceleration for NVHPC compilers along with gpu-enabled MPI libraries as it is assumed that the majority of applications to be compiled on Polaris will target the GPUs for acceleration. Users building cpu-only applications may find it useful to unload this module to silence \"gpu code generation\" warnings.\n\nMixed C/C++ & Fortran Applications\n\nFor applications consisting of a mix of C/C++ and Fortran that also uses MPI, it is suggested that the programming environment chosen for Fortran be used to build the full application because of mpi.mod (and similar) incompatibilities.\n\nCompiling for GPUs",
                "data/md/polaris/compiling-and-linking/compiling-and-linking-overview.md"
            ],
            [
                "```\n\npolaris_nvhpc_kokkos = Flags for NVIDIA A100, GNU Compiler, MPICH, CUDA\n\nmodule load craype-accel-nvidia80\n\nmodule swap PrgEnv-nvhpc PrgEnv-gnu\n\nmodule use /soft/modulefiles\n\nmodule load cudatoolkit-standalone\n\nmake polaris_gnu_kokkos -j 32\n\nSHELL = /bin/sh\n\n---------------------------------------------------------------------\n\ncompiler/linker settings\n\nspecify flags and libraries needed for your compiler\n\nKOKKOS_DEVICES = Cuda,OpenMP\nKOKKOS_ARCH = Ampere80\nKOKKOS_ABSOLUTE_PATH = $(shell cd $(KOKKOS_PATH); pwd)\nKOKKOS_CUDA_OPTIONS = \"enable_lambda,disable_malloc_async\"\nexport NVCC_WRAPPER_DEFAULT_COMPILER = nvc++\n\nCC =        $(KOKKOS_ABSOLUTE_PATH)/bin/nvcc_wrapper\nCCFLAGS =  -g -O3 -mp -DLAMMPS_MEMALIGN=64\n\nCCFLAGS += -DLAMMPS_BIGBIG\n\nSHFLAGS =   -fPIC\nDEPFLAGS =  -M\n\nLINK =      $(CC)\nLINKFLAGS = $(CCFLAGS)\nLIB =\nSIZE =      size\n```\n\nWith the appropriate LAMMPS Makefile in place an executable can be compiled as in the following example. Note, per-user limits on the login nodes will reduce the maximum parallelism for compilation. Users are encouraged to compile on a compute node in an interactive session if necessary.\n\ncd lammps-<version>/src\nmake yes-KOKKOS\nmake polaris_gnu_kokkos -j 32\n\nKOKKOS package and NVHPC compilers",
                "data/md/polaris/applications-and-libraries/applications/lammps.md"
            ],
            "Running Jobs on Polaris\n\nQueues\n\nThere are five production queues you can target in your qsub (-q <queue name>):\n\nQueue Name Node Min Node Max Time Min Time Max Notes debug 1 2 5 min 1 hr max 24 nodes in use by this queue ay any given time; Only 8 nodes are exclusive (see Note below) debug-scaling 1 10 5 min 1 hr max 1 job running/accruing/queued per-user prod 10 496 5 min 24 hrs Routing queue; See below preemptable 1 10 5 min 72 hrs Please be aware that jobs in the preemptable queue can be killed at any time if jobs are submitted to the demand queue. Max 20 jobs running/accruing/queued per-project ; see Note below demand 1 56 5 min 1 hr By request only ; max 100 jobs running/accruing/queued per-project\n\nNote: Please be aware that jobs in the preemptable queue can be killed at any time if jobs are submitted to the demand queue.\nJobs in the demand queue take priority over jobs in the preemptable queue.\nThis means jobs in the preemptable queue may be preempted (killed without any warning) if there are jobs in the demand queue.\nUnfortunately, there's always an inherent risk of jobs being killed when using the preemptable queue. \nPlease use the following command to view details of a queue: qstat -Qf <queuename>\n\nTo make your job re-runable add the following PBS directive: #PBS -r y This will ensure your job will restart once the demand job is complete.",
            [
                "Running Multiple MPI Applications on a node\n\nMultiple applications can be run simultaneously on a node by launching several mpiexec commands and backgrounding them. For performance, it will likely be necessary to ensure that each application runs on a distinct set of CPU resources and/or targets specific GPUs. One can provide a list of CPUs using the --cpu-bind option, which when combined with CUDA_VISIBLE_DEVICES provides a user with specifying exactly which CPU and GPU resources to run each application on. In the example below, four instances of the application are simultaneously running on a single node. In the first instance, the application is spawning MPI ranks 0-7 on CPUs 24-31 and using GPU 0. This mapping is based on output from the nvidia-smi topo -m command and pairs CPUs with the closest GPU.\n\n```bash\nexport CUDA_VISIBLE_DEVICES=0\nmpiexec -n 8 --ppn 8 --cpu-bind list:24:25:26:27:28:29:30:31 ./hello_affinity &\n\nexport CUDA_VISIBLE_DEVICES=1\nmpiexec -n 8 --ppn 8 --cpu-bind list:16:17:18:19:20:21:22:23 ./hello_affinity &\n\nexport CUDA_VISIBLE_DEVICES=2\nmpiexec -n 8 --ppn 8 --cpu-bind list:8:9:10:11:12:13:14:15 ./hello_affinity &\n\nexport CUDA_VISIBLE_DEVICES=3\nmpiexec -n 8 --ppn 8 --cpu-bind list:0:1:2:3:4:5:6:7 ./hello_affinity &\n\nwait\n```\n\nCompute Node Access to the Internet\n\nCurrently, the only access the internet is via a proxy.  Here are the proxy environment variables for Polaris:",
                "data/md/polaris/running-jobs.md"
            ]
        ],
        "golden_context": "Running Jobs on Polaris\n\nQueues\n\nThere are five production queues you can target in your qsub (-q <queue name>):\n\nQueue Name Node Min Node Max Time Min Time Max Notes debug 1 2 5 min 1 hr max 24 nodes in use by this queue ay any given time; Only 8 nodes are exclusive (see Note below) debug-scaling 1 10 5 min 1 hr max 1 job running/accruing/queued per-user prod 10 496 5 min 24 hrs Routing queue; See below preemptable 1 10 5 min 72 hrs Please be aware that jobs in the preemptable queue can be killed at any time if jobs are submitted to the demand queue. Max 20 jobs running/accruing/queued per-project ; see Note below demand 1 56 5 min 1 hr By request only ; max 100 jobs running/accruing/queued per-project\n\nNote: Please be aware that jobs in the preemptable queue can be killed at any time if jobs are submitted to the demand queue.\nJobs in the demand queue take priority over jobs in the preemptable queue.\nThis means jobs in the preemptable queue may be preempted (killed without any warning) if there are jobs in the demand queue.\nUnfortunately, there's always an inherent risk of jobs being killed when using the preemptable queue. \nPlease use the following command to view details of a queue: qstat -Qf <queuename>\n\nTo make your job re-runable add the following PBS directive: #PBS -r y This will ensure your job will restart once the demand job is complete.",
        "question": "Can jobs in the preemptable queue be killed at any time?",
        "cot_answer": "Here's my step-by-step reasoning to answer the question:\n\nTo determine if jobs in the preemptable queue can be killed at any time, I will examine the context provided. The relevant information is found in the \"Queues\" section and the note below.\n\nThe note states: ##begin_quote##Please be aware that jobs in the preemptable queue can be killed at any time if jobs are submitted to the demand queue. Jobs in the demand queue take priority over jobs in the preemptable queue. This means jobs in the preemptable queue may be preempted (killed without any warning) if there are jobs in the demand queue.##end_quote##\n\nFrom this, I can infer that jobs in the preemptable queue can indeed be killed at any time if jobs are submitted to the demand queue.\n\nTherefore, my final answer is:\n\n<ANSWER>: Yes"
    },
    {
        "id": "data/md/polaris/running-jobs.md_seed_task_4_2",
        "context": [
            [
                "SmartSim and SmartRedis\n\nSmartSim is an open source tool developed by the Hewlett Packard Enterprise (HPE) designed to facilitate the integration of traditional HPC simulation applications with machine learning workflows.\nThere are two core components to SmartSim:\n\nInfrastructure library (IL)\nProvides API to start, stop and monitor HPC applications from Python\nInterfaces with the scheduler launch jobs (PBSPro on Polaris and Cobalt on Theta/ThetaGPU)\nDeploys a distributed in-memory database called the Orchestrator\n\nSmartRedis client library\nProvides clients that connect to the Orchestrator from Fortran, C, C++, Python code\nThe client API library enables data transfer to/from database and ability to load and run JIT-traced Python and ML runtimes acting on stored data\n\nFor more resources on SmartSim, follow the links below:\n\nSource code\n\nDocumentation\n\nZoo of examples\n\nFall 2023 ALCF User Hands-On Workshop\n\nNekRS-ML\n\nInstallation\n\nSmartSim on Polaris can be installed creating a virtual environment based on the ML conda module\nmodule load conda/2023-10-04\nconda activate\nmodule load cmake\nmodule load gcc/11.2.0\nmodule load cudatoolkit-standalone/11.8.0\npython -m venv --clear /path/to/_ssim_env --system-site-packages\nsource /path/to/_ssim_env/bin/activate\npip install --upgrade pip\nNote that /path/to/ can either be a user's home or project directory.\n\nTo use SmartSim in the future, simply load the same modules and source the virtual environment.",
                "data/md/polaris/workflows/smartsim.md"
            ],
            [
                "oneAPI Compilers and Support\n\nThe Intel oneAPI compiler and Codeplay plugins for Nvidia GPUs are available on Polaris.\nThe oneAPI compilers are not enabled under the Cray Programming Environment system but can be used separately.\nTwo oneAPI variants are provided, the first being a \"release\" version based on Intel's officially released oneAPI toolkit.\nIntel Release Notes\n\n!!! note\n\nComponents\n\nThese are the list of components associated with this module\n\nUser Application Component Compilers DPC++ oneMKL Interfaces oneMKL\n\nThe other variant being a build from the open-source. This variant will be more up-to-date at the risk of bugs and breakages based on code that has not undergone a full release cycle.\nThe documentation is located on the SYCL page. Most notable differences being, icx/icpx are the names of C/C++ compilers respectively when using the release version of the module where as clang/clang++ are for open-source variant.\n\nCompile and Link\n\noneAPI uses the clang (or icx/icpx wrapper) for compiling and linking for the Nvidia A100 SM80 architecture.\n\nmodule load oneapi/release\nicpx -std=c++17 -fsycl -fsycl-targets=nvptx64-nvidia-cuda -Xsycl-target-backend --cuda-gpu-arch=sm_80 test.cpp",
                "data/md/polaris/compiling-and-linking/oneapi-compiler.md"
            ],
            [
                "or selecting File->Connect from the main menu\n\n2. Set Up Servers (first time only)\n\nThe first time you want to run a server on Polaris and have it connect to your local ParaView client, you will need to set up a Server. Once this server is set up, you can reuse it each time you run the ParaView client with the ParaView server on Polaris.\n\nKitware, the developers of ParaView, maintain a database of server configurations which you can retrieve through the ParaView client. In the File->Connect menu press the button named \"Fetch Servers\" and select POLARIS@ANL. Windows users should select \"windows to POLARIS@ANL\". Press \"Import Selected\"\n\n3. Use ParaView\n\nAfter the previous step, you can now select POLARIS@ANL in the File->Connect menu and press Connect\n\nAt this point a new window will pop up\n\nThere are a number of parameters that you must enter manually here:\n\nXterm executable: the path of a terminal on your system. The figure shows the case of a Mac with XQuartz. You may need to change these values for Windows or Linux.\n\nSSH executable: the name of your ssh command. It may be different on Windows depending on the ssh client installed (i.e putty)\n\nRemote machine: leave this value at polaris.alcf.anl.gov\n\nUsername: your ALCF user name\n\nParaView version: the version of Paraview that you want to use. Verify first that this version is installed on the system (as described at the top of this document). You will also need to add a -EGL suffix.\n\nExample:\n5.12.0-EGL",
                "data/md/polaris/visualization/paraview.md"
            ],
            [
                "with jax.default_device(target_device):\n    a = jnp.zeros((3, 3))\n    print(f\"Rank {rank}, local rank {local_rank}, a.device is {a.device()}\")\n    result = foo(a)\n    print(f\"Rank {rank}, local rank {local_rank}, result.device is {result.device()}\")\n\nif rank == 0:\n   print(result)\n\n```\n\nJAX and mpi4jax are both still somewhat early in their software lifecycles.  Updates are frequent, and if you require assistance please contact support@alcf.anl.gov.",
                "data/md/polaris/data-science-workflows/frameworks/jax.md"
            ],
            "Running Jobs on Polaris\n\nQueues\n\nThere are five production queues you can target in your qsub (-q <queue name>):\n\nQueue Name Node Min Node Max Time Min Time Max Notes debug 1 2 5 min 1 hr max 24 nodes in use by this queue ay any given time; Only 8 nodes are exclusive (see Note below) debug-scaling 1 10 5 min 1 hr max 1 job running/accruing/queued per-user prod 10 496 5 min 24 hrs Routing queue; See below preemptable 1 10 5 min 72 hrs Please be aware that jobs in the preemptable queue can be killed at any time if jobs are submitted to the demand queue. Max 20 jobs running/accruing/queued per-project ; see Note below demand 1 56 5 min 1 hr By request only ; max 100 jobs running/accruing/queued per-project\n\nNote: Please be aware that jobs in the preemptable queue can be killed at any time if jobs are submitted to the demand queue.\nJobs in the demand queue take priority over jobs in the preemptable queue.\nThis means jobs in the preemptable queue may be preempted (killed without any warning) if there are jobs in the demand queue.\nUnfortunately, there's always an inherent risk of jobs being killed when using the preemptable queue. \nPlease use the following command to view details of a queue: qstat -Qf <queuename>\n\nTo make your job re-runable add the following PBS directive: #PBS -r y This will ensure your job will restart once the demand job is complete."
        ],
        "golden_context": "Running Jobs on Polaris\n\nQueues\n\nThere are five production queues you can target in your qsub (-q <queue name>):\n\nQueue Name Node Min Node Max Time Min Time Max Notes debug 1 2 5 min 1 hr max 24 nodes in use by this queue ay any given time; Only 8 nodes are exclusive (see Note below) debug-scaling 1 10 5 min 1 hr max 1 job running/accruing/queued per-user prod 10 496 5 min 24 hrs Routing queue; See below preemptable 1 10 5 min 72 hrs Please be aware that jobs in the preemptable queue can be killed at any time if jobs are submitted to the demand queue. Max 20 jobs running/accruing/queued per-project ; see Note below demand 1 56 5 min 1 hr By request only ; max 100 jobs running/accruing/queued per-project\n\nNote: Please be aware that jobs in the preemptable queue can be killed at any time if jobs are submitted to the demand queue.\nJobs in the demand queue take priority over jobs in the preemptable queue.\nThis means jobs in the preemptable queue may be preempted (killed without any warning) if there are jobs in the demand queue.\nUnfortunately, there's always an inherent risk of jobs being killed when using the preemptable queue. \nPlease use the following command to view details of a queue: qstat -Qf <queuename>\n\nTo make your job re-runable add the following PBS directive: #PBS -r y This will ensure your job will restart once the demand job is complete.",
        "question": "How many jobs can run concurrently per-project in the demand queue?",
        "cot_answer": "To answer this question, I will follow these steps:\n\n1. Identify the relevant queue mentioned in the context that corresponds to the number of jobs that can run concurrently per-project.\n\nFrom the context, we have five production queues: debug, debug-scaling, prod, preemptable, and demand. The demand queue is the one that has a specific constraint on the maximum number of jobs running/accruing/queued per-project.\n\n##begin_quote##\nBy request only ; max 100 jobs running/accruing/queued per-project\n##end_quote##\n\n2. Extract the relevant information from the quote above.\n\nThe demand queue allows up to 100 jobs running/accruing/queued per-project.\n\n3. Formulate the answer based on the extracted information.\n\n<ANSWER>: The maximum number of jobs that can run concurrently per-project in the demand queue is 100."
    },
    {
        "id": "data/md/polaris/running-jobs.md_seed_task_5_0",
        "context": [
            [
                "For help, type \"help\".\nType \"apropos word\" to search for commands related to \"word\"...\nReading symbols from ./cuda-stream-debug...\n(cuda-gdb) b CUDAStream.cu:203\nBreakpoint 1 at 0x412598: CUDAStream.cu:203. (2 locations)\n(cuda-gdb) r\n\nStarting program: /home/jkwack/BabelStream/build_polaris_debug/cuda-stream-debug \n[Thread debugging using libthread_db enabled]\nUsing host libthread_db library \"/lib64/libthread_db.so.1\".\nBabelStream\nVersion: 4.0\nImplementation: CUDA\nRunning kernels 100 times\nPrecision: double\nArray size: 268.4 MB (=0.3 GB)\nTotal size: 805.3 MB (=0.8 GB)\n[Detaching after fork from child process 58459]\n[New Thread 0x15554c6bb000 (LWP 58475)]\nUsing CUDA device NVIDIA A100-SXM4-40GB\nDriver: 11040\n[New Thread 0x15554c4ba000 (LWP 58476)]\n[Switching focus to CUDA kernel 0, grid 5, block (0,0,0), thread (0,0,0), device 0, sm 0, warp 3, lane 0]\n\nThread 1 \"cuda-stream-deb\" hit Breakpoint 1, triad_kernel<<<(32768,1,1),(1024,1,1)>>> (a=0x155506000000, b=0x1554f6000000, c=0x1554e6000000)\n    at ../src/cuda/CUDAStream.cu:203\n203   a[i] = b[i] + scalar * c[i];\n(cuda-gdb) c\nContinuing.\n[Switching focus to CUDA kernel 0, grid 5, block (1,0,0), thread (0,0,0), device 0, sm 0, warp 32, lane 0]\n\nThread 1 \"cuda-stream-deb\" hit Breakpoint 1, triad_kernel",
                "data/md/polaris/debugging-tools/CUDA-GDB.md"
            ],
            [
                "JAX\n\nJAX is another popular python package for accelerated computing.  JAX is built on XLA (the same XLA TensorFlow uses) as well as AutoGrad, and additionally has acceleration tools that operate on functions such as vmap, jit, etc.  JAX is not as widespread in machine learning as TensorFlow and PyTorch for traditional models (Computer Vision, Language Models) though it is quickly gaining promienence.  JAX is very powerful when a program needs non-traditional autodifferentiation or vectorizatoin, such as: forward-mode AD, higher order derivatives, Jacobians, Hessians, or any combination of the above.  Users of JAX on Polaris are encouraged to read the user documentation in detail, particularly the details about pure-functional programming, no in-place operations, and the common mistakes in writing functions for the @jit decorator.\n\nJAX on Polaris\n\nJAX is installed on Polaris via the conda module, available with:\nbash\nmodule load conda; conda activate\n\nThen, you can load JAX in python as usual (below showing results from the conda/2022-07-19 module):\n\n```python\n\nimport jax\njax.version\n'0.3.15'\n\n```\n\nNotes on JAX 0.3.15\n\nOn Polaris, due to a bug, an environment variable must be set to use JAX on GPUs.  The following code will crash:\npython\nimport jax.numpy as numpy\na = numpy.zeros(1000)\noutputting an error that looks like:\npython\njaxlib.xla_extension.XlaRuntimeError: UNKNOWN: no kernel image is available for execution on the device",
                "data/md/polaris/data-science-workflows/frameworks/jax.md"
            ],
            [
                "Spack PE\n\nSpack is an HPC-oriented package manager which ALCF uses to install software for\nthe user environment.\n\nALCF's Spack PE is a Spack-managed software stack which provides various build\ntools, utilities, and libraries. It consists of a base stack (spack-pe-base)\nand PrgEnv-dependent stacks (currently spack-pe-gnu).\n\nspack-pe-base contains commonplace software compiled for CPU with the system\nGCC compilers. Accordingly, the software in spack-pe-base can be used\nregardless of programming environment.\n\nspack-pe-gnu is based on the E4S Project and\nprovides performant HPC libraries built with PrgEnv-gnu and the nvcc CUDA\ncompiler driver for GPU code. spack-pe-gnu is dependent on both\nspack-pe-base and PrgEnv-gnu.\n\nUsing software from the Spack PE\n\nThe base suite of software tools and libraries can be accessed by loading the\nspack-pe-base module. This adds a path to $MODULEPATH which contains\nnumerous modules.\n\nFor example, to load cmake starting from the default environment, a user\nshould run the following commands:\nmodule use /soft/modulefiles\nmodule load spack-pe-base\nmodule load cmake\n\nThe spack-pe-base module adds paths to the user's MODULEPATH; individual packages are subsequently loaded through the newly available modules. The full list of available packages can be viewed by running module avail or module --show-hidden avail for a complete listing. Packages are loaded in the same way from spack-pe-gnu.\n\nInspecting packages",
                "data/md/polaris/applications-and-libraries/libraries/spack-pe.md"
            ],
            "Note: The debug queue has 8 exclusively dedicated nodes.\nIf there are free nodes in production, then debug jobs can take another 16 nodes for a total of 24.\n\nprod is routing queue and routes your job to one of the following six execution queues:\n\nQueue Name Node Min Node Max Time Min Time Max Notes small 10 24 5 min 3 hrs medium 25 99 5 min 6 hrs large 100 496 5 min 24 hrs backfill-small 10 24 5 min 3 hrs low priority, negative project balance backfill-medium 25 99 5 min 6 hrs low priority, negative project balance backfill-large 100 496 5 min 24 hrs low priority, negative project balance\n\nNote 1: You cannot submit to these queues directly, you can only submit to the routing queue \"prod\".\n\nNote 2: All of these queues have a limit of ten (10) jobs running/accruing per-project\n\nNote 3: All of these queues have a limit of one hundred (100) jobs queued (not accruing score) per-project\n\nNote 4: As of January 2023, it is recommended to submit jobs with a maximum node count of 476-486 nodes given current rates of downed nodes (larger jobs may sit in the queue indefinitely).\n\nRunning MPI+OpenMP Applications\n\nOnce a submitted job is running calculations can be launched on the compute nodes using mpiexec to start an MPI application. Documentation is accessible via man mpiexec and some helpful options follow.\n\n-n total number of MPI ranks\n\n-ppn number of MPI ranks per node\n\n--cpu-bind CPU binding for application\n\n--depth number of cpus per rank (useful with --cpu-bind)",
            [
                "the default cpu instance for any gpu instance has the same identifier as the gpu instance(in which case it will be the only one configurable)\n\nother cpu instances can be configured with the identifier syntax Xc.Y, where X is the number of slots available in that gpu instance, and Y is the gpu instance identifier string\n\nsome gpu instances cannot be configured adjacently, despite there being sufficient slots/memory remaining(ex. 3g.20gb and 4g.20gb). Please see NVIDIA MIG documentation for further details\n\nCurrently, MIG configuration is only available in the debug, debug-scaling, and preemptable queues.  submissions to other queues will result in any MIG config files passed being silently ignored\n\nFiles which do not match the above syntax will be silently rejected, and any invalid configurations in properly formatted files will be silently ignored. Please test any changes to your configuration in an interactive job session before use\n\nA basic validator script is available at /soft/pbs/mig_conf_validate.sh. It will check for simple errors in your config, and print the expected configuration. For example:\n\n```shell\nascovel@polaris-login-02:~> /soft/pbs/mig_conf_validate.sh -h\nusage: mig_conf_validate.sh -c CONFIG_FILE\nascovel@polaris-login-02:~> /soft/pbs/mig_conf_validate.sh -c ./polaris-mig/mig_config.json\nexpected MIG configuration:\nGPU     GPU_INST   COMPUTE_INST",
                "data/md/polaris/workflows/mig-compute.md"
            ]
        ],
        "golden_context": "Note: The debug queue has 8 exclusively dedicated nodes.\nIf there are free nodes in production, then debug jobs can take another 16 nodes for a total of 24.\n\nprod is routing queue and routes your job to one of the following six execution queues:\n\nQueue Name Node Min Node Max Time Min Time Max Notes small 10 24 5 min 3 hrs medium 25 99 5 min 6 hrs large 100 496 5 min 24 hrs backfill-small 10 24 5 min 3 hrs low priority, negative project balance backfill-medium 25 99 5 min 6 hrs low priority, negative project balance backfill-large 100 496 5 min 24 hrs low priority, negative project balance\n\nNote 1: You cannot submit to these queues directly, you can only submit to the routing queue \"prod\".\n\nNote 2: All of these queues have a limit of ten (10) jobs running/accruing per-project\n\nNote 3: All of these queues have a limit of one hundred (100) jobs queued (not accruing score) per-project\n\nNote 4: As of January 2023, it is recommended to submit jobs with a maximum node count of 476-486 nodes given current rates of downed nodes (larger jobs may sit in the queue indefinitely).\n\nRunning MPI+OpenMP Applications\n\nOnce a submitted job is running calculations can be launched on the compute nodes using mpiexec to start an MPI application. Documentation is accessible via man mpiexec and some helpful options follow.\n\n-n total number of MPI ranks\n\n-ppn number of MPI ranks per node\n\n--cpu-bind CPU binding for application\n\n--depth number of cpus per rank (useful with --cpu-bind)",
        "question": "How many nodes can I request in the debug queue?",
        "cot_answer": "To answer this question, I will follow these steps:\n\n1. Identify the relevant information about the debug queue from the context.\n2. Determine how many nodes can be requested in the debug queue based on the available information.\n\nStep 1: Relevant information about the debug queue\nThe context states that the debug queue has 8 exclusively dedicated nodes. Additionally, if there are free nodes in production, then debug jobs can take another 16 nodes for a total of 24.\n\n##begin_quote##\nNote: The debug queue has 8 exclusively dedicated nodes.\nIf there are free nodes in production, then debug jobs can take another 16 nodes for a total of 24.\n##end_quote##\n\nStep 2: Determine the maximum number of nodes that can be requested\nBased on the information provided, if there are no free nodes in production, the maximum number of nodes that can be requested is 8 (the exclusively dedicated nodes). If there are free nodes in production, then an additional 16 nodes can be taken for a total of 24.\n\nFinal Answer:\n<ANSWER>: 24"
    },
    {
        "id": "data/md/polaris/running-jobs.md_seed_task_5_1",
        "context": [
            [
                "jkwack@x3008c0s13b1n0:~/BabelStream/build_polaris> ncu --set detailed -k triad_kernel -o JKreport-ncu_detailed-triad_kernel-BableStream ./cuda-stream\nBabelStream\nVersion: 4.0\nImplementation: CUDA\nRunning kernels 100 times\nPrecision: double\nArray size: 268.4 MB (=0.3 GB)\nTotal size: 805.3 MB (=0.8 GB)\n==PROF== Connected to process 56600 (/home/jkwack/BabelStream/build_polaris/cuda-stream)\nUsing CUDA device NVIDIA A100-SXM4-40GB\nDriver: 11040\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes",
                "data/md/polaris/performance-tools/NVIDIA-Nsight.md"
            ],
            [
                "Programming Models on Polaris\n\nThe software environment on Polaris supports several parallel programming models targeting the CPUs and GPUs.\n\nCPU Parallel Programming Models\n\nThe Cray compiler wrappers cc, CC, and ftn are recommended for MPI applications as they provide the needed include paths and libraries for each programming environment. A summary of available CPU parallel programming models and relevant compiler flags is shown below. Users are encouraged to review the corresponding man pages and documentation.\n\nProgramming Model GNU NVHPC LLVM OpenMP -fopenmp -mp -fopenmp OpenACC -- -acc=multicore --\n\nHigher-level programming models such as Kokkos and Raja may also be used for CPU programming.\n\nGPU Programming Models\n\nA summary of available GPU programming models and relevant compiler flags is shown below for compilers that generate offloadable code. Users are encouraged to review the corresponding man pages and documentation.\n\nProgramming Model GNU NVHPC LLVM ONEAPI CUDA -- -cuda [-gpu=cuda8.0,cc11.0] -- -- HIP* -- -- -- -- OpenACC -- -acc -- -- OpenCL* -- -- -- -- OpenMP -- -mp=gpu -fopenmp-targets=nvptx64 -- SYCL -- -- -- -fsycl -fsycl-targets=nvptx64-nvidia-cuda -Xsycl-target-backend --cuda-gpu-arch=sm_80\n\nNote, the llvm and oneapi modules are provided by ALCF to complement the compilers provided by the Cray PE on Polaris.\n\nHigher-level programming models such as Kokkos and Raja may also be used for GPU programming.",
                "data/md/polaris/compiling-and-linking/polaris-programming-models.md"
            ],
            [
                "Note: By default, there is no GPU-aware MPI library linking support.  The example above shows how the user can enable the linking by specifying the path to the GTL (GPU Transport Layer) library (libmpi_gtl_cuda) to the link line.\n\noneAPI Math Kernel Library (oneMKL) Interfaces\n\noneMKL Interfaces is an open-source implementation of the oneMKL Data Parallel C++ (DPC++) interface according to the oneMKL specification. It works with multiple devices (backends) using device-specific libraries underneath.\n\noneMKL is part of oneAPI. Various backend supported are shown below. More Information here.\n\nUser Application Third-Party Library cuBLAS oneMKL interface cuSOLVER cuRAND\n\nExample (using onemkl::gemm)\n\nThe following snippet shows how to compile and run a SYCL code with oneMKL library. For instance, a GPU-based GEMM is performed using mkl::gemm API and the results are compared to a CPU-based GEMM performed using the traditional blas (e.g., AOCL-BLIS) library.\n```c++\n\ninclude\n\ninclude\n\ninclude\n\ninclude   // ONEMKL GPU header\n\ninclude          // BLIS   CPU header\n\n// Matrix size constants\n\ndefine SIZE 4800 // Must be a multiple of 8.\n\ndefine M SIZE / 8\n\ndefine N SIZE / 4\n\ndefine P SIZE / 2\n\n//////////////////////////////////////////////////////////////////////////////////////////\n\nbool ValueSame(double a, double b) { return std::fabs(a - b) < 1.0e-08; }\nint VerifyResult(double c_A, double c_B) {\n  bool MismatchFound = false;",
                "data/md/polaris/programming-models/sycl-polaris.md"
            ],
            [
                "for (size_t i = 0; i < M; i++) {\n    for (size_t j = 0; j < P; j++) {\n      if (!ValueSame(c_A[i * P + j], c_B[i * P + j])) {\n        std::cout << \"fail - The result is incorrect for element: [\" << i << \", \" << j\n                  << \"], expected: \" << c_A[i * P + j] << \" , but got: \" << c_B[i * P + j]\n                  << std::endl;\n        MismatchFound = true;\n      }\n    }\n  }\n\nif (!MismatchFound) {\n    std::cout << \"SUCCESS - The results are correct!\" << std::endl;\n    return 0;\n  } else {\n    std::cout << \"FAIL - The results mis-match!\" << std::endl;\n    return -1;\n  }\n}\n\n//////////////////////////////////////////////////////////////////////////////////////////\n\nint main() {\n  std::random_device rd;  // Will be used to obtain a seed for the random number engine\n  std::mt19937 gen(rd()); // Standard mersenne_twister_engine seeded with rd()\n  std::uniform_real_distribution<> dis(1.0, 2.0);\n\n// C = alpha * op(A) * op(B)  + beta * C\n  oneapi::mkl::transpose transA = oneapi::mkl::transpose::nontrans;\n  oneapi::mkl::transpose transB = oneapi::mkl::transpose::nontrans;\n\n// matrix data sizes\n  int m = M;\n  int n = P;\n  int k = N;\n\n// leading dimensions of data\n  int ldA = k;\n  int ldB = n;\n  int ldC = n;\n\n// set scalar fp values\n  double alpha = 1.0;\n  double beta = 0.0;\n\n// 1D arrays on host side\n  double A;\n  double B;\n  double C_host_onemkl, C_cblas;\n\nA = new double[M * N]{};\n  B = new double[N * P]{};\n  C_cblas = new double[M * P]{};\n  C_host_onemkl = new double[M * P]{};",
                "data/md/polaris/programming-models/sycl-polaris.md"
            ],
            [
                "Name= NVIDIA A100-SXM4-40GB\n  Locally unique identifier= \n  Clock Frequency(KHz)= 1410000\n  Compute Mode= 0\n  Major compute capability= 8\n  Minor compute capability= 0\n  Number of multiprocessors on device= 108\n  Warp size in threads= 32\n  Single precision performance ratio= 2\n\nResult is CORRECT!! :)\n```\n\nGPU OpenACC\n\nA simple MPI-parallel OpenACC example is available here. Compilation proceeds similar to the above CUDA example except for the use of the -acc=gpu compiler flag to indicate compilation of OpenACC code for GPUs.\nCC -g -O3 -std=c++0x -acc=gpu -gpu=cc80,cuda11.0 main.cpp -o vecadd\nIn this example, each MPI rank sees all four GPUs on a Polaris node and GPUs are bound to MPI ranks round-robin within the application.\n\n```\n$ mpiexec -n 4 ./vecadd\n\nof devices= 4\n\nUsing single-precision\n\nRank 0 running on GPU 0!\nRank 1 running on GPU 1!\nRank 2 running on GPU 2!\nRank 3 running on GPU 3!\n\nResult is CORRECT!! :)\n``\nIf the application instead relies on the job launcher to bind MPI ranks to available GPUs, then a small helper script can be used to explicitly setCUDA_VISIBLE_DEVICES` appropriately for each MPI rank. One example is available here where each MPI rank is similarly bound to a single GPU with round-robin assignment. The binding of MPI ranks to GPUs is discussed in more detail here.\n\nGPU OpenCL",
                "data/md/polaris/compiling-and-linking/polaris-example-program-makefile.md"
            ]
        ],
        "golden_context": "Note: The debug queue has 8 exclusively dedicated nodes.\nIf there are free nodes in production, then debug jobs can take another 16 nodes for a total of 24.\n\nprod is routing queue and routes your job to one of the following six execution queues:\n\nQueue Name Node Min Node Max Time Min Time Max Notes small 10 24 5 min 3 hrs medium 25 99 5 min 6 hrs large 100 496 5 min 24 hrs backfill-small 10 24 5 min 3 hrs low priority, negative project balance backfill-medium 25 99 5 min 6 hrs low priority, negative project balance backfill-large 100 496 5 min 24 hrs low priority, negative project balance\n\nNote 1: You cannot submit to these queues directly, you can only submit to the routing queue \"prod\".\n\nNote 2: All of these queues have a limit of ten (10) jobs running/accruing per-project\n\nNote 3: All of these queues have a limit of one hundred (100) jobs queued (not accruing score) per-project\n\nNote 4: As of January 2023, it is recommended to submit jobs with a maximum node count of 476-486 nodes given current rates of downed nodes (larger jobs may sit in the queue indefinitely).\n\nRunning MPI+OpenMP Applications\n\nOnce a submitted job is running calculations can be launched on the compute nodes using mpiexec to start an MPI application. Documentation is accessible via man mpiexec and some helpful options follow.\n\n-n total number of MPI ranks\n\n-ppn number of MPI ranks per node\n\n--cpu-bind CPU binding for application\n\n--depth number of cpus per rank (useful with --cpu-bind)",
        "question": "What is the maximum node count for jobs that can run simultaneously per-project in all execution queues?",
        "cot_answer": "To answer this question, I will follow these steps:\n\n1. Identify the relevant information about node counts from the context.\n2. Analyze the execution queues and their corresponding node counts to determine the maximum node count for jobs that can run simultaneously per-project.\n\nFrom the context, we know that there are six execution queues: small, medium, large, backfill-small, backfill-medium, and backfill-large. Each of these queues has a minimum and maximum node count, as well as a time limit.\n\n##begin_quote##\nprod is routing queue and routes your job to one of the following six execution queues:\n\nQueue Name Node Min Node Max Time Min Time Max Notes small 10 24 5 min 3 hrs medium 25 99 5 min 6 hrs large 100 496 5 min 24 hrs backfill-small 10 24 5 min 3 hrs low priority, negative project balance backfill-medium 25 99 5 min 6 hrs low priority, negative project balance backfill-large 100 496 5 min 24 hrs low priority, negative project balance\n##end_quote##\n\nFrom this information, we can see that the maximum node count for jobs that can run simultaneously per-project is 496 nodes (from the large queue).\n\nHowever, there is an additional note that recommends submitting jobs with a maximum node count of 476-486 nodes due to current rates of downed nodes. This suggests that it may not be possible to run jobs with more than 486 nodes without them sitting in the queue indefinitely.\n\nTherefore, considering both the maximum node count and the recommendation for job submission, I conclude that the maximum node count for jobs that can run simultaneously per-project is:\n\n<ANSWER>: 486"
    },
    {
        "id": "data/md/polaris/running-jobs.md_seed_task_5_2",
        "context": [
            [
                "oneAPI Compilers and Support\n\nThe Intel oneAPI compiler and Codeplay plugins for Nvidia GPUs are available on Polaris.\nThe oneAPI compilers are not enabled under the Cray Programming Environment system but can be used separately.\nTwo oneAPI variants are provided, the first being a \"release\" version based on Intel's officially released oneAPI toolkit.\nIntel Release Notes\n\n!!! note\n\nComponents\n\nThese are the list of components associated with this module\n\nUser Application Component Compilers DPC++ oneMKL Interfaces oneMKL\n\nThe other variant being a build from the open-source. This variant will be more up-to-date at the risk of bugs and breakages based on code that has not undergone a full release cycle.\nThe documentation is located on the SYCL page. Most notable differences being, icx/icpx are the names of C/C++ compilers respectively when using the release version of the module where as clang/clang++ are for open-source variant.\n\nCompile and Link\n\noneAPI uses the clang (or icx/icpx wrapper) for compiling and linking for the Nvidia A100 SM80 architecture.\n\nmodule load oneapi/release\nicpx -std=c++17 -fsycl -fsycl-targets=nvptx64-nvidia-cuda -Xsycl-target-backend --cuda-gpu-arch=sm_80 test.cpp",
                "data/md/polaris/compiling-and-linking/oneapi-compiler.md"
            ],
            [
                "Note: that the particles.000* is still visible.\n\nUnhide the particles.000*data: click Eye icon\n\nSelect particles.000*data: click on name\n\nFilters->Alphabetical->Glyph or click on the Glyph icon from the menu bar\n\nGlyph Type: Sphere\n\nRadius:. 0.15\n\nOrient: Unchecked\n\nScale Mode: off\n\nSet Scale Factor: 1 - Edit: Checked\n\nMaximum Number of Points: 3000\n\nMask Points: Checked\n\nRandom Mode: Unchecked\n\nClick the Apply button\n\nSince our goal was to unclutter the display, let's hide the particles.000*by toggling them off, by clicking on the Eye icon next to it in the Pipeline Browser\n\nLet's also switch to the Display tab in the Object Inspector, with Glyph1 selected, and change the Color by value to GlyphVector. Since the GlyphVector value is based on the velocity. We can Edit Color Map...and choose the same Blue to Red Rainbow preset that we previously chose for velocity\n\n12. Enter: Red Blood Cells\n\nNow let's add in both of the other data sets, which are polygonal meshes which make up Red Blood Cells (RBCs).\n\nThese two data sets are essentially the same kind of data, so we can apply the same filters and make the same types of representation changes to each of them. However, some of the RBCs are marked by the simulation that generated them as healthy (rbc.000) and some of them are marked as diseased (bad_rbc.000).\n\nUnhide the rbc.000 and bad_rbc.000 data sets by clicking the Eye icon next to each of them to make them visible\n\n13. Using Color to Differentiate Data",
                "data/md/polaris/visualization/paraview-tutorial.md"
            ],
            [
                "GPU OpenCL\n\nA simple OpenCL example is available here. The OpenCL headers and library are available in the NVHPC SDK and cuda toolkits. The environment variable NVIDIA_PATH is defined for the PrgEnv-nvhpc programming environment. \nCC -o vecadd -g -O3 -std=c++0x  -I${NVIDIA_PATH}/cuda/include main.o -L${NVIDIA_PATH}/cuda/lib64 -lOpenCL\n\nThis simple example can be run on a Polaris compute node as follows.\n```\n$ ./vecadd\nRunning on GPU!\nUsing single-precision\n\nResult is CORRECT!! :)\n```\n\nGPU OpenMP\n\nA simple MPI-parallel OpenMP example is available here. Compilation proceeds similar to the above examples except for use of the -mp=gpu compiler flag to indicated compilation of OpenMP code for GPUs.\n\nCC -g -O3 -std=c++0x -mp=gpu -gpu=cc80,cuda11.0 -c main.cpp -o vecadd\n\nSimilar to the OpenACC example above, this code binds MPI ranks to GPUs in a round-robin fashion. \n```\n$ mpiexec -n 4 ./vecadd\n\nof devices= 4\n\nRank 0 running on GPU 0!\nRank 1 running on GPU 1!\nRank 2 running on GPU 2!\nRank 3 running on GPU 3!\n\nResult is CORRECT!! :)\n```",
                "data/md/polaris/compiling-and-linking/polaris-example-program-makefile.md"
            ],
            [
                "To use SmartSim in the future, simply load the same modules and source the virtual environment.\n\nThen set up the environment variables\nexport SMARTSIM_REDISAI=1.2.7\nexport CC=cc\nexport CXX=CC\nexport CUDA_DEPS_BASE=/soft/libraries\nexport CUDA_VERSION_MAJOR=11\nexport CUDNN_VERSION_MAJOR=8\nexport CUDNN_VERSION_MINOR=6\nexport CUDNN_VERSION_EXTRA=0.163\nexport CUDNN_VERSION=$CUDNN_VERSION_MAJOR.$CUDNN_VERSION_MINOR.$CUDNN_VERSION_EXTRA\nexport CUDNN_BASE=$CUDA_DEPS_BASE/cudnn/cudnn-$CUDA_VERSION_MAJOR-linux-x64-v$CUDNN_VERSION\nexport CUDNN_LIBRARY=$CUDNN_BASE/lib/\nexport CUDNN_INCLUDE_DIR=$CUDNN_BASE/include/\nexport LD_LIBRARY_PATH=$CUDNN_LIBRARY:$LD_LIBRARY_PATH\n\nNow, install SmartSim and the GPU backend\ngit clone https://github.com/CrayLabs/SmartSim.git\ncd SmartSim\npip install -e .\nexport TORCH_PATH=$( python -c 'import torch;print(torch.utils.cmake_prefix_path)' )\nexport TF_PATH=$( python -c 'import tensorflow;print(\"/\".join(tensorflow.__file__.split(\"/\")[:-1]))' )\nsmart build -v --device gpu --torch_dir $TORCH_PATH --libtensorflow_dir $TF_PATH\ncd ..\n\nFinally, install the SmartRedis library\nexport LDFLAGS=-L/opt/cray/pe/gcc/11.2.0/snos/lib64/libstdc++.so.6\ngit clone https://github.com/CrayLabs/SmartRedis.git\ncd SmartRedis\npip install -e .\nmake lib\ncd ..\n\nExamples",
                "data/md/polaris/workflows/smartsim.md"
            ],
            [
                "\"/home/zippy/smalltests/bool_constant.cpp\", line 10: error: class or struct definition is missing\n      : std::bool_constant<(UnaryPred::value || ...)> {};\n                          ^\n\n2 errors detected in the compilation of \"/home/zippy/smalltests/bool_constant.cpp\".\npolaris-login-01(~)>\n```\n\nyou will need to work around it by loading the latest cudatoolkit module atop PrgEnv-nvhpc:\n\nmodule load cudatoolkit-standalone/11.6.2",
                "data/md/polaris/compiling-and-linking/nvidia-compiler-polaris.md"
            ]
        ],
        "golden_context": "Note: The debug queue has 8 exclusively dedicated nodes.\nIf there are free nodes in production, then debug jobs can take another 16 nodes for a total of 24.\n\nprod is routing queue and routes your job to one of the following six execution queues:\n\nQueue Name Node Min Node Max Time Min Time Max Notes small 10 24 5 min 3 hrs medium 25 99 5 min 6 hrs large 100 496 5 min 24 hrs backfill-small 10 24 5 min 3 hrs low priority, negative project balance backfill-medium 25 99 5 min 6 hrs low priority, negative project balance backfill-large 100 496 5 min 24 hrs low priority, negative project balance\n\nNote 1: You cannot submit to these queues directly, you can only submit to the routing queue \"prod\".\n\nNote 2: All of these queues have a limit of ten (10) jobs running/accruing per-project\n\nNote 3: All of these queues have a limit of one hundred (100) jobs queued (not accruing score) per-project\n\nNote 4: As of January 2023, it is recommended to submit jobs with a maximum node count of 476-486 nodes given current rates of downed nodes (larger jobs may sit in the queue indefinitely).\n\nRunning MPI+OpenMP Applications\n\nOnce a submitted job is running calculations can be launched on the compute nodes using mpiexec to start an MPI application. Documentation is accessible via man mpiexec and some helpful options follow.\n\n-n total number of MPI ranks\n\n-ppn number of MPI ranks per node\n\n--cpu-bind CPU binding for application\n\n--depth number of cpus per rank (useful with --cpu-bind)",
        "question": "Is there a recommended maximum node count for submitted jobs as of January 2023?",
        "cot_answer": "To answer this question, I will follow these steps:\n\n1. Identify the relevant information about node counts from the context.\n2. Analyze the information to determine if there is a recommended maximum node count for submitted jobs.\n\nStep 1: Relevant Information\nThe context provides information about the different execution queues and their corresponding node minimums and maximums. It also mentions that as of January 2023, it is recommended to submit jobs with a maximum node count of 476-486 nodes.\n\n##begin_quote##\nprod is routing queue and routes your job to one of the following six execution queues:\n\nQueue Name Node Min Node Max Time Min Time Max Notes small 10 24 5 min 3 hrs medium 25 99 5 min 6 hrs large 100 496 5 min 24 hrs backfill-small 10 24 5 min 3 hrs low priority, negative project balance backfill-medium 25 99 5 min 6 hrs low priority, negative project balance backfill-large 100 496 5 min 24 hrs low priority, negative project balance\n\nNote 4: As of January 2023, it is recommended to submit jobs with a maximum node count of 476-486 nodes given current rates of downed nodes (larger jobs may sit in the queue indefinitely).\n##end_quote##\n\nStep 2: Analysis\nThe context suggests that there are different execution queues with varying node minimums and maximums. However, it is not clear which queue would be most suitable for a job with a specific node count.\n\nThe recommended maximum node count of 476-486 nodes as of January 2023 seems to be the most relevant information in this case. This suggests that there may be limitations on the number of nodes available for jobs, and submitting jobs with more than 476-486 nodes may result in them sitting in the queue indefinitely.\n\nFinal Answer:\n<ANSWER>: The recommended maximum node count for submitted jobs as of January 2023 is 476-486 nodes."
    },
    {
        "id": "data/md/polaris/running-jobs.md_seed_task_6_0",
        "context": [
            "--cpu-bind CPU binding for application\n\n--depth number of cpus per rank (useful with --cpu-bind)\n\n--env set environment variables (--env OMP_NUM_THREADS=2)\n\n--hostfile indicate file with hostnames (the default is --hostfile $PBS_NODEFILE)\n\nA sample submission script with directives is below for a 4-node job with 8 MPI ranks on each node and 8 OpenMP threads per rank. Each hardware thread runs a single OpenMP thread since there are 64 hardware threads on the CPU (2 per core).\nYou can download and compile hello_affinity from this link.\n\n```bash\n\n!/bin/bash -l\n\nPBS -N AFFINITY\n\nPBS -l select=4:ncpus=256\n\nPBS -l walltime=0:10:00\n\nPBS -q debug-scaling\n\nPBS -A Catalyst  # Replace with your project\n\nNNODES=wc -l < $PBS_NODEFILE\nNRANKS=8 # Number of MPI ranks to spawn per node\nNDEPTH=8 # Number of hardware threads per rank (i.e. spacing between MPI ranks)\nNTHREADS=8 # Number of software threads per rank to launch (i.e. OMP_NUM_THREADS)\n\nNTOTRANKS=$(( NNODES * NRANKS ))\n\necho \"NUM_OF_NODES= ${NNODES} TOTAL_NUM_RANKS= ${NTOTRANKS} RANKS_PER_NODE= ${NRANKS} THREADS_PER_RANK= ${NTHREADS}\"\n\nChange the directory to work directory, which is the directory you submit the job.\n\ncd $PBS_O_WORKDIR\nmpiexec --np ${NTOTRANKS} -ppn ${NRANKS} -d ${NDEPTH} --cpu-bind depth -env OMP_NUM_THREADS=${NTHREADS} ./hello_affinity\n```\n\nRunning GPU-enabled Applications",
            [
                "CPP        = nvfortran -Mpreprocess -Mfree -Mextend -E $(CPP_OPTIONS) $$(FUFFIX)  > $$(SUFFIX)\n\nFC         = ftn -acc -gpu=cc80 -mp -target-accel=nvidia80\nFCL        = ftn -acc -gpu=cc80 -c++libs -target-accel=nvidia80\n\nFREE       = -Mfree\n\nFFLAGS     = -Mbackslash -Mlarge_arrays\n\nOFLAG      = -fast\n\nDEBUG      = -Mfree -O0 -traceback\n\nSpecify your NV HPC-SDK installation, try to set NVROOT automatically\n\nNVROOT     =$(shell which nvfortran | awk -F /compilers/bin/nvfortran '{ print $$1 }')\n\n...or set NVROOT manually\n\nNVHPC      ?= /opt/nvidia/hpc_sdk\nNVVERSION  = 23.9\nNVROOT     = $(NVHPC)/Linux_x86_64/$(NVVERSION)\n\nUse NV HPC-SDK provided BLAS and LAPACK libraries\n\nLIBAOCL=/soft/libraries/aocl/3.2.0\nBLAS       = /soft/applications/vasp/aol-libs/3.2/amd-blis/lib/LP64/libblis-mt.a\nLAPACK     = /soft/applications/vasp/aol-libs/3.2/amd-libflame/lib/LP64/libflame.a\n\nBLACS      =\nSCALAPACK  =\n\nSCALAPACK  = -Mscalapack\n\nSCALAPACK  = ${LIBAOCL}/lib/libscalapack.a\n\nCUDA       = -cudalib=cublas,cusolver,cufft,nccl -cuda\n\nLLIBS      = $(SCALAPACK) $(LAPACK) $(BLAS) $(CUDA)\n\nSoftware emulation of quadruple precsion\n\nQD         ?= $(NVROOT)/compilers/extras/qd\nLLIBS      += -L$(QD)/lib -lqdmod -lqd\nINCS       += -I$(QD)/include/qd\n\nINCS       += -I/usr/include/linux\n\nINCS       += -I/usr/include/c++/7/tr1\n\nINCS       += -I/usr/include/c++/7\n\nINCS       += -I/usr/include/x86_64-linux-gnu/c++/7",
                "data/md/polaris/applications-and-libraries/applications/vasp.md"
            ],
            [
                "bash\nexport PATH=\"$PYTHONUSERBASE/bin:$PATH\"\n\nBe sure to remove this location from #!bash $PATH if you deactivate the base\nAnaconda environment or unload the module.\n\nCloning the Anaconda environment, or using venv are both more flexible and\ntransparent when compared to #!bash --user installs.",
                "data/md/polaris/data-science-workflows/python.md"
            ],
            [
                "PyTorch Best Practices on Polaris\n\nSingle Node Performance\n\nWhen running PyTorch applications, we have found the following practices to be generally, if not universally, useful and encourage you to try some of these techniques to boost performance of your own applications.\n\nUse Reduced Precision. Reduced Precision is available on A100 via tensorcores and is supported with PyTorch operations.  In general, the way to do this is via the PyTorch Automatic Mixed Precision package (AMP), as descibed in the mixed precision documentation.  In PyTorch, users generally need to manage casting and loss scaling manually,  though context managers and function decorators can provide easy tools to do this.\n\nPyTorch has a JIT module as well as backends to support op fusion, similar to TensorFlow's tf.function tools.  However, PyTorch JIT capabilities are newer and may not yield performance improvements.  Please see TorchScript for more information.\n\nMulti-GPU / Multi-Node Scale up\n\nPyTorch is compatible with scaling up to multiple GPUs per node, and across multiple nodes.  Good scaling performance has been seen up to the entire Polaris system, > 2048 GPUs.  Good performance with PyTorch has been seen with both DDP and Horovod.  For details, please see the Horovod documentation or the Distributed Data Parallel documentation.  Some Polaris-specific details that may be helpful to you:",
                "data/md/polaris/data-science-workflows/frameworks/pytorch.md"
            ],
            [
                "the default cpu instance for any gpu instance has the same identifier as the gpu instance(in which case it will be the only one configurable)\n\nother cpu instances can be configured with the identifier syntax Xc.Y, where X is the number of slots available in that gpu instance, and Y is the gpu instance identifier string\n\nsome gpu instances cannot be configured adjacently, despite there being sufficient slots/memory remaining(ex. 3g.20gb and 4g.20gb). Please see NVIDIA MIG documentation for further details\n\nCurrently, MIG configuration is only available in the debug, debug-scaling, and preemptable queues.  submissions to other queues will result in any MIG config files passed being silently ignored\n\nFiles which do not match the above syntax will be silently rejected, and any invalid configurations in properly formatted files will be silently ignored. Please test any changes to your configuration in an interactive job session before use\n\nA basic validator script is available at /soft/pbs/mig_conf_validate.sh. It will check for simple errors in your config, and print the expected configuration. For example:\n\n```shell\nascovel@polaris-login-02:~> /soft/pbs/mig_conf_validate.sh -h\nusage: mig_conf_validate.sh -c CONFIG_FILE\nascovel@polaris-login-02:~> /soft/pbs/mig_conf_validate.sh -c ./polaris-mig/mig_config.json\nexpected MIG configuration:\nGPU     GPU_INST   COMPUTE_INST",
                "data/md/polaris/workflows/mig-compute.md"
            ]
        ],
        "golden_context": "--cpu-bind CPU binding for application\n\n--depth number of cpus per rank (useful with --cpu-bind)\n\n--env set environment variables (--env OMP_NUM_THREADS=2)\n\n--hostfile indicate file with hostnames (the default is --hostfile $PBS_NODEFILE)\n\nA sample submission script with directives is below for a 4-node job with 8 MPI ranks on each node and 8 OpenMP threads per rank. Each hardware thread runs a single OpenMP thread since there are 64 hardware threads on the CPU (2 per core).\nYou can download and compile hello_affinity from this link.\n\n```bash\n\n!/bin/bash -l\n\nPBS -N AFFINITY\n\nPBS -l select=4:ncpus=256\n\nPBS -l walltime=0:10:00\n\nPBS -q debug-scaling\n\nPBS -A Catalyst  # Replace with your project\n\nNNODES=wc -l < $PBS_NODEFILE\nNRANKS=8 # Number of MPI ranks to spawn per node\nNDEPTH=8 # Number of hardware threads per rank (i.e. spacing between MPI ranks)\nNTHREADS=8 # Number of software threads per rank to launch (i.e. OMP_NUM_THREADS)\n\nNTOTRANKS=$(( NNODES * NRANKS ))\n\necho \"NUM_OF_NODES= ${NNODES} TOTAL_NUM_RANKS= ${NTOTRANKS} RANKS_PER_NODE= ${NRANKS} THREADS_PER_RANK= ${NTHREADS}\"\n\nChange the directory to work directory, which is the directory you submit the job.\n\ncd $PBS_O_WORKDIR\nmpiexec --np ${NTOTRANKS} -ppn ${NRANKS} -d ${NDEPTH} --cpu-bind depth -env OMP_NUM_THREADS=${NTHREADS} ./hello_affinity\n```\n\nRunning GPU-enabled Applications",
        "question": "What is the purpose of the '--cpu-bind CPU binding for application' directive?",
        "cot_answer": "To answer this question, I will follow these steps:\n\n1. Identify the directive mentioned in the context that is related to CPU binding for an application.\n\nReasoning:\nThe context mentions several directives, including `--cpu-bind`. This directive seems relevant to CPU binding for an application.\n\n##begin_quote##\n--cpu-bind CPU binding for application\n##end_quote##\n\n2. Understand what this directive does based on the provided information and context.\n\nReasoning:\nFrom the context, it appears that the `--cpu-bind` directive is used to specify the CPU binding for an application. This means that it allows the application to bind its threads or processes to specific CPUs or cores.\n\n3. Determine the purpose of the `--cpu-bind` directive based on this understanding.\n\nReasoning:\nThe purpose of the `--cpu-bind` directive is to allow the application to control how its threads or processes are scheduled and executed by the operating system. This can be useful for applications that require specific CPU affinity, such as those that use multi-threading or parallel processing.\n\nFinal Answer:\n<ANSWER>: The purpose of the '--cpu-bind' directive is to specify the CPU binding for an application, allowing it to control how its threads or processes are scheduled and executed by the operating system."
    },
    {
        "id": "data/md/polaris/running-jobs.md_seed_task_6_1",
        "context": [
            [
                "Inspecting packages\n\nWhen a module in the Spack PE is loaded, several environment variables are\nupdated to integrate the package into the user's environment. Additionally, the\nPACKAGE_ROOT variable is set to the path to the installation prefix of the\npackage. For example, after loading cmake as above:\n\n$ echo $CMAKE_ROOT\n/soft/spack/gcc/0.6.1/install/linux-sles15-x86_64/gcc-12.3.0/cmake-3.27.7-a435jtzvweeos2es6enirbxdjdqhqgdp/\n$ ls -a $CMAKE_ROOT\n.  ..  bin  doc  share  .spack\n\nThis variable can be used to inspect software installations and find header or\nlibrary paths. Additionally, Spack packages have a .spack directory in the\ninstallation prefix which contains build information and logs.\n\nBuilding software with Spack\n\nSpack is a powerful package manager designed for HPC.\nThe Spack PE is installed and managed with Spack; users can also install Spack\nin their own home or project directory to manage their software builds. Spack\nhas a steep learning curve, but it may benefit workflows involving frequent\nbuilds with complex dependencies.",
                "data/md/polaris/applications-and-libraries/libraries/spack-pe.md"
            ],
            "--cpu-bind CPU binding for application\n\n--depth number of cpus per rank (useful with --cpu-bind)\n\n--env set environment variables (--env OMP_NUM_THREADS=2)\n\n--hostfile indicate file with hostnames (the default is --hostfile $PBS_NODEFILE)\n\nA sample submission script with directives is below for a 4-node job with 8 MPI ranks on each node and 8 OpenMP threads per rank. Each hardware thread runs a single OpenMP thread since there are 64 hardware threads on the CPU (2 per core).\nYou can download and compile hello_affinity from this link.\n\n```bash\n\n!/bin/bash -l\n\nPBS -N AFFINITY\n\nPBS -l select=4:ncpus=256\n\nPBS -l walltime=0:10:00\n\nPBS -q debug-scaling\n\nPBS -A Catalyst  # Replace with your project\n\nNNODES=wc -l < $PBS_NODEFILE\nNRANKS=8 # Number of MPI ranks to spawn per node\nNDEPTH=8 # Number of hardware threads per rank (i.e. spacing between MPI ranks)\nNTHREADS=8 # Number of software threads per rank to launch (i.e. OMP_NUM_THREADS)\n\nNTOTRANKS=$(( NNODES * NRANKS ))\n\necho \"NUM_OF_NODES= ${NNODES} TOTAL_NUM_RANKS= ${NTOTRANKS} RANKS_PER_NODE= ${NRANKS} THREADS_PER_RANK= ${NTHREADS}\"\n\nChange the directory to work directory, which is the directory you submit the job.\n\ncd $PBS_O_WORKDIR\nmpiexec --np ${NTOTRANKS} -ppn ${NRANKS} -d ${NDEPTH} --cpu-bind depth -env OMP_NUM_THREADS=${NTHREADS} ./hello_affinity\n```\n\nRunning GPU-enabled Applications",
            [
                "conda/2023-10-04                                           (D)    oneapi/release/2023.2.1\n   cudatoolkit-standalone/11.2.2                                     oneapi/release/2024.0\n   cudatoolkit-standalone/11.4.4                                     oneapi/upstream\n   cudatoolkit-standalone/11.6.2                                     paraview/paraview-5.11.1-mesa\n   cudatoolkit-standalone/11.7.1                                     paraview/paraview-5.11.2-EGL-test\n   cudatoolkit-standalone/12.0.0                                     paraview/paraview-5.11.2-mesa\n   e4s/22.05/mvapich2                                                paraview/paraview-5.12.0-RC1-mesa\n   e4s/22.05/PrgEnv-gnu                                       (D)    paraview/paraview-5.12.0-mesa                                            (D)\n   e4s/22.08/mvapich2                                                singularity/3.8.7\n   e4s/22.08/PrgEnv-gnu                                       (D)    tau/2.31.1\n   ffmpeg/ffmpeg-6.0                                                 tau/2.32\n   forge/23.0.4                                                      tau/2.33.1                                                               (D)\n   ginkgo/mpi/20230314/ginkgo                                        visit/visit\n   ginkgo/20230314/ginkgo                                     (D)    vmd/vmd-1.9.4a55\n   gnu-parallel/2021-09-22                                           xalt/3.0.1-202308151751",
                "data/md/polaris/system-updates.md"
            ],
            [
                "Note: that the particles.000* is still visible.\n\nUnhide the particles.000*data: click Eye icon\n\nSelect particles.000*data: click on name\n\nFilters->Alphabetical->Glyph or click on the Glyph icon from the menu bar\n\nGlyph Type: Sphere\n\nRadius:. 0.15\n\nOrient: Unchecked\n\nScale Mode: off\n\nSet Scale Factor: 1 - Edit: Checked\n\nMaximum Number of Points: 3000\n\nMask Points: Checked\n\nRandom Mode: Unchecked\n\nClick the Apply button\n\nSince our goal was to unclutter the display, let's hide the particles.000*by toggling them off, by clicking on the Eye icon next to it in the Pipeline Browser\n\nLet's also switch to the Display tab in the Object Inspector, with Glyph1 selected, and change the Color by value to GlyphVector. Since the GlyphVector value is based on the velocity. We can Edit Color Map...and choose the same Blue to Red Rainbow preset that we previously chose for velocity\n\n12. Enter: Red Blood Cells\n\nNow let's add in both of the other data sets, which are polygonal meshes which make up Red Blood Cells (RBCs).\n\nThese two data sets are essentially the same kind of data, so we can apply the same filters and make the same types of representation changes to each of them. However, some of the RBCs are marked by the simulation that generated them as healthy (rbc.000) and some of them are marked as diseased (bad_rbc.000).\n\nUnhide the rbc.000 and bad_rbc.000 data sets by clicking the Eye icon next to each of them to make them visible\n\n13. Using Color to Differentiate Data",
                "data/md/polaris/visualization/paraview-tutorial.md"
            ],
            [
                "Creating final output files...\nProcessing [===============================================================100%]\nSaved report file to \"/var/tmp/pbs.308834.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov/nsys-report-f594-c524-6b4c-300a.qdrep\"\nExporting 7675 events: [===================================================100%]\n\nExported successfully to\n/var/tmp/pbs.308834.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov/nsys-report-f594-c524-6b4c-300a.sqlite\n\nCUDA API Statistics:\n\nTime(%)  Total Time (ns)  Num Calls  Average (ns)  Minimum (ns)  Maximum (ns)  StdDev (ns)           Name\n\nCUDA Kernel Statistics:\n\nTime(%)  Total Time (ns)  Instances  Average (ns)  Minimum (ns)  Maximum (ns)  StdDev (ns)                             Name\n\nCUDA Memory Operation Statistics (by time):\n\nTime(%)  Total Time (ns)  Count  Average (ns)  Minimum (ns)  Maximum (ns)  StdDev (ns)      Operation\n\n100.0       61,323,171    103     595,370.6         2,399    20,470,146  3,439,982.0  [CUDA memcpy DtoH]\n\nCUDA Memory Operation Statistics (by size):\n\nTotal (MB)  Count  Average (MB)  Minimum (MB)  Maximum (MB)  StdDev (MB)      Operation\n\nOperating System Runtime API Statistics:\n\nTime(%)  Total Time (ns)  Num Calls  Average (ns)  Minimum (ns)  Maximum (ns)  StdDev (ns)        Name\n\nReport file moved to \"/home/jkwack/BabelStream/build_polaris/JKreport-nsys-BableStream.qdrep\"\nReport file moved to \"/home/jkwack/BabelStream/build_polaris/JKreport-nsys-BableStream.sqlite\"\n```\n\nReviewing the Nsight Systems data via GUI",
                "data/md/polaris/performance-tools/NVIDIA-Nsight.md"
            ]
        ],
        "golden_context": "--cpu-bind CPU binding for application\n\n--depth number of cpus per rank (useful with --cpu-bind)\n\n--env set environment variables (--env OMP_NUM_THREADS=2)\n\n--hostfile indicate file with hostnames (the default is --hostfile $PBS_NODEFILE)\n\nA sample submission script with directives is below for a 4-node job with 8 MPI ranks on each node and 8 OpenMP threads per rank. Each hardware thread runs a single OpenMP thread since there are 64 hardware threads on the CPU (2 per core).\nYou can download and compile hello_affinity from this link.\n\n```bash\n\n!/bin/bash -l\n\nPBS -N AFFINITY\n\nPBS -l select=4:ncpus=256\n\nPBS -l walltime=0:10:00\n\nPBS -q debug-scaling\n\nPBS -A Catalyst  # Replace with your project\n\nNNODES=wc -l < $PBS_NODEFILE\nNRANKS=8 # Number of MPI ranks to spawn per node\nNDEPTH=8 # Number of hardware threads per rank (i.e. spacing between MPI ranks)\nNTHREADS=8 # Number of software threads per rank to launch (i.e. OMP_NUM_THREADS)\n\nNTOTRANKS=$(( NNODES * NRANKS ))\n\necho \"NUM_OF_NODES= ${NNODES} TOTAL_NUM_RANKS= ${NTOTRANKS} RANKS_PER_NODE= ${NRANKS} THREADS_PER_RANK= ${NTHREADS}\"\n\nChange the directory to work directory, which is the directory you submit the job.\n\ncd $PBS_O_WORKDIR\nmpiexec --np ${NTOTRANKS} -ppn ${NRANKS} -d ${NDEPTH} --cpu-bind depth -env OMP_NUM_THREADS=${NTHREADS} ./hello_affinity\n```\n\nRunning GPU-enabled Applications",
        "question": "How many hardware threads are there on the CPU according to the sample submission script?",
        "cot_answer": "To answer this question, I will follow these steps:\n\n1. Identify the relevant information from the context that provides the number of hardware threads on the CPU.\n\n##begin_quote##\nEach hardware thread runs a single OpenMP thread since there are 64 hardware threads on the CPU (2 per core).\n##end_quote##\n\nFrom this quote, we can infer that each hardware thread corresponds to one physical core. Since there are 2 hardware threads per core, the total number of hardware threads is equal to the number of cores.\n\n2. Determine the number of nodes and ranks in the job submission script.\n\nAccording to the script, there are 4 nodes, and each node has 8 MPI ranks (NRANKS=8).\n\n3. Calculate the total number of hardware threads per rank.\n\nThe script specifies NDEPTH=8, which represents the number of hardware threads per rank. Since each hardware thread corresponds to one physical core, this means that each rank has 8 cores.\n\n4. Calculate the total number of hardware threads on the CPU.\n\nSince there are 4 nodes and each node has 8 MPI ranks, the total number of ranks is NTOTRANKS=$(( NNODES * NRANKS )) = 32 (4 nodes x 8 ranks per node). Each rank has 8 cores, so the total number of hardware threads on the CPU is 32 ranks x 8 cores per rank = 256.\n\nFinal Answer:\n<ANSWER>: 256"
    },
    {
        "id": "data/md/polaris/running-jobs.md_seed_task_6_2",
        "context": [
            "--cpu-bind CPU binding for application\n\n--depth number of cpus per rank (useful with --cpu-bind)\n\n--env set environment variables (--env OMP_NUM_THREADS=2)\n\n--hostfile indicate file with hostnames (the default is --hostfile $PBS_NODEFILE)\n\nA sample submission script with directives is below for a 4-node job with 8 MPI ranks on each node and 8 OpenMP threads per rank. Each hardware thread runs a single OpenMP thread since there are 64 hardware threads on the CPU (2 per core).\nYou can download and compile hello_affinity from this link.\n\n```bash\n\n!/bin/bash -l\n\nPBS -N AFFINITY\n\nPBS -l select=4:ncpus=256\n\nPBS -l walltime=0:10:00\n\nPBS -q debug-scaling\n\nPBS -A Catalyst  # Replace with your project\n\nNNODES=wc -l < $PBS_NODEFILE\nNRANKS=8 # Number of MPI ranks to spawn per node\nNDEPTH=8 # Number of hardware threads per rank (i.e. spacing between MPI ranks)\nNTHREADS=8 # Number of software threads per rank to launch (i.e. OMP_NUM_THREADS)\n\nNTOTRANKS=$(( NNODES * NRANKS ))\n\necho \"NUM_OF_NODES= ${NNODES} TOTAL_NUM_RANKS= ${NTOTRANKS} RANKS_PER_NODE= ${NRANKS} THREADS_PER_RANK= ${NTHREADS}\"\n\nChange the directory to work directory, which is the directory you submit the job.\n\ncd $PBS_O_WORKDIR\nmpiexec --np ${NTOTRANKS} -ppn ${NRANKS} -d ${NDEPTH} --cpu-bind depth -env OMP_NUM_THREADS=${NTHREADS} ./hello_affinity\n```\n\nRunning GPU-enabled Applications",
            [
                "with jax.default_device(target_device):\n    a = jnp.zeros((3, 3))\n    print(f\"Rank {rank}, local rank {local_rank}, a.device is {a.device()}\")\n    result = foo(a)\n    print(f\"Rank {rank}, local rank {local_rank}, result.device is {result.device()}\")\n\nif rank == 0:\n   print(result)\n\n```\n\nJAX and mpi4jax are both still somewhat early in their software lifecycles.  Updates are frequent, and if you require assistance please contact support@alcf.anl.gov.",
                "data/md/polaris/data-science-workflows/frameworks/jax.md"
            ],
            [
                "GNU Compilers on Polaris\n\nThe GNU compilers are available on Polaris via the PrgEnv-gnu and gcc-mixed modules. The gcc-mixed module can be useful when, for example, the PrgEnv-nvhpc compilers are used to compile C/C++ MPI-enabled code and gfortran is needed.\n\nThe GNU compilers currently on Polaris do not support GPU code generation and thus can only be used for compiling CPU codes.\n\nThe nvhpc and llvm compilers can be used for compiling GPU-enabled applications.",
                "data/md/polaris/compiling-and-linking/gnu-compilers-polaris.md"
            ],
            [
                "Parsl on Polaris\n\nParsl is a flexible and scalable parallel programming library for Python.\n\n-- Parsl Documentation\n\nFor many applications, managing an ensemble of jobs into a workflow is a critical step that can easily become a performance bottleneck.  Many tools exist to address this, of which parsl is just one.  On this page, we'll highlight some of the key pieces of information about parsl that are relevant to Polaris.  Parsl is also extensively documented, has a dedicated Slack Channel, and a large community of users and developers beyond ALCF.  We encourage you to engage with the parsl community for support with parsl specific questions, and for Polaris-specific questions or problems, please contact support@alcf.anl.gov.\n\nGetting Parsl on Polaris\n\nYou can install parsl building off of the conda modules.  You have some flexibility in how you want to extend the conda module to include parsl, but here is an example way to do it:\n\n```python\n\nLoad the Conda Module (needed everytime you use parsl)\n\nmodule load conda\nconda activate\n\nCreate a virtual env that uses the conda env as the system packages.\n\nOnly do the next line on initial set up:\n\npython -m venv --system-site-packages /path/to/your/virtualenv\n\nLoad the virtual env (every time):\n\nsource /path/to/your/virtualenv/bin/activate\n\nInstall parsl (only once)\n\npip install parsl\n\n```\n\nUsing Parsl on Polaris",
                "data/md/polaris/workflows/parsl.md"
            ],
            [
                "Note: By default, there is no GPU-aware MPI library linking support.  The example above shows how the user can enable the linking by specifying the path to the GTL (GPU Transport Layer) library (libmpi_gtl_cuda) to the link line.\n\noneAPI Math Kernel Library (oneMKL) Interfaces\n\noneMKL Interfaces is an open-source implementation of the oneMKL Data Parallel C++ (DPC++) interface according to the oneMKL specification. It works with multiple devices (backends) using device-specific libraries underneath.\n\noneMKL is part of oneAPI. Various backend supported are shown below. More Information here.\n\nUser Application Third-Party Library cuBLAS oneMKL interface cuSOLVER cuRAND\n\nExample (using onemkl::gemm)\n\nThe following snippet shows how to compile and run a SYCL code with oneMKL library. For instance, a GPU-based GEMM is performed using mkl::gemm API and the results are compared to a CPU-based GEMM performed using the traditional blas (e.g., AOCL-BLIS) library.\n```c++\n\ninclude\n\ninclude\n\ninclude\n\ninclude   // ONEMKL GPU header\n\ninclude          // BLIS   CPU header\n\n// Matrix size constants\n\ndefine SIZE 4800 // Must be a multiple of 8.\n\ndefine M SIZE / 8\n\ndefine N SIZE / 4\n\ndefine P SIZE / 2\n\n//////////////////////////////////////////////////////////////////////////////////////////\n\nbool ValueSame(double a, double b) { return std::fabs(a - b) < 1.0e-08; }\nint VerifyResult(double c_A, double c_B) {\n  bool MismatchFound = false;",
                "data/md/polaris/programming-models/sycl-polaris.md"
            ]
        ],
        "golden_context": "--cpu-bind CPU binding for application\n\n--depth number of cpus per rank (useful with --cpu-bind)\n\n--env set environment variables (--env OMP_NUM_THREADS=2)\n\n--hostfile indicate file with hostnames (the default is --hostfile $PBS_NODEFILE)\n\nA sample submission script with directives is below for a 4-node job with 8 MPI ranks on each node and 8 OpenMP threads per rank. Each hardware thread runs a single OpenMP thread since there are 64 hardware threads on the CPU (2 per core).\nYou can download and compile hello_affinity from this link.\n\n```bash\n\n!/bin/bash -l\n\nPBS -N AFFINITY\n\nPBS -l select=4:ncpus=256\n\nPBS -l walltime=0:10:00\n\nPBS -q debug-scaling\n\nPBS -A Catalyst  # Replace with your project\n\nNNODES=wc -l < $PBS_NODEFILE\nNRANKS=8 # Number of MPI ranks to spawn per node\nNDEPTH=8 # Number of hardware threads per rank (i.e. spacing between MPI ranks)\nNTHREADS=8 # Number of software threads per rank to launch (i.e. OMP_NUM_THREADS)\n\nNTOTRANKS=$(( NNODES * NRANKS ))\n\necho \"NUM_OF_NODES= ${NNODES} TOTAL_NUM_RANKS= ${NTOTRANKS} RANKS_PER_NODE= ${NRANKS} THREADS_PER_RANK= ${NTHREADS}\"\n\nChange the directory to work directory, which is the directory you submit the job.\n\ncd $PBS_O_WORKDIR\nmpiexec --np ${NTOTRANKS} -ppn ${NRANKS} -d ${NDEPTH} --cpu-bind depth -env OMP_NUM_THREADS=${NTHREADS} ./hello_affinity\n```\n\nRunning GPU-enabled Applications",
        "question": "What does the '--env set environment variables (--env OMP_NUM_THREADS=2)' directive do?",
        "cot_answer": "To answer this question, I will break down the reasoning step by step:\n\n1. The context mentions that the `--env` directive is used to set environment variables. This suggests that the `--env OMP_NUM_THREADS=2` directive is setting an environment variable named `OMP_NUM_THREADS` with a value of 2.\n\n2. The context also mentions that OpenMP threads are being used, and each hardware thread runs a single OpenMP thread. This implies that the number of OpenMP threads per rank is set to 8 (as mentioned in the sample submission script).\n\n3. Therefore, when `--env OMP_NUM_THREADS=2` is used, it sets the environment variable `OMP_NUM_THREADS` to 2, which means that each OpenMP thread will run with a reduced number of hardware threads compared to the default setting.\n\n4. This reduction in the number of hardware threads per OpenMP thread can potentially improve performance by reducing contention and improving cache locality.\n\nFinal Answer: <ANSWER>: The '--env set environment variables (--env OMP_NUM_THREADS=2)' directive sets the environment variable `OMP_NUM_THREADS` to 2, which reduces the number of hardware threads per OpenMP thread."
    },
    {
        "id": "data/md/polaris/running-jobs.md_seed_task_7_0",
        "context": [
            [
                "ImageMagick on Polaris\n\nTo use ImageMagick on Polaris first load the corresponding module:\n\nmodule use /soft/modulefiles\nmodule load spack-pe-base imagemagick",
                "data/md/polaris/visualization/imagemagick.md"
            ],
            "Running GPU-enabled Applications\n\nGPU-enabled applications will similarly run on the compute nodes using the above example script.\n- The environment variable MPICH_GPU_SUPPORT_ENABLED=1 needs to be set if your application requires MPI-GPU support whereby the MPI library sends and receives data directly from GPU buffers. In this case, it will be important to have the craype-accel-nvidia80 module loaded both when compiling your application and during runtime to correctly link against a GPU Transport Layer (GTL) MPI library. Otherwise, you'll likely see GPU_SUPPORT_ENABLED is requested, but GTL library is not linked errors during runtime.\n- If running on a specific GPU or subset of GPUs is desired, then the CUDA_VISIBLE_DEVICES environment variable can be used. For example, if one only wanted an application to access the first two GPUs on a node, then setting CUDA_VISIBLE_DEVICES=0,1 could be used.\n\nBinding MPI ranks to GPUs\n\nThe Cray MPI on Polaris does not currently support binding MPI ranks to GPUs. For applications that need this support, this instead can be handled by use of a small helper script that will appropriately set CUDA_VISIBLE_DEVICES for each MPI rank. One example is available here where each MPI rank is similarly bound to a single GPU with round-robin assignment.\n\nA example set_affinity_gpu_polaris.sh script follows where GPUs are assigned round-robin to MPI ranks.\n\n```bash\n\n!/bin/bash -l\n\nnum_gpus=4\n\nneed to assign GPUs in reverse order due to topology",
            [
                "Running Jobs on Polaris\n\nQueues\n\nThere are five production queues you can target in your qsub (-q <queue name>):\n\nQueue Name Node Min Node Max Time Min Time Max Notes debug 1 2 5 min 1 hr max 24 nodes in use by this queue ay any given time; Only 8 nodes are exclusive (see Note below) debug-scaling 1 10 5 min 1 hr max 1 job running/accruing/queued per-user prod 10 496 5 min 24 hrs Routing queue; See below preemptable 1 10 5 min 72 hrs Please be aware that jobs in the preemptable queue can be killed at any time if jobs are submitted to the demand queue. Max 20 jobs running/accruing/queued per-project ; see Note below demand 1 56 5 min 1 hr By request only ; max 100 jobs running/accruing/queued per-project\n\nNote: Please be aware that jobs in the preemptable queue can be killed at any time if jobs are submitted to the demand queue.\nJobs in the demand queue take priority over jobs in the preemptable queue.\nThis means jobs in the preemptable queue may be preempted (killed without any warning) if there are jobs in the demand queue.\nUnfortunately, there's always an inherent risk of jobs being killed when using the preemptable queue. \nPlease use the following command to view details of a queue: qstat -Qf <queuename>\n\nTo make your job re-runable add the following PBS directive: #PBS -r y This will ensure your job will restart once the demand job is complete.",
                "data/md/polaris/running-jobs.md"
            ],
            [
                "Simply click the Play button on the animation bar at the top of the GUI\n\nPause to make it stop\n\nLoop: With this button toggled on, animation will repeat until stopped\n\n10. Animations\n\nAnimations can be saved to disk as a movie file, to be played back later.\n\nFrom the main menu: File->Save Animation\n\nAnimation Settings Dialog: Save Animation\n\nFiles of type: AVI files (*.avi)\n\nEnter a name in File name:\n\nClick OK\n\nMovie can be played back with standard media players (Windows Media Player, QuickTime, VLC, etc.)\n\n11. Particles as Glyphs\n\nGlyphs are another way of visually representing data where the attributes of a graphical element are dictated by attributes of the data.\n\nAll of the particles are displayed as red points in the graphics window. There are ~39K particles in this particular data set, which makes the display a bit cluttered. In order to both filter some of these out, and create 3D representations for them, let's apply a glyph filter to this data.\n\nNow let's add some of our other data back into the scene. Let's start with the particle data.\n\nAll of the particles are displayed as red points in the graphics window. There are ~39K particles in this particular data set, which makes the display rather cluttered. In order to both filter some of these out, and create 3D representations for them, we will apply the glyph filter to this data.\n\nNote: that the particles.000* is still visible.\n\nUnhide the particles.000*data: click Eye icon",
                "data/md/polaris/visualization/paraview-tutorial.md"
            ],
            [
                "==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes",
                "data/md/polaris/performance-tools/NVIDIA-Nsight.md"
            ]
        ],
        "golden_context": "Running GPU-enabled Applications\n\nGPU-enabled applications will similarly run on the compute nodes using the above example script.\n- The environment variable MPICH_GPU_SUPPORT_ENABLED=1 needs to be set if your application requires MPI-GPU support whereby the MPI library sends and receives data directly from GPU buffers. In this case, it will be important to have the craype-accel-nvidia80 module loaded both when compiling your application and during runtime to correctly link against a GPU Transport Layer (GTL) MPI library. Otherwise, you'll likely see GPU_SUPPORT_ENABLED is requested, but GTL library is not linked errors during runtime.\n- If running on a specific GPU or subset of GPUs is desired, then the CUDA_VISIBLE_DEVICES environment variable can be used. For example, if one only wanted an application to access the first two GPUs on a node, then setting CUDA_VISIBLE_DEVICES=0,1 could be used.\n\nBinding MPI ranks to GPUs\n\nThe Cray MPI on Polaris does not currently support binding MPI ranks to GPUs. For applications that need this support, this instead can be handled by use of a small helper script that will appropriately set CUDA_VISIBLE_DEVICES for each MPI rank. One example is available here where each MPI rank is similarly bound to a single GPU with round-robin assignment.\n\nA example set_affinity_gpu_polaris.sh script follows where GPUs are assigned round-robin to MPI ranks.\n\n```bash\n\n!/bin/bash -l\n\nnum_gpus=4\n\nneed to assign GPUs in reverse order due to topology",
        "question": "How do I set up my environment for running GPU-enabled applications on compute nodes?",
        "cot_answer": "Reasoning:\n\nTo set up my environment for running GPU-enabled applications on compute nodes, I will follow these steps.\n\n1. First, I need to check if my application requires MPI-GPU support. If it does, I need to set the environment variable MPICH_GPU_SUPPORT_ENABLED=1.\n##begin_quote##\nThe environment variable MPICH_GPU_SUPPORT_ENABLED=1 needs to be set if your application requires MPI-GPU support whereby the MPI library sends and receives data directly from GPU buffers.\n##end_quote##\n\n2. Next, I need to ensure that the craype-accel-nvidia80 module is loaded both when compiling my application and during runtime. This will allow me to correctly link against a GPU Transport Layer (GTL) MPI library.\n##begin_quote##\nIn this case, it will be important to have the craype-accel-nvidia80 module loaded both when compiling your application and during runtime to correctly link against a GPU Transport Layer (GTL) MPI library. Otherwise, you'll likely see GPU_SUPPORT_ENABLED is requested, but GTL library is not linked errors during runtime.\n##end_quote##\n\n3. If I want to run my application on a specific GPU or subset of GPUs, I can use the CUDA_VISIBLE_DEVICES environment variable. For example, if I only want an application to access the first two GPUs on a node, I can set CUDA_VISIBLE_DEVICES=0,1.\n\n4. Finally, if I need to bind MPI ranks to GPUs, I can use a small helper script that sets CUDA_VISIBLE_DEVICES for each MPI rank. One example is available here where each MPI rank is bound to a single GPU with round-robin assignment.\n\nFinal Answer:\n<ANSWER>: Set the environment variable MPICH_GPU_SUPPORT_ENABLED=1 if required, load the craype-accel-nvidia80 module, set CUDA_VISIBLE_DEVICES as needed, and use a helper script for binding MPI ranks to GPUs."
    },
    {
        "id": "data/md/polaris/running-jobs.md_seed_task_8_0",
        "context": [
            [
                "compiler/bin - cuda-gdb, ncu, nsys, ...\n\nexamples - CUDA-Fortran, OpenMP, ...\n\ncomm_libs - nccl, nvshmem, ...\n\ncompiler/libs - blas, lapack, ...\n\ncuda/lib64 - cudart, OpenCL, ...\n\nmath_libs/lib64 - cublas, cufft, ...\n\nDifferences between nvcc and nvc/nvc++\n\nFor users that want to continue using nvcc it is important to be mindful of differences with the newer nvc and nvc++ compilers. For example, the -cuda flag instructs nvcc to compile .cu input files to .cu.cpp.ii output files which are to be separately compiled, whereas the same -cuda flag instructs nvc, nvc++, and nvfortran to enable CUDA C/C++ or CUDA Fortran code generation. The resulting output file in each case is different (text vs. object) and one may see unrecognized format error when -cuda is incorrectly passed to nvcc.\n\nKnown Issues and Workarounds\n\nIf you are using nvcc to invoke nvc++ and compiling C++17 code, and are seeing the following warning and unable to compile C++17 constructs:\n\n```\npolaris-login-01(~)> nvcc --std=c++17 -ccbin nvc++ ~/smalltests/bool_constant.cpp\nnvcc warning : The -std=c++17 flag is not supported with the configured host compiler. Flag will be ignored.\n\"/home/zippy/smalltests/bool_constant.cpp\", line 10: error: namespace \"std\" has no member class \"bool_constant\"\n      : std::bool_constant<(UnaryPred::value || ...)> {};\n             ^",
                "data/md/polaris/compiling-and-linking/nvidia-compiler-polaris.md"
            ],
            [
                "LAMMPS\n\nOverview\n\nLAMMPS is a general-purpose molecular dynamics software package for massively parallel computers. It is written in an exceptionally clean style that makes it one of the more popular codes for users to extend and it currently has dozens of user-developed extensions.\n\nFor details about the code and its usage, see the LAMMPS home page. This page provides information specific to running on Polaris at the ALCF.\n\nUsing LAMMPS at ALCF\n\nALCF provides assistance with build instructions, compiling executables, submitting jobs, and providing prebuilt binaries (upon request). A collection of Makefiles and submission scripts are available in the ALCF GettingStarted repo here. For questions, contact us at support@alcf.anl.gov.\n\nHow to Obtain the Code\n\nLAMMPS is an open-source code, which can be downloaded from the LAMMPS website.\n\nBuilding on Polaris",
                "data/md/polaris/applications-and-libraries/applications/lammps.md"
            ],
            [
                "Running Multiple MPI Applications on a node\n\nMultiple applications can be run simultaneously on a node by launching several mpiexec commands and backgrounding them. For performance, it will likely be necessary to ensure that each application runs on a distinct set of CPU resources and/or targets specific GPUs. One can provide a list of CPUs using the --cpu-bind option, which when combined with CUDA_VISIBLE_DEVICES provides a user with specifying exactly which CPU and GPU resources to run each application on. In the example below, four instances of the application are simultaneously running on a single node. In the first instance, the application is spawning MPI ranks 0-7 on CPUs 24-31 and using GPU 0. This mapping is based on output from the nvidia-smi topo -m command and pairs CPUs with the closest GPU.\n\n```bash\nexport CUDA_VISIBLE_DEVICES=0\nmpiexec -n 8 --ppn 8 --cpu-bind list:24:25:26:27:28:29:30:31 ./hello_affinity &\n\nexport CUDA_VISIBLE_DEVICES=1\nmpiexec -n 8 --ppn 8 --cpu-bind list:16:17:18:19:20:21:22:23 ./hello_affinity &\n\nexport CUDA_VISIBLE_DEVICES=2\nmpiexec -n 8 --ppn 8 --cpu-bind list:8:9:10:11:12:13:14:15 ./hello_affinity &\n\nexport CUDA_VISIBLE_DEVICES=3\nmpiexec -n 8 --ppn 8 --cpu-bind list:0:1:2:3:4:5:6:7 ./hello_affinity &\n\nwait\n```\n\nCompute Node Access to the Internet\n\nCurrently, the only access the internet is via a proxy.  Here are the proxy environment variables for Polaris:",
                "data/md/polaris/running-jobs.md"
            ],
            [
                "ascovel@x3209c0s19b0n0:~> cat ./polaris-mig/mig_config.json\n{\n  \"group1\": {\n    \"gpus\": [0,1],\n    \"mig_enabled\": true,\n    \"instances\": {\"7g.40gb\": [\"4c.7g.40gb\", \"3c.7g.40gb\"] }\n  },\n  \"group2\": {\n    \"gpus\": [2,3],\n    \"mig_enabled\": true,\n    \"instances\": {\"4g.20gb\": [\"2c.4g.20gb\", \"2c.4g.20gb\"], \"2g.10gb\": [\"2g.10gb\"] }\n  }\n}\nascovel@x3209c0s19b0n0:~> nvidia-smi -L | grep -Po -e \"MIG[0-9a-f-]+\"\nMIG-63aa1884-acb8-5880-a586-173f6506966c\nMIG-b86283ae-9953-514f-81df-99be7e0553a5\nMIG-79065f64-bdbb-53ff-89e3-9d35f270b208\nMIG-6dd56a9d-e362-567e-95b1-108afbcfc674\nMIG-76459138-79df-5d00-a11f-b0a2a747bd9e\nMIG-4d5c9fb3-b0e3-50e8-a60c-233104222611\nMIG-bdfeeb2d-7a50-5e39-b3c5-767838a0b7a3\nMIG-87a2c2f3-d008-51be-b64b-6adb56deb679\nMIG-3d4cdd8c-fc36-5ce9-9676-a6e46d4a6c86\nMIG-773e8e18-f62a-5250-af1e-9343c9286ce1\nascovel@x3209c0s19b0n0:~> for mig in $( nvidia-smi -L | grep -Po -e \"MIG[0-9a-f-]+\" ) ; do CUDA_VISIBLE_DEVICES=${mig} ./saxpy & done 2>/dev/null\nascovel@x3209c0s19b0n0:~> nvidia-smi | tail -n 16\n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|    0    0    0      17480      C   ./saxpy                          8413MiB |",
                "data/md/polaris/workflows/mig-compute.md"
            ],
            [
                "harms@x3004c0s7b0n0:~> sycl-ls\n[opencl:acc:0] Intel(R) FPGA Emulation Platform for OpenCL(TM), Intel(R) FPGA Emulation Device 1.2 [2023.16.7.0.21_160000]\n[opencl:cpu:1] Intel(R) OpenCL, AMD EPYC 7543P 32-Core Processor                3.0 [2023.16.7.0.21_160000]\n[ext_oneapi_cuda:gpu:0] NVIDIA CUDA BACKEND, NVIDIA A100-SXM4-40GB 8.8 [CUDA 11.4]\n[ext_oneapi_cuda:gpu:1] NVIDIA CUDA BACKEND, NVIDIA A100-SXM4-40GB 8.8 [CUDA 11.4]\n[ext_oneapi_cuda:gpu:2] NVIDIA CUDA BACKEND, NVIDIA A100-SXM4-40GB 8.8 [CUDA 11.4]\n[ext_oneapi_cuda:gpu:3] NVIDIA CUDA BACKEND, NVIDIA A100-SXM4-40GB 8.8 [CUDA 11.4]\n```",
                "data/md/polaris/compiling-and-linking/oneapi-compiler.md"
            ]
        ],
        "golden_context": "```bash\n\n!/bin/bash -l\n\nnum_gpus=4\n\nneed to assign GPUs in reverse order due to topology\n\nSee Polaris Device Affinity Information:\n\nhttps://www.alcf.anl.gov/support/user-guides/polaris/hardware-overview/machine-overview/index.html\n\ngpu=$((${num_gpus} - 1 - ${PMI_LOCAL_RANK} % ${num_gpus}))\nexport CUDA_VISIBLE_DEVICES=$gpu\necho “RANK= ${PMI_RANK} LOCAL_RANK= ${PMI_LOCAL_RANK} gpu= ${gpu}”\nexec \"$@\"\nThis script can be placed just before the executable in the `mpiexec` command like so.bash\nmpiexec -n ${NTOTRANKS} --ppn ${NRANKS_PER_NODE} --depth=${NDEPTH} --cpu-bind depth ./set_affinity_gpu_polaris.sh ./hello_affinity\n```\nUsers with different needs, such as assigning multiple GPUs per MPI rank, can modify the above script to suit their needs.\n\nInteractive Jobs on Compute Nodes\n\nHere is how to submit an interactive job to, for example, edit/build/test an application Polaris compute nodes:\nqsub -I -l select=1 -l filesystems=home:eagle -l walltime=1:00:00 -q debug -A <project_name>\n\nThis command requests 1 node for a period of 1 hour in the debug queue, requiring access to the /home and eagle filesystems. After waiting in the queue for a node to become available, a shell prompt on a compute node will appear. You may then start building applications and testing gpu affinity scripts on the compute node.\n\nNOTE: If you want to ssh or scp to one of your assigned compute nodes you will need to make sure your $HOME directory and your $HOME/.ssh directory permissions are both set to 700.",
        "question": "How do I assign GPUs in reverse order when running a job on Polaris compute nodes?",
        "cot_answer": "To assign GPUs in reverse order when running a job on Polaris compute nodes, you can modify the script provided in the context. Here's the step-by-step reasoning:\n\n1. The original script assigns GPUs based on the `PMI_LOCAL_RANK` variable, which represents the rank of each process within a node. To assign GPUs in reverse order, we need to adjust this calculation.\n\n##begin_quote##\ngpu=$((${num_gpus} - 1 - ${PMI_LOCAL_RANK} % ${num_gpus}))\n##end_quote##\n\n2. Instead of subtracting `PMI_LOCAL_RANK` from the total number of GPUs (`num_gpus`), we can add it to get the desired reverse order.\n\n3. The modified script would be:\n```bash\ngpu=$((${num_gpus} - 1 + ${PMI_LOCAL_RANK} % ${num_gpus}))\nexport CUDA_VISIBLE_DEVICES=$gpu\necho “RANK= ${PMI_RANK} LOCAL_RANK= ${PMI_LOCAL_RANK} gpu= ${gpu}”\nexec \"$@\"\n```\n\n<ANSWER>: The final answer is: Modify the script to `gpu=$((${num_gpus} - 1 + ${PMI_LOCAL_RANK} % ${num_gpus}))`."
    },
    {
        "id": "data/md/polaris/running-jobs.md_seed_task_8_1",
        "context": [
            [
                "Polaris\n\nPolaris is a 560 node HPE Apollo 6500 Gen 10+ based system.  Each node has a single 2.8 GHz AMD EPYC Milan 7543P 32 core CPU with 512 GB of DDR4 RAM and four NVIDIA A100 GPUs connected via NVLink, a pair of local 1.6TB of SSDs in RAID0 for the users use, and a pair of Slingshot 11 network adapters.  There are two nodes per chassis, seven chassis per rack, and 40 racks for a total of 560 nodes.  More detailed specifications are as follows:\n\nPolaris Compute Nodes\n\nPOLARIS COMPUTE DESCRIPTION PER NODE AGGREGATE Processor (Note 1) 2.8 GHz 7543P 1 560 Cores/Threads AMD Zen 3 (Milan) 32/64 17,920/35,840 RAM (Note 2) DDR4 512 GiB 280 TiB GPUS NVIDIA A100 4 2240 Local SSD 1.6 TB 2/3.2 TB 1120/1.8PB\n\nNote 1: 256MB shared L3 cache, 512KB L2 cache per core, 32 KB L1 cache per core\nNote 2: 8 memory channels rated at 204.8 GiB/s\n\nPolaris A100 GPU Information\n\nDESCRIPTION A100 PCIe A100 HGX (Polaris) GPU Memory 40 GiB HBM2 160 GiB HBM2 GPU Memory BW 1.6 TB/s 6.4 TB/s Interconnect PCIe Gen4 64 GB/s NVLink 600 GB/s FP 64 9.7 TF 38.8 TF FP64 Tensor Core 19.5 TF 78 TF FP 32 19.5 TF 78 TF BF16 Tensor Core 312 TF 1.3 PF FP16 Tensor Core 312 TF 1.3 PF INT8 Tensor Core 624 TOPS 2496 TOPS Max TDP Power 250 W 400 W\n\nPolaris Device Affinity Information",
                "data/md/polaris/hardware-overview/machine-overview.md"
            ],
            [
                "Spack PE\n\nSpack is an HPC-oriented package manager which ALCF uses to install software for\nthe user environment.\n\nALCF's Spack PE is a Spack-managed software stack which provides various build\ntools, utilities, and libraries. It consists of a base stack (spack-pe-base)\nand PrgEnv-dependent stacks (currently spack-pe-gnu).\n\nspack-pe-base contains commonplace software compiled for CPU with the system\nGCC compilers. Accordingly, the software in spack-pe-base can be used\nregardless of programming environment.\n\nspack-pe-gnu is based on the E4S Project and\nprovides performant HPC libraries built with PrgEnv-gnu and the nvcc CUDA\ncompiler driver for GPU code. spack-pe-gnu is dependent on both\nspack-pe-base and PrgEnv-gnu.\n\nUsing software from the Spack PE\n\nThe base suite of software tools and libraries can be accessed by loading the\nspack-pe-base module. This adds a path to $MODULEPATH which contains\nnumerous modules.\n\nFor example, to load cmake starting from the default environment, a user\nshould run the following commands:\nmodule use /soft/modulefiles\nmodule load spack-pe-base\nmodule load cmake\n\nThe spack-pe-base module adds paths to the user's MODULEPATH; individual packages are subsequently loaded through the newly available modules. The full list of available packages can be viewed by running module avail or module --show-hidden avail for a complete listing. Packages are loaded in the same way from spack-pe-gnu.\n\nInspecting packages",
                "data/md/polaris/applications-and-libraries/libraries/spack-pe.md"
            ],
            [
                "Polaris System Updates\n\n2024-04-22\n\nThe management software on Polaris has been upgraded to HPCM 1.10\nThe following version changes are in place with the upgrade to HPCM 1.10:\n\nHPE Cray Programming Environment (CPE) 23.12\n\nSlingShot version 2.1.2\n\nNVIDIA SDK 23.9\n\nNVIDIA driver version 535.154.05\n\nCUDA 12.2\n\nSUSE 15 SP5\n\nReleasing jobs\n\nJobs that were queued before the upgrade have been restored to the appropriate queues but are placed on user hold. \nJobs are not expected to complete successfully due to the changes made to the system and software environments resulting from the upgrade. \nWe recommend you review your jobs and either release the hold (qrls <jobid>) or delete it (qdel <jobid>) and resubmit as appropriate.\n\nUsers need to rebuild for the new PE environment and major OS upgrade. Existing binaries are unlikely to run successfully.\n\nWe have held all jobs submitted prior to the upgrade as a user hold. Users may release their existing jobs with qrls to run after they have rebuilt their binaries.\n\nPBS does cache the job execution script.  If a change to the script is required due to a path changing post rebuild, the job will have to be resubmitted.\n\nAll application binaries should be rebuilt prior to further job submissions.\n\nRe-building user codes\n\nMany user codes will need to be re-built and/or re-linked against the newer version of the programming environment (23.12) and Spack provided dependencies.\n\nChanges to the user software environment",
                "data/md/polaris/system-updates.md"
            ],
            "```bash\n\n!/bin/bash -l\n\nnum_gpus=4\n\nneed to assign GPUs in reverse order due to topology\n\nSee Polaris Device Affinity Information:\n\nhttps://www.alcf.anl.gov/support/user-guides/polaris/hardware-overview/machine-overview/index.html\n\ngpu=$((${num_gpus} - 1 - ${PMI_LOCAL_RANK} % ${num_gpus}))\nexport CUDA_VISIBLE_DEVICES=$gpu\necho “RANK= ${PMI_RANK} LOCAL_RANK= ${PMI_LOCAL_RANK} gpu= ${gpu}”\nexec \"$@\"\nThis script can be placed just before the executable in the `mpiexec` command like so.bash\nmpiexec -n ${NTOTRANKS} --ppn ${NRANKS_PER_NODE} --depth=${NDEPTH} --cpu-bind depth ./set_affinity_gpu_polaris.sh ./hello_affinity\n```\nUsers with different needs, such as assigning multiple GPUs per MPI rank, can modify the above script to suit their needs.\n\nInteractive Jobs on Compute Nodes\n\nHere is how to submit an interactive job to, for example, edit/build/test an application Polaris compute nodes:\nqsub -I -l select=1 -l filesystems=home:eagle -l walltime=1:00:00 -q debug -A <project_name>\n\nThis command requests 1 node for a period of 1 hour in the debug queue, requiring access to the /home and eagle filesystems. After waiting in the queue for a node to become available, a shell prompt on a compute node will appear. You may then start building applications and testing gpu affinity scripts on the compute node.\n\nNOTE: If you want to ssh or scp to one of your assigned compute nodes you will need to make sure your $HOME directory and your $HOME/.ssh directory permissions are both set to 700.",
            [
                "Math Libraries\n\nBLAS, LAPACK, and ScaLAPACK for CPUs\n\nSome math libraries targeting CPUs are made available as part of the nvhpc modules and are based on the OpenBLAS project. Additional documentation is available from NVIDIA.\n\nBLAS & LAPACK can be found in the $NVIDIA_PATH/compilers/lib directory.\n\nScaLAPACK can be found in the $NVIDIA_PATH/comm_libs directory.\n\nGNU Scientific Library, GSL-2.7 available as module help math_libs/gsl\n\nAMD Optiming CPU Libraries, AOCL v4.2 available as module help math_libs/aocl\n\nOther Cray-based math libs such as Libsci, FFTW were made available by module load cray-libsci & module load cray-fftw\n\nNVIDIA Math Libraries for GPUs\n\nMath libraries from NVIDIA are made available via the nvhpc modules. Many of the libraries users typically use can be found in the $NVIDIA_PATH/math_libs directory. Some examples follow and additional documentation is available from NVIDIA.\n\nlibcublas\n\nlibcufft\n\nlibcurand\n\nlibcusolver\n\nlibcusparse",
                "data/md/polaris/applications-and-libraries/libraries/math-libraries.md"
            ]
        ],
        "golden_context": "```bash\n\n!/bin/bash -l\n\nnum_gpus=4\n\nneed to assign GPUs in reverse order due to topology\n\nSee Polaris Device Affinity Information:\n\nhttps://www.alcf.anl.gov/support/user-guides/polaris/hardware-overview/machine-overview/index.html\n\ngpu=$((${num_gpus} - 1 - ${PMI_LOCAL_RANK} % ${num_gpus}))\nexport CUDA_VISIBLE_DEVICES=$gpu\necho “RANK= ${PMI_RANK} LOCAL_RANK= ${PMI_LOCAL_RANK} gpu= ${gpu}”\nexec \"$@\"\nThis script can be placed just before the executable in the `mpiexec` command like so.bash\nmpiexec -n ${NTOTRANKS} --ppn ${NRANKS_PER_NODE} --depth=${NDEPTH} --cpu-bind depth ./set_affinity_gpu_polaris.sh ./hello_affinity\n```\nUsers with different needs, such as assigning multiple GPUs per MPI rank, can modify the above script to suit their needs.\n\nInteractive Jobs on Compute Nodes\n\nHere is how to submit an interactive job to, for example, edit/build/test an application Polaris compute nodes:\nqsub -I -l select=1 -l filesystems=home:eagle -l walltime=1:00:00 -q debug -A <project_name>\n\nThis command requests 1 node for a period of 1 hour in the debug queue, requiring access to the /home and eagle filesystems. After waiting in the queue for a node to become available, a shell prompt on a compute node will appear. You may then start building applications and testing gpu affinity scripts on the compute node.\n\nNOTE: If you want to ssh or scp to one of your assigned compute nodes you will need to make sure your $HOME directory and your $HOME/.ssh directory permissions are both set to 700.",
        "question": "What command can I use to submit an interactive job on Polaris compute nodes for building and testing applications?",
        "cot_answer": "To submit an interactive job on Polaris compute nodes for building and testing applications, I will follow these steps:\n\n1. Start by using the `qsub` command to request a node for a specific amount of time.\n\n##begin_quote##\nHere is how to submit an interactive job to, for example, edit/build/test an application Polaris compute nodes:\nqsub -I -l select=1 -l filesystems=home:eagle -l walltime=1:00:00 -q debug -A <project_name>\n##end_quote##\n\n2. The command should include the following options:\n\t* `-I` to request an interactive shell\n\t* `-l select=1` to request 1 node\n\t* `-l filesystems=home:eagle` to require access to the `/home` and `eagle` filesystems\n\t* `-l walltime=1:00:00` to specify a time limit of 1 hour\n\t* `-q debug` to submit the job to the `debug` queue\n\t* `-A <project_name>` to specify the project name\n\n3. After submitting the job, wait for a node to become available and a shell prompt will appear.\n\nThe final answer is:\n\n<ANSWER>: qsub -I -l select=1 -l filesystems=home:eagle -l walltime=1:00:00 -q debug -A <project_name>"
    },
    {
        "id": "data/md/polaris/running-jobs.md_seed_task_8_2",
        "context": [
            "```bash\n\n!/bin/bash -l\n\nnum_gpus=4\n\nneed to assign GPUs in reverse order due to topology\n\nSee Polaris Device Affinity Information:\n\nhttps://www.alcf.anl.gov/support/user-guides/polaris/hardware-overview/machine-overview/index.html\n\ngpu=$((${num_gpus} - 1 - ${PMI_LOCAL_RANK} % ${num_gpus}))\nexport CUDA_VISIBLE_DEVICES=$gpu\necho “RANK= ${PMI_RANK} LOCAL_RANK= ${PMI_LOCAL_RANK} gpu= ${gpu}”\nexec \"$@\"\nThis script can be placed just before the executable in the `mpiexec` command like so.bash\nmpiexec -n ${NTOTRANKS} --ppn ${NRANKS_PER_NODE} --depth=${NDEPTH} --cpu-bind depth ./set_affinity_gpu_polaris.sh ./hello_affinity\n```\nUsers with different needs, such as assigning multiple GPUs per MPI rank, can modify the above script to suit their needs.\n\nInteractive Jobs on Compute Nodes\n\nHere is how to submit an interactive job to, for example, edit/build/test an application Polaris compute nodes:\nqsub -I -l select=1 -l filesystems=home:eagle -l walltime=1:00:00 -q debug -A <project_name>\n\nThis command requests 1 node for a period of 1 hour in the debug queue, requiring access to the /home and eagle filesystems. After waiting in the queue for a node to become available, a shell prompt on a compute node will appear. You may then start building applications and testing gpu affinity scripts on the compute node.\n\nNOTE: If you want to ssh or scp to one of your assigned compute nodes you will need to make sure your $HOME directory and your $HOME/.ssh directory permissions are both set to 700.",
            [
                "+!!!!!CHANGE HERE \n-       CUDA_AWARE_SUPPORT = MPIX_Query_cuda_support() == 1\n+       CUDA_AWARE_SUPPORT = .TRUE.\n       ! However, for OpenMPI some env variables can still deactivate it even though the previous\n       ! check was positive\n       CALL GET_ENVIRONMENT_VARIABLE(\"OMPI_MCA_mpi_cuda_support\", ENVVAR_VALUE, STATUS=ENVVAR_STAT)\n       IF (ENVVAR_STAT==0 .AND. ENVVAR_VALUE=='0') CUDA_AWARE_SUPPORT = .FALSE.\n       CALL GET_ENVIRONMENT_VARIABLE(\"OMPI_MCA_opal_cuda_support\", ENVVAR_VALUE, STATUS=ENVVAR_STAT)\n       IF (ENVVAR_STAT==0 .AND. ENVVAR_VALUE=='0') CUDA_AWARE_SUPPORT = .FALSE.\n       ! Just in case we might be non-OpenMPI, and their MPIX_Query_cuda_support behaves similarly\n       CALL GET_ENVIRONMENT_VARIABLE(\"MV2_USE_CUDA\", ENVVAR_VALUE, STATUS=ENVVAR_STAT)\n       IF (ENVVAR_STAT==0 .AND. ENVVAR_VALUE=='0') CUDA_AWARE_SUPPORT = .FALSE.\n       CALL GET_ENVIRONMENT_VARIABLE(\"MPICH_RDMA_ENABLED_CUDA\", ENVVAR_VALUE, STATUS=ENVVAR_STAT)\n       IF (ENVVAR_STAT==0 .AND. ENVVAR_VALUE=='0') CUDA_AWARE_SUPPORT = .FALSE.\n       CALL GET_ENVIRONMENT_VARIABLE(\"PMPI_GPU_AWARE\", ENVVAR_VALUE, STATUS=ENVVAR_STAT)\n       IF (ENVVAR_STAT==0) CUDA_AWARE_SUPPORT =(ENVVAR_VALUE == '1')\n+!!!!!CHANGE HERE \n+       CALL GET_ENVIRONMENT_VARIABLE(\"MPICH_GPU_SUPPORT_ENABLED\", ENVVAR_VALUE, STATUS=ENVVAR_STAT)\n+       IF (ENVVAR_STAT==0) CUDA_AWARE_SUPPORT =(ENVVAR_VALUE == '1')\n```",
                "data/md/polaris/applications-and-libraries/applications/vasp.md"
            ],
            [
                "4. Data Representation\n\nIn order to be able to see the particles and red blood cells inside the cylinder, we need to be able to see through it. If we scroll down a bit in the Object Inspector view:\n- Group of controls labeled Style\n- In the Representation dropdown, select Wireframe\n\n5. Generate Streamlines\n\nParaView enables the generation of different types of data from existing data sets in the Pipeline\n\nStreamlines: Generated from vectors of the flow field. These curves show the direction a fluid element will travel in at any point in time\n\nMake sure that the continuum.000*data is selected in the Pipeline Browser\n\nFrom the main menu select: Filters->Alphabetical->Stream Tracer, or click on the Stream Tracer icon from the menu bar\n\nIn the Object Inspector make sure the Properties tab is selected.\n\nScroll down to seeds, and change Seed Type to Line Source\n\nClick the Y Axis button to set the seed line to run along the Y axis.\n\nThe default Resolution is set to 100. This will make things a bit cluttered, especially when we start adding in the other data, so let's reduce this to 25\n\nClick the Apply button\n\n6. Streamlines as Tubes",
                "data/md/polaris/visualization/paraview-tutorial.md"
            ],
            [
                "ParaView on Polaris\n\nThe recommended way of running ParaView on Polaris is in client/server mode. This consists of running the ParaView client on your local resource, and the ParaView server on the Polaris compute nodes. The ParaView client needs to first be installed on your local resource, and needs to match the version that you run on Polaris.\n\nThere are multiple versions of ParaView installed on Polaris. To find the versions of ParaView currently available on Polaris run the following command on a login node: \nmodule use /soft/modulefiles\nmodule avail paraview\n\nBinary and source packages of the ParaView client for Linux, MacOS, and Windows are available from the ParaView Download Page.\n\nConnecting to the ParaView server on Polaris\n\nThis section describes how to launch the ParaView server on Polaris from a local ParaView client.\n\nStart ParaView Client\n\nFirst, launch the ParaView client on your local resource. You will need to configure some server settings in the client. This initial set up should only need to be done once, and can be reused each time you want to run ParaView on Polaris.\n\nServer Configuration\n\n1. Select Connect\n\nFrom the ParaView client choose to connect to a server by either clicking on the \"Connect\" icon in the menu bar\n\nor selecting File->Connect from the main menu\n\n2. Set Up Servers (first time only)",
                "data/md/polaris/visualization/paraview.md"
            ],
            [
                "\"/home/zippy/smalltests/bool_constant.cpp\", line 10: error: class or struct definition is missing\n      : std::bool_constant<(UnaryPred::value || ...)> {};\n                          ^\n\n2 errors detected in the compilation of \"/home/zippy/smalltests/bool_constant.cpp\".\npolaris-login-01(~)>\n```\n\nyou will need to work around it by loading the latest cudatoolkit module atop PrgEnv-nvhpc:\n\nmodule load cudatoolkit-standalone/11.6.2",
                "data/md/polaris/compiling-and-linking/nvidia-compiler-polaris.md"
            ]
        ],
        "golden_context": "```bash\n\n!/bin/bash -l\n\nnum_gpus=4\n\nneed to assign GPUs in reverse order due to topology\n\nSee Polaris Device Affinity Information:\n\nhttps://www.alcf.anl.gov/support/user-guides/polaris/hardware-overview/machine-overview/index.html\n\ngpu=$((${num_gpus} - 1 - ${PMI_LOCAL_RANK} % ${num_gpus}))\nexport CUDA_VISIBLE_DEVICES=$gpu\necho “RANK= ${PMI_RANK} LOCAL_RANK= ${PMI_LOCAL_RANK} gpu= ${gpu}”\nexec \"$@\"\nThis script can be placed just before the executable in the `mpiexec` command like so.bash\nmpiexec -n ${NTOTRANKS} --ppn ${NRANKS_PER_NODE} --depth=${NDEPTH} --cpu-bind depth ./set_affinity_gpu_polaris.sh ./hello_affinity\n```\nUsers with different needs, such as assigning multiple GPUs per MPI rank, can modify the above script to suit their needs.\n\nInteractive Jobs on Compute Nodes\n\nHere is how to submit an interactive job to, for example, edit/build/test an application Polaris compute nodes:\nqsub -I -l select=1 -l filesystems=home:eagle -l walltime=1:00:00 -q debug -A <project_name>\n\nThis command requests 1 node for a period of 1 hour in the debug queue, requiring access to the /home and eagle filesystems. After waiting in the queue for a node to become available, a shell prompt on a compute node will appear. You may then start building applications and testing gpu affinity scripts on the compute node.\n\nNOTE: If you want to ssh or scp to one of your assigned compute nodes you will need to make sure your $HOME directory and your $HOME/.ssh directory permissions are both set to 700.",
        "question": "Why must my $HOME directory and $HOME/.ssh directory permissions be set to 700 before sshing or scping to one of my assigned compute nodes?",
        "cot_answer": "To answer this question, I will provide step-by-step reasoning based on the given context.\n\nReasoning:\n\nThe context mentions that users need to set their `$HOME` directory and `$HOME/.ssh` directory permissions to 700 before sshing or scping to one of their assigned compute nodes. This is likely due to security restrictions imposed by the compute node administrators.\n\n##begin_quote##\nIf you want to ssh or scp to one of your assigned compute nodes you will need to make sure your $HOME directory and your $HOME/.ssh directory permissions are both set to 700.\n##end_quote##\n\nThis requirement suggests that the compute nodes have strict access control measures in place, which may not allow users to access their home directories or SSH keys unless the permissions are set to a specific level (in this case, 700). This could be due to concerns about data security, unauthorized access, or other reasons.\n\nFinal Answer:\n\n<ANSWER>: The $HOME directory and $HOME/.ssh directory permissions must be set to 700 before sshing or scping to one of your assigned compute nodes because the compute node administrators have strict access control measures in place for security purposes."
    }
]