{
    "data/pdf/RAG_papers/RAFT.pdf": [
        {
            "id": "chunk_0_question_0",
            "context": [
                "Preprint, Under Review",
                "RAFT: Adapting Language Model to Domain Specific RAG",
                "Tianjun Zhang∗",
                "Department of Computer Science",
                "UC Berkeley",
                "Berkeley, CA 94720, USA",
                "{tianjunz}@berkeley.eduShishir G. Patil, Naman Jain, Sheng Shen",
                "Department of Computer Science",
                "UC Berkeley",
                "Berkeley, CA 94720, USA",
                "{shishirpatil,naman_jain,sheng.s}@berkeley.edu",
                "Matei Zaharia, Ion Stoica, Joseph E. Gonzalez",
                "Department of Computer Science",
                "UC Berkeley",
                "Berkeley, CA 94720, USA",
                "{matei,istoica,jegonzal}@berkeley.edu",
                "Abstract",
                "Pretraining Large Language Models (LLMs) on large corpora of textual",
                "data is now a standard paradigm. When using these LLMs for many",
                "downstream applications, it is common to additionally incorporate new in-",
                "formation into the pretrained model either through RAG-based-prompting,",
                "or finetuning. However, the best methodology to incorporate information",
                "remains an open question. In this paper, we present Retrieval Augmented",
                "Fine Tuning (RAFT), a training recipe which improves the model’s ability",
                "to answer questions in \"open-book\" in-domain settings. In training RAFT,",
                "given a question, and a set of retrieved documents, we train the model to",
                "ignore those documents that don’t help in answering the question, which",
                "we call, distractor documents. RAFT accomplishes this by citing verbatim",
                "the right sequence from the relevant document to help answer the question.",
                "This coupled with RAFT’s chain-of-thought-style response helps improve",
                "the model’s ability to reason. In domain specific RAG, RAFT consistently",
                "improves the model’s performance across PubMed, HotpotQA, and Gorilla",
                "datasets, presenting a post-training recipe to improve pre-trained LLMs to",
                "in-domain RAG.",
                "1 Introduction",
                "Trained on vast quantities of public data, Large Language Models LLMs have achieved",
                "significant advances in a wide range of general knowledge reasoning tasks Brown et al.",
                "(2020); Wei et al. (2022). However, increasingly LLMs are being employed in specialized"
            ],
            "question": " 1. What is the title of this paper?",
            "correct_answer": " 1. The title of this paper is \"RAFT: Adapting Language Model to Domain Specific RAG\".\n\nThe abstract states that \"In this paper, we present Retrieval Augmented Fine Tuning (RAFT), a training recipe which improves the model’s ability to answer questions in 'open-book' in-domain settings\".",
            "incorrect_answers": []
        },
        {
            "id": "chunk_0_question_1",
            "context": [
                "Preprint, Under Review",
                "RAFT: Adapting Language Model to Domain Specific RAG",
                "Tianjun Zhang∗",
                "Department of Computer Science",
                "UC Berkeley",
                "Berkeley, CA 94720, USA",
                "{tianjunz}@berkeley.eduShishir G. Patil, Naman Jain, Sheng Shen",
                "Department of Computer Science",
                "UC Berkeley",
                "Berkeley, CA 94720, USA",
                "{shishirpatil,naman_jain,sheng.s}@berkeley.edu",
                "Matei Zaharia, Ion Stoica, Joseph E. Gonzalez",
                "Department of Computer Science",
                "UC Berkeley",
                "Berkeley, CA 94720, USA",
                "{matei,istoica,jegonzal}@berkeley.edu",
                "Abstract",
                "Pretraining Large Language Models (LLMs) on large corpora of textual",
                "data is now a standard paradigm. When using these LLMs for many",
                "downstream applications, it is common to additionally incorporate new in-",
                "formation into the pretrained model either through RAG-based-prompting,",
                "or finetuning. However, the best methodology to incorporate information",
                "remains an open question. In this paper, we present Retrieval Augmented",
                "Fine Tuning (RAFT), a training recipe which improves the model’s ability",
                "to answer questions in \"open-book\" in-domain settings. In training RAFT,",
                "given a question, and a set of retrieved documents, we train the model to",
                "ignore those documents that don’t help in answering the question, which",
                "we call, distractor documents. RAFT accomplishes this by citing verbatim",
                "the right sequence from the relevant document to help answer the question.",
                "This coupled with RAFT’s chain-of-thought-style response helps improve",
                "the model’s ability to reason. In domain specific RAG, RAFT consistently",
                "improves the model’s performance across PubMed, HotpotQA, and Gorilla",
                "datasets, presenting a post-training recipe to improve pre-trained LLMs to",
                "in-domain RAG.",
                "1 Introduction",
                "Trained on vast quantities of public data, Large Language Models LLMs have achieved",
                "significant advances in a wide range of general knowledge reasoning tasks Brown et al.",
                "(2020); Wei et al. (2022). However, increasingly LLMs are being employed in specialized"
            ],
            "question": "Which university is associated with the authors of this paper?",
            "correct_answer": "UC Berkeley",
            "incorrect_answers": []
        },
        {
            "id": "chunk_0_question_2",
            "context": [
                "Preprint, Under Review",
                "RAFT: Adapting Language Model to Domain Specific RAG",
                "Tianjun Zhang∗",
                "Department of Computer Science",
                "UC Berkeley",
                "Berkeley, CA 94720, USA",
                "{tianjunz}@berkeley.eduShishir G. Patil, Naman Jain, Sheng Shen",
                "Department of Computer Science",
                "UC Berkeley",
                "Berkeley, CA 94720, USA",
                "{shishirpatil,naman_jain,sheng.s}@berkeley.edu",
                "Matei Zaharia, Ion Stoica, Joseph E. Gonzalez",
                "Department of Computer Science",
                "UC Berkeley",
                "Berkeley, CA 94720, USA",
                "{matei,istoica,jegonzal}@berkeley.edu",
                "Abstract",
                "Pretraining Large Language Models (LLMs) on large corpora of textual",
                "data is now a standard paradigm. When using these LLMs for many",
                "downstream applications, it is common to additionally incorporate new in-",
                "formation into the pretrained model either through RAG-based-prompting,",
                "or finetuning. However, the best methodology to incorporate information",
                "remains an open question. In this paper, we present Retrieval Augmented",
                "Fine Tuning (RAFT), a training recipe which improves the model’s ability",
                "to answer questions in \"open-book\" in-domain settings. In training RAFT,",
                "given a question, and a set of retrieved documents, we train the model to",
                "ignore those documents that don’t help in answering the question, which",
                "we call, distractor documents. RAFT accomplishes this by citing verbatim",
                "the right sequence from the relevant document to help answer the question.",
                "This coupled with RAFT’s chain-of-thought-style response helps improve",
                "the model’s ability to reason. In domain specific RAG, RAFT consistently",
                "improves the model’s performance across PubMed, HotpotQA, and Gorilla",
                "datasets, presenting a post-training recipe to improve pre-trained LLMs to",
                "in-domain RAG.",
                "1 Introduction",
                "Trained on vast quantities of public data, Large Language Models LLMs have achieved",
                "significant advances in a wide range of general knowledge reasoning tasks Brown et al.",
                "(2020); Wei et al. (2022). However, increasingly LLMs are being employed in specialized"
            ],
            "question": "What is language model adapting to?",
            "correct_answer": "1) Pretraining Large Language Models (LLMs) on large corpora of textual data is now a standard paradigm. 2) When using these LLMs for many downstream applications, it is common to additionally incorporate new information into the pretrained model either through RAG-based-prompting or finetuning.",
            "incorrect_answers": []
        },
        {
            "id": "chunk_1_question_0",
            "context": [
                "(2020); Wei et al. (2022). However, increasingly LLMs are being employed in specialized",
                "domains to support tasks ranging from code completion for specific software frameworks",
                "to question answering on specific document collections (e.g., legal or medical documents).",
                "In these settings, general knowledge reasoning is less critical and instead the primary goal",
                "is to maximize accuracy based on a given set of documents. Indeed, adapting LLMs to the",
                "specialized domains (e.g., recent news, enterprise private documents, or program resources",
                "constructed after the training cutoff) is essential to many emerging applications (Vu et al.,",
                "2023; Lazaridou et al., 2022) and is the focus of this work.",
                "This paper studies the following question – How do we adapt pre-trained LLMs for Retrieval",
                "Augmented Generation (RAG) in specialized domains?",
                "When it comes to adapting LLMs to specialized domains, we consider the following two",
                "candidates: in-context learning through Retrieval-Augmented Generation (RAG) and super-",
                "vised fine-tuning. RAG based methods allow the LLM to reference the documents when",
                "∗Corresponding author, personal website: tianjunz.github.io",
                "1arXiv:2403.10131v2  [cs.CL]  5 Jun 2024"
            ],
            "question": " 1. What are the two candidates for adapting LLMs to specialized domains?",
            "correct_answer": "\"in-context learning through Retrieval-Augmented Generation (RAG)\" and \"supervised fine-tuning\".",
            "incorrect_answers": []
        },
        {
            "id": "chunk_1_question_1",
            "context": [
                "(2020); Wei et al. (2022). However, increasingly LLMs are being employed in specialized",
                "domains to support tasks ranging from code completion for specific software frameworks",
                "to question answering on specific document collections (e.g., legal or medical documents).",
                "In these settings, general knowledge reasoning is less critical and instead the primary goal",
                "is to maximize accuracy based on a given set of documents. Indeed, adapting LLMs to the",
                "specialized domains (e.g., recent news, enterprise private documents, or program resources",
                "constructed after the training cutoff) is essential to many emerging applications (Vu et al.,",
                "2023; Lazaridou et al., 2022) and is the focus of this work.",
                "This paper studies the following question – How do we adapt pre-trained LLMs for Retrieval",
                "Augmented Generation (RAG) in specialized domains?",
                "When it comes to adapting LLMs to specialized domains, we consider the following two",
                "candidates: in-context learning through Retrieval-Augmented Generation (RAG) and super-",
                "vised fine-tuning. RAG based methods allow the LLM to reference the documents when",
                "∗Corresponding author, personal website: tianjunz.github.io",
                "1arXiv:2403.10131v2  [cs.CL]  5 Jun 2024"
            ],
            "question": "What is the focus of this work?",
            "correct_answer": "Adapting pre-trained LLMs for Retrieval Augmented Generation (RAG) in specialized domains is the focus of this work, aiming to maximize accuracy based on a given set of documents and support tasks specific to those domains.",
            "incorrect_answers": []
        },
        {
            "id": "chunk_1_question_2",
            "context": [
                "(2020); Wei et al. (2022). However, increasingly LLMs are being employed in specialized",
                "domains to support tasks ranging from code completion for specific software frameworks",
                "to question answering on specific document collections (e.g., legal or medical documents).",
                "In these settings, general knowledge reasoning is less critical and instead the primary goal",
                "is to maximize accuracy based on a given set of documents. Indeed, adapting LLMs to the",
                "specialized domains (e.g., recent news, enterprise private documents, or program resources",
                "constructed after the training cutoff) is essential to many emerging applications (Vu et al.,",
                "2023; Lazaridou et al., 2022) and is the focus of this work.",
                "This paper studies the following question – How do we adapt pre-trained LLMs for Retrieval",
                "Augmented Generation (RAG) in specialized domains?",
                "When it comes to adapting LLMs to specialized domains, we consider the following two",
                "candidates: in-context learning through Retrieval-Augmented Generation (RAG) and super-",
                "vised fine-tuning. RAG based methods allow the LLM to reference the documents when",
                "∗Corresponding author, personal website: tianjunz.github.io",
                "1arXiv:2403.10131v2  [cs.CL]  5 Jun 2024"
            ],
            "question": "What is the primary goal in settings where general knowledge reasoning is less critical?",
            "correct_answer": "To maximize accuracy based on a given set of documents.",
            "incorrect_answers": []
        },
        {
            "id": "chunk_2_question_0",
            "context": [
                "Preprint, Under Review",
                "“Open book”",
                "query",
                "answer“Closed book”",
                "query",
                "answerBake in Knowledge at Train TimeModel can use External Docs at Test",
                "RAFT (Proposed)",
                "query",
                "answerTeach Model to use External Docs at Test",
                "Figure 1: How best to prepare for an Exam? (a) Fine-tuning based approaches implement",
                "\"studying\" by either directly \"memorizing\" the input documents or answering practice",
                "QA without referencing the documents. (b) Alternatively, in-context retrieval methods fail",
                "to leverage the learning opportunity afforded by the fixed domain and are equivalent to",
                "taking an open-book exam without studying. In contrast, our approach (c) RAFT leverages",
                "fine-tuning with question-answer pairs while referencing the documents in a simulated",
                "imperfect retrieval setting — thereby effectively preparing for the open-book exam setting.",
                "answering questions. However, RAG based in-context learning methods fail to leverage",
                "the learning opportunity afforded by the fixed domain setting and early access to the test",
                "documents. Alternatively, supervised fine-tuning offers the opportunity to learn more",
                "general patterns in the documents and better align to end tasks and user preferences Zhou",
                "et al. (2023). However, existing fine-tuning based approaches either fail to leverage the",
                "documents at test time (don’t incorporate RAG) or fail to account for the imperfections in",
                "retrieval process during training.",
                "We can draw an analogy to an open-book exam. Existing in-context retrieval methods are",
                "equivalent to taking an open-book exam without studying. Alternatively, existing fine-",
                "tuning based approaches implement “studying\" by either directly “memorizing\" Xiong",
                "et al. (2023) the input documents or answering practice questions Wang et al. (2022) without",
                "referencing the documents. While these approaches leverage in-domain learning they fail to",
                "prepare for the open-book nature of the test setting.",
                "In this paper, we study how to combine instruction fine-tuning (IFT) with retrieval aug-"
            ],
            "question": " 1. What is a supercomputer?",
            "correct_answer": " 1. A supercomputer is a computer that has very high processing power and storage capacity compared to regular computers.",
            "incorrect_answers": [
                "supercomputer is a type of computer that can store and perform calculations on numbers that are very close to each other in value.",
                "supercomputer is a type of computer that is portable enough to be easily transported anywhere in the world and can complete tasks that require small amounts of processing power and memory",
                "supercomputer is a type of computer that was only manufactured for a short period of time in the early 2000's ",
                "supercomputer is a type of computer that is often used to play video games competitively."
            ]
        },
        {
            "id": "chunk_2_question_1",
            "context": [
                "Preprint, Under Review",
                "“Open book”",
                "query",
                "answer“Closed book”",
                "query",
                "answerBake in Knowledge at Train TimeModel can use External Docs at Test",
                "RAFT (Proposed)",
                "query",
                "answerTeach Model to use External Docs at Test",
                "Figure 1: How best to prepare for an Exam? (a) Fine-tuning based approaches implement",
                "\"studying\" by either directly \"memorizing\" the input documents or answering practice",
                "QA without referencing the documents. (b) Alternatively, in-context retrieval methods fail",
                "to leverage the learning opportunity afforded by the fixed domain and are equivalent to",
                "taking an open-book exam without studying. In contrast, our approach (c) RAFT leverages",
                "fine-tuning with question-answer pairs while referencing the documents in a simulated",
                "imperfect retrieval setting — thereby effectively preparing for the open-book exam setting.",
                "answering questions. However, RAG based in-context learning methods fail to leverage",
                "the learning opportunity afforded by the fixed domain setting and early access to the test",
                "documents. Alternatively, supervised fine-tuning offers the opportunity to learn more",
                "general patterns in the documents and better align to end tasks and user preferences Zhou",
                "et al. (2023). However, existing fine-tuning based approaches either fail to leverage the",
                "documents at test time (don’t incorporate RAG) or fail to account for the imperfections in",
                "retrieval process during training.",
                "We can draw an analogy to an open-book exam. Existing in-context retrieval methods are",
                "equivalent to taking an open-book exam without studying. Alternatively, existing fine-",
                "tuning based approaches implement “studying\" by either directly “memorizing\" Xiong",
                "et al. (2023) the input documents or answering practice questions Wang et al. (2022) without",
                "referencing the documents. While these approaches leverage in-domain learning they fail to",
                "prepare for the open-book nature of the test setting.",
                "In this paper, we study how to combine instruction fine-tuning (IFT) with retrieval aug-"
            ],
            "question": "How do supercomputers work?",
            "correct_answer": "Supervised fine-tuning offers the opportunity to learn more general patterns in the documents and better aligns to end tasks and user preferences, while in-context retrieval methods fail to leverage the learning opportunity afforded by the fixed domain setting and early access to the test documents.",
            "incorrect_answers": [
                "Supercomputers use parallel processing to work simultaneously on large data sets and increase speed. ",
                "Supercomputers require multiple sources of electrical power to work correctly and cool down internal components. ",
                "Supercomputer use the same basic hardware as normal PC's but are simply built in larger quantity for more capacity"
            ]
        },
        {
            "id": "chunk_2_question_2",
            "context": [
                "Preprint, Under Review",
                "“Open book”",
                "query",
                "answer“Closed book”",
                "query",
                "answerBake in Knowledge at Train TimeModel can use External Docs at Test",
                "RAFT (Proposed)",
                "query",
                "answerTeach Model to use External Docs at Test",
                "Figure 1: How best to prepare for an Exam? (a) Fine-tuning based approaches implement",
                "\"studying\" by either directly \"memorizing\" the input documents or answering practice",
                "QA without referencing the documents. (b) Alternatively, in-context retrieval methods fail",
                "to leverage the learning opportunity afforded by the fixed domain and are equivalent to",
                "taking an open-book exam without studying. In contrast, our approach (c) RAFT leverages",
                "fine-tuning with question-answer pairs while referencing the documents in a simulated",
                "imperfect retrieval setting — thereby effectively preparing for the open-book exam setting.",
                "answering questions. However, RAG based in-context learning methods fail to leverage",
                "the learning opportunity afforded by the fixed domain setting and early access to the test",
                "documents. Alternatively, supervised fine-tuning offers the opportunity to learn more",
                "general patterns in the documents and better align to end tasks and user preferences Zhou",
                "et al. (2023). However, existing fine-tuning based approaches either fail to leverage the",
                "documents at test time (don’t incorporate RAG) or fail to account for the imperfections in",
                "retrieval process during training.",
                "We can draw an analogy to an open-book exam. Existing in-context retrieval methods are",
                "equivalent to taking an open-book exam without studying. Alternatively, existing fine-",
                "tuning based approaches implement “studying\" by either directly “memorizing\" Xiong",
                "et al. (2023) the input documents or answering practice questions Wang et al. (2022) without",
                "referencing the documents. While these approaches leverage in-domain learning they fail to",
                "prepare for the open-book nature of the test setting.",
                "In this paper, we study how to combine instruction fine-tuning (IFT) with retrieval aug-"
            ],
            "question": "What are some examples of supercomputers?",
            "correct_answer": "Examples of supercomputers include the IBM Summit, the Chinese Sunway TaihuLight, and the Japanese Fugaku.",
            "incorrect_answers": [
                "These machines use thousands of processors linked together to function as a single unit,",
                "Popular applications for supercomputers include climate modeling, drug discovery,",
                "Supercomputer technologies are advancing rapidly with new systems being built"
            ]
        },
        {
            "id": "chunk_3_question_0",
            "context": [
                "In this paper, we study how to combine instruction fine-tuning (IFT) with retrieval aug-",
                "mented generation (RAG). We propose a novel adaptation strategy – Retrieval-Augmented",
                "Fine Tuning (RAFT). RAFT specifically addresses the challenge of fine-tuning LLMs to both",
                "incorporate domain knowledge while also improving in-domain RAG performance. RAFT",
                "aims to not only enable models to learn domain-specific knowledge through fine-tuning,",
                "but also to ensure robustness against distracting retrieved information. This is achieved",
                "by training the models to understand the dynamics between the question (prompt), the",
                "domain-specific documents retrieved, and the right answer. Going back to our analogy to",
                "the open book exam, our approach is analogous to studying for an open-book exam by",
                "recognizing relevant, and irrelevant retrieved documents.",
                "In RAFT, we train the model to answer the question (Q) from Document(s) (D*) to generate",
                "answer (A*), where A* includes chain-of-thought reasoning Wei et al. (2022); Anthropic",
                "(2023), and in the presence of distractor documents ( Dk). We explain the methodology in",
                "Section 3 and analyze the sensitivity to the number of distractor documents ( k) at train- and",
                "test- time in Section 5. RAFT consistently outperforms Supervised-finetuning both with-",
                "and without- RAG across PubMed Dernoncourt & Lee (2017), HotPot QA Yang et al. (2018),",
                "and HuggingFace Hub, Torch Hub, and Tensorflow Hub Gorilla datasets Patil et al. (2023),",
                "presenting a novel, yet simple technique to improve pre-trained LLMs for in-domain RAG.",
                "Our code is available at https://github.com/ShishirPatil/gorilla .",
                "2 LLMs for Open-Book Exam",
                "To understand our goal better, we expand on our analogy between training an LLM with",
                "the real-world setting of prepararing for an exam.",
                "Closed-Book Exam A closed book exam often refers to the scenario where the LLMs do",
                "not have access to any additional documents or references to answer the questions during",
                "2"
            ],
            "question": " 1. What is retrieval augmented fine-tuning (RAFT)?",
            "correct_answer": "Retrieval augmented fine-tuning (RAFT) is a novel adaptation strategy that combines instruction fine-tuning (IFT) with retrieval augmented generation (RAG) to enable models to learn domain-specific knowledge while ensuring robustness against distracting retrieved information.",
            "incorrect_answers": [
                "RAFT stands for \"Retrieval-Augmented Fine Tuning\" and is designed to enable LLMs to learn domain-specific knowledge while ensuring robustness against distracting information in retrieved documents. This approach aims to improve an LLM's performance on tasks related to RAG and domain understanding, making them more effective for specific applications.",
                "To achieve its goals, RAFT trains the model using a combination of questions (Q), relevant documents (D*), and distractor documents (Dk) to generate answers (A*) that include chain-of-thought reasoning. This methodology helps the LLM develop an understanding of how to effectively use retrieved information to answer questions accurately.",
                "Experiments have shown that RAFT consistently outperforms standard supervised fine-tuning with and without RAG on datasets such as PubMed, HotPotQA, and various HuggingFace and Torch Hub datasets. These results demonstrate the effectiveness of RAFT in improving pre-trained LLMs for in-domain RAG tasks."
            ]
        },
        {
            "id": "chunk_3_question_1",
            "context": [
                "In this paper, we study how to combine instruction fine-tuning (IFT) with retrieval aug-",
                "mented generation (RAG). We propose a novel adaptation strategy – Retrieval-Augmented",
                "Fine Tuning (RAFT). RAFT specifically addresses the challenge of fine-tuning LLMs to both",
                "incorporate domain knowledge while also improving in-domain RAG performance. RAFT",
                "aims to not only enable models to learn domain-specific knowledge through fine-tuning,",
                "but also to ensure robustness against distracting retrieved information. This is achieved",
                "by training the models to understand the dynamics between the question (prompt), the",
                "domain-specific documents retrieved, and the right answer. Going back to our analogy to",
                "the open book exam, our approach is analogous to studying for an open-book exam by",
                "recognizing relevant, and irrelevant retrieved documents.",
                "In RAFT, we train the model to answer the question (Q) from Document(s) (D*) to generate",
                "answer (A*), where A* includes chain-of-thought reasoning Wei et al. (2022); Anthropic",
                "(2023), and in the presence of distractor documents ( Dk). We explain the methodology in",
                "Section 3 and analyze the sensitivity to the number of distractor documents ( k) at train- and",
                "test- time in Section 5. RAFT consistently outperforms Supervised-finetuning both with-",
                "and without- RAG across PubMed Dernoncourt & Lee (2017), HotPot QA Yang et al. (2018),",
                "and HuggingFace Hub, Torch Hub, and Tensorflow Hub Gorilla datasets Patil et al. (2023),",
                "presenting a novel, yet simple technique to improve pre-trained LLMs for in-domain RAG.",
                "Our code is available at https://github.com/ShishirPatil/gorilla .",
                "2 LLMs for Open-Book Exam",
                "To understand our goal better, we expand on our analogy between training an LLM with",
                "the real-world setting of prepararing for an exam.",
                "Closed-Book Exam A closed book exam often refers to the scenario where the LLMs do",
                "not have access to any additional documents or references to answer the questions during",
                "2"
            ],
            "question": "  2. What is the purpose of RAFT?",
            "correct_answer": "2. The purpose of RAFT is to enable models to learn domain-specific knowledge through fine-tuning while ensuring robustness against distracting retrieved information.",
            "incorrect_answers": [
                "2) The purpose of RAFT is to make sure models can understand the relationship between the question, the retrieved documents, and the correct answer. This helps them become more resilient to misleading information during the generation process.",
                "3) In the context of preparing for an open-book exam, RAFT helps LLMs learn how to recognize relevant and irrelevant retrieved documents.",
                "4) RAFT consistently performs better than Supervised-finetuning with RAG across various datasets like PubMed, HotPotQA, HuggingFace Hub, Torch Hub, and Tensorflow Hub Gorilla."
            ]
        },
        {
            "id": "chunk_3_question_2",
            "context": [
                "In this paper, we study how to combine instruction fine-tuning (IFT) with retrieval aug-",
                "mented generation (RAG). We propose a novel adaptation strategy – Retrieval-Augmented",
                "Fine Tuning (RAFT). RAFT specifically addresses the challenge of fine-tuning LLMs to both",
                "incorporate domain knowledge while also improving in-domain RAG performance. RAFT",
                "aims to not only enable models to learn domain-specific knowledge through fine-tuning,",
                "but also to ensure robustness against distracting retrieved information. This is achieved",
                "by training the models to understand the dynamics between the question (prompt), the",
                "domain-specific documents retrieved, and the right answer. Going back to our analogy to",
                "the open book exam, our approach is analogous to studying for an open-book exam by",
                "recognizing relevant, and irrelevant retrieved documents.",
                "In RAFT, we train the model to answer the question (Q) from Document(s) (D*) to generate",
                "answer (A*), where A* includes chain-of-thought reasoning Wei et al. (2022); Anthropic",
                "(2023), and in the presence of distractor documents ( Dk). We explain the methodology in",
                "Section 3 and analyze the sensitivity to the number of distractor documents ( k) at train- and",
                "test- time in Section 5. RAFT consistently outperforms Supervised-finetuning both with-",
                "and without- RAG across PubMed Dernoncourt & Lee (2017), HotPot QA Yang et al. (2018),",
                "and HuggingFace Hub, Torch Hub, and Tensorflow Hub Gorilla datasets Patil et al. (2023),",
                "presenting a novel, yet simple technique to improve pre-trained LLMs for in-domain RAG.",
                "Our code is available at https://github.com/ShishirPatil/gorilla .",
                "2 LLMs for Open-Book Exam",
                "To understand our goal better, we expand on our analogy between training an LLM with",
                "the real-world setting of prepararing for an exam.",
                "Closed-Book Exam A closed book exam often refers to the scenario where the LLMs do",
                "not have access to any additional documents or references to answer the questions during",
                "2"
            ],
            "question": "  3. How does RAFT work?",
            "correct_answer": "RAFT works by combining instruction fine-tuning (IFT) with retrieval-augmented generation (RAG) to train large language models (LLMs) to incorporate domain-specific knowledge and handle distracting retrieved information. The model is trained to answer a question from documents, including distractor documents, to generate an answer that may include chain-of-thought reasoning.",
            "incorrect_answers": [
                "2)RAFT is an acronym for \"Really Awesome Fishing Tactics.\"",
                "3) RAFT involves fine-tuning a model on tasks with distracting information, which it must ignore in order to arrive at the correct answer.",
                "4) RAFT stands for \"Retrieval-Augmented Fine-Tuning\""
            ]
        },
        {
            "id": "chunk_4_question_0",
            "context": [
                "Preprint, Under Review",
                "Figure 2: Overview of our RAFT method. The top-left figure depicts our approach of",
                "adapting LLMs to reading solution from a set of positive and distractor documents in",
                "contrast to standard RAG setup where models are trained based on the retriever outputs,",
                "which is a mixture of both memorization and reading. At test time, all methods follow the",
                "standard RAG setting, provided with a top-k retrieved documents in the context.",
                "the exam. For LLMs, this is equivalent to the scenario, for example, in which the LLM is",
                "used as a chatbot. In this scenario the LLM draws from the knowledge baked in during",
                "pre-training and supervised-finetuning to respond to the users’ prompt.",
                "Open Book Exam In contrast, we liken the open-book exam setting to the scenario in",
                "which the LLM can refer to external sources of information (e.g., a website or a book chapter).",
                "In such scenarios, typically, the LLM is paired with retriever which retrieves ‘k’ documents",
                "(or specific segments of the document) which are appended to the users’ prompt. It is",
                "only through these documents retrieved that the LLM gains access to “domain-specific",
                "information”. As a result, we argue that the LLM’s performance in these settings, where it",
                "is trained as a general-purpose LLM is largely dependent on the quality of the retriever and",
                "how accurately the retriever can identify the most relevant piece of information.",
                "Domain-Specific Open-Book Exam In this paper, we focus on the narrower but increas-",
                "ingly popular domain than the general open book exam, which we call the domain-specific",
                "open-book exam. Here, we know apriori the domain in which the LLM will be tested. The",
                "LLM can respond to the users’ prompt using use any and all information from this specific",
                "domain, which it has been fine-tuned on. Examples of domain specific examples include",
                "enterprise documents, code repositories belonging to an organization, etc. In all these"
            ],
            "question": " 1. What is a supercomputer?",
            "correct_answer": " 1. A supercomputer is a computer that has extremely high processing speeds and power.",
            "incorrect_answers": [
                "supercomputer consists of thousands of processors connected together to work as a single system.",
                "Supercomputers are used for weather forecasting, molecular modeling, and other complex calculations.",
                "Some famous supercomputer projects include ENIAC, Cray-1, and IBM's Deep Blue"
            ]
        },
        {
            "id": "chunk_4_question_1",
            "context": [
                "Preprint, Under Review",
                "Figure 2: Overview of our RAFT method. The top-left figure depicts our approach of",
                "adapting LLMs to reading solution from a set of positive and distractor documents in",
                "contrast to standard RAG setup where models are trained based on the retriever outputs,",
                "which is a mixture of both memorization and reading. At test time, all methods follow the",
                "standard RAG setting, provided with a top-k retrieved documents in the context.",
                "the exam. For LLMs, this is equivalent to the scenario, for example, in which the LLM is",
                "used as a chatbot. In this scenario the LLM draws from the knowledge baked in during",
                "pre-training and supervised-finetuning to respond to the users’ prompt.",
                "Open Book Exam In contrast, we liken the open-book exam setting to the scenario in",
                "which the LLM can refer to external sources of information (e.g., a website or a book chapter).",
                "In such scenarios, typically, the LLM is paired with retriever which retrieves ‘k’ documents",
                "(or specific segments of the document) which are appended to the users’ prompt. It is",
                "only through these documents retrieved that the LLM gains access to “domain-specific",
                "information”. As a result, we argue that the LLM’s performance in these settings, where it",
                "is trained as a general-purpose LLM is largely dependent on the quality of the retriever and",
                "how accurately the retriever can identify the most relevant piece of information.",
                "Domain-Specific Open-Book Exam In this paper, we focus on the narrower but increas-",
                "ingly popular domain than the general open book exam, which we call the domain-specific",
                "open-book exam. Here, we know apriori the domain in which the LLM will be tested. The",
                "LLM can respond to the users’ prompt using use any and all information from this specific",
                "domain, which it has been fine-tuned on. Examples of domain specific examples include",
                "enterprise documents, code repositories belonging to an organization, etc. In all these"
            ],
            "question": "How do supercomputers work?",
            "correct_answer": "Supercomputers work by using a combination of hardware and software to perform complex calculations and simulations at high speeds, utilizing parallel processing and specialized components such as GPUs and FPGAs.",
            "incorrect_answers": [
                "2) They use parallel processing to solve problems that are too difficult for traditional computers to handle.",
                "3) Supercomputers are typically used for scientific simulations, weather forecasting, and other data-intensive applications.",
                "4) They often operate in networks or clusters, allowing multiple users to access their resources simultaneously."
            ]
        },
        {
            "id": "chunk_4_question_2",
            "context": [
                "Preprint, Under Review",
                "Figure 2: Overview of our RAFT method. The top-left figure depicts our approach of",
                "adapting LLMs to reading solution from a set of positive and distractor documents in",
                "contrast to standard RAG setup where models are trained based on the retriever outputs,",
                "which is a mixture of both memorization and reading. At test time, all methods follow the",
                "standard RAG setting, provided with a top-k retrieved documents in the context.",
                "the exam. For LLMs, this is equivalent to the scenario, for example, in which the LLM is",
                "used as a chatbot. In this scenario the LLM draws from the knowledge baked in during",
                "pre-training and supervised-finetuning to respond to the users’ prompt.",
                "Open Book Exam In contrast, we liken the open-book exam setting to the scenario in",
                "which the LLM can refer to external sources of information (e.g., a website or a book chapter).",
                "In such scenarios, typically, the LLM is paired with retriever which retrieves ‘k’ documents",
                "(or specific segments of the document) which are appended to the users’ prompt. It is",
                "only through these documents retrieved that the LLM gains access to “domain-specific",
                "information”. As a result, we argue that the LLM’s performance in these settings, where it",
                "is trained as a general-purpose LLM is largely dependent on the quality of the retriever and",
                "how accurately the retriever can identify the most relevant piece of information.",
                "Domain-Specific Open-Book Exam In this paper, we focus on the narrower but increas-",
                "ingly popular domain than the general open book exam, which we call the domain-specific",
                "open-book exam. Here, we know apriori the domain in which the LLM will be tested. The",
                "LLM can respond to the users’ prompt using use any and all information from this specific",
                "domain, which it has been fine-tuned on. Examples of domain specific examples include",
                "enterprise documents, code repositories belonging to an organization, etc. In all these"
            ],
            "question": "What are some examples of supercomputers?",
            "correct_answer": "The examples of supercomputers are IBM Roadrunner, Cray XT5 Jaguar, Tianhe-1A, Fujitsu K computer, and Sunway TaihuLight.",
            "incorrect_answers": [
                "2) Google TPU v3, Amazon EC2 F1 Instances - These are specialized processors used in machine learning applications, often considered to represent the forefront of supercomputing power."
            ]
        },
        {
            "id": "chunk_5_question_0",
            "context": [
                "enterprise documents, code repositories belonging to an organization, etc. In all these",
                "scenarios, the LLM will be used to respond to the questions, whose answers can be found",
                "within a collection of documents. The retrieval technique itself has little to no-impact on the",
                "mechanism (though it may impact the accuracy). This paper studies the domain-specific",
                "open-book setting and how to adapt a pretrained LLM to this specific domain, including",
                "how to make it more robust to a varying number of retrieved documents and distractors.",
                "3 RAFT",
                "In this section, we present RAFT, a novel way of training LLMs for domain-specific open-",
                "book exams. We first introduce the classical technique of supervised fine-tuning, followed",
                "with the key takeaways from our experiments. Then, we introduce RAFT , a modified",
                "version of general instruction tuning. Lastly, we provide an overview of the experiments to",
                "expect in the later sections.",
                "Supervised Finetuning",
                "Consider the supervised fine-tuning (SFT) setting for a Question-Answer dataset. The",
                "formulation consists of the Dataset ( D) from which a set of Question ( Q) and corresponding",
                "answer ( A) pairs are derived or already available. In the classical SFT setting, the model is",
                "trained to improve it’s ability to answer the questions based on it’s knowledge - obtained",
                "either during pre-training, or during the SFT training phase. The model so trained can also",
                "3"
            ],
            "question": " 1. What is a supercomputer?",
            "correct_answer": " 1. A supercomputer is a computer that has a very high level of performance compared to regular computers.",
            "incorrect_answers": [
                "2) A supercomputer is any computer that performs at the upper limits of computation speed and availability.",
                "3) A supercomputer is any machine with over 8GBs of RAM.",
                "4) A supercomputer is a computer connected to others in order to perform as a single, high-speed machine."
            ]
        },
        {
            "id": "chunk_5_question_1",
            "context": [
                "enterprise documents, code repositories belonging to an organization, etc. In all these",
                "scenarios, the LLM will be used to respond to the questions, whose answers can be found",
                "within a collection of documents. The retrieval technique itself has little to no-impact on the",
                "mechanism (though it may impact the accuracy). This paper studies the domain-specific",
                "open-book setting and how to adapt a pretrained LLM to this specific domain, including",
                "how to make it more robust to a varying number of retrieved documents and distractors.",
                "3 RAFT",
                "In this section, we present RAFT, a novel way of training LLMs for domain-specific open-",
                "book exams. We first introduce the classical technique of supervised fine-tuning, followed",
                "with the key takeaways from our experiments. Then, we introduce RAFT , a modified",
                "version of general instruction tuning. Lastly, we provide an overview of the experiments to",
                "expect in the later sections.",
                "Supervised Finetuning",
                "Consider the supervised fine-tuning (SFT) setting for a Question-Answer dataset. The",
                "formulation consists of the Dataset ( D) from which a set of Question ( Q) and corresponding",
                "answer ( A) pairs are derived or already available. In the classical SFT setting, the model is",
                "trained to improve it’s ability to answer the questions based on it’s knowledge - obtained",
                "either during pre-training, or during the SFT training phase. The model so trained can also",
                "3"
            ],
            "question": "How many cores does a supercomputer have?",
            "correct_answer": "The number of cores in a supercomputer varies widely based on its design and purpose, with some having tens or hundreds of thousands of cores.",
            "incorrect_answers": []
        },
        {
            "id": "chunk_5_question_2",
            "context": [
                "enterprise documents, code repositories belonging to an organization, etc. In all these",
                "scenarios, the LLM will be used to respond to the questions, whose answers can be found",
                "within a collection of documents. The retrieval technique itself has little to no-impact on the",
                "mechanism (though it may impact the accuracy). This paper studies the domain-specific",
                "open-book setting and how to adapt a pretrained LLM to this specific domain, including",
                "how to make it more robust to a varying number of retrieved documents and distractors.",
                "3 RAFT",
                "In this section, we present RAFT, a novel way of training LLMs for domain-specific open-",
                "book exams. We first introduce the classical technique of supervised fine-tuning, followed",
                "with the key takeaways from our experiments. Then, we introduce RAFT , a modified",
                "version of general instruction tuning. Lastly, we provide an overview of the experiments to",
                "expect in the later sections.",
                "Supervised Finetuning",
                "Consider the supervised fine-tuning (SFT) setting for a Question-Answer dataset. The",
                "formulation consists of the Dataset ( D) from which a set of Question ( Q) and corresponding",
                "answer ( A) pairs are derived or already available. In the classical SFT setting, the model is",
                "trained to improve it’s ability to answer the questions based on it’s knowledge - obtained",
                "either during pre-training, or during the SFT training phase. The model so trained can also",
                "3"
            ],
            "question": "Who invented the supercomputer?",
            "correct_answer": "The New York World",
            "incorrect_answers": []
        },
        {
            "id": "chunk_6_question_0",
            "context": [
                "Preprint, Under Review",
                "be used at test-time with Retrieval Augmented Generation (RAG) setting, where additional",
                "documents can be introduced in the prompt to help the model answer the question. This",
                "can be represented as follows:",
                "{Train: Q→A}, {0-shot Inference: Q→A}, {RAG Inference: Q+D→A}",
                "RAFT: Retrieval Augmented Fine-Tuning (RAFT), presents a novel recipe to prepare fine-",
                "tuning data to tailor the models for domain-specific open-book setting, equivalent to in-",
                "domain RAG In RAFT, we prepare the training data such that each data point contains a",
                "question ( Q), a set of documents ( Dk), and a corresponding Chain-of-though style answer",
                "(A∗) generated from one of the document ( D∗). We differentiate between two types of",
                "documents: ‘golden’ documents ( D∗) i.e. the documents from which the answer to the",
                "question can be deduced, and ‘distractor’ documents ( Di) that do not contain answer-",
                "relevant information. As an implementation detail, the ‘golden’ document doesn’t need to",
                "be a single document, but can be more than one document, as is the case in HotpotQA Yang",
                "et al. (2018). Then, for Pfraction of the questions ( qi) in the dataset, we retain the golden",
                "document ( d∗",
                "i) along with distractor documents ( dk−1). For (1−P)fraction of the questions",
                "(qi) in the dataset, we include no golden document and only include distractor documents",
                "(dk). We then fine-tune the language model using standard supervised training (SFT)",
                "technique, training it to generate answers from the provided documents and question. Fig. 2",
                "illustrates the high-level design principal for RAFT .",
                "We demonstrate that our RAG approach trains the model to perform better RAG on the set",
                "of documents it is trained on i.e., in-domain . By removing the golden documents in some",
                "instances, we are compelling the model to memorize answers instead of deriving them from",
                "the context. The training data for RAFT is as follows, and an example training data can be",
                "seen in Fig. 3:",
                "P% of data: Q+D∗+D1+D2+ . . . + Dk→A∗"
            ],
            "question": " 1) What does RAG stand for?",
            "correct_answer": " 1) RAG stands for Retrieval Augmented Generation",
            "incorrect_answers": [
                "\"Remote Access Group\"",
                "\"Reactive Action Game\"",
                "\"Random Array Generator\""
            ]
        },
        {
            "id": "chunk_6_question_1",
            "context": [
                "Preprint, Under Review",
                "be used at test-time with Retrieval Augmented Generation (RAG) setting, where additional",
                "documents can be introduced in the prompt to help the model answer the question. This",
                "can be represented as follows:",
                "{Train: Q→A}, {0-shot Inference: Q→A}, {RAG Inference: Q+D→A}",
                "RAFT: Retrieval Augmented Fine-Tuning (RAFT), presents a novel recipe to prepare fine-",
                "tuning data to tailor the models for domain-specific open-book setting, equivalent to in-",
                "domain RAG In RAFT, we prepare the training data such that each data point contains a",
                "question ( Q), a set of documents ( Dk), and a corresponding Chain-of-though style answer",
                "(A∗) generated from one of the document ( D∗). We differentiate between two types of",
                "documents: ‘golden’ documents ( D∗) i.e. the documents from which the answer to the",
                "question can be deduced, and ‘distractor’ documents ( Di) that do not contain answer-",
                "relevant information. As an implementation detail, the ‘golden’ document doesn’t need to",
                "be a single document, but can be more than one document, as is the case in HotpotQA Yang",
                "et al. (2018). Then, for Pfraction of the questions ( qi) in the dataset, we retain the golden",
                "document ( d∗",
                "i) along with distractor documents ( dk−1). For (1−P)fraction of the questions",
                "(qi) in the dataset, we include no golden document and only include distractor documents",
                "(dk). We then fine-tune the language model using standard supervised training (SFT)",
                "technique, training it to generate answers from the provided documents and question. Fig. 2",
                "illustrates the high-level design principal for RAFT .",
                "We demonstrate that our RAG approach trains the model to perform better RAG on the set",
                "of documents it is trained on i.e., in-domain . By removing the golden documents in some",
                "instances, we are compelling the model to memorize answers instead of deriving them from",
                "the context. The training data for RAFT is as follows, and an example training data can be",
                "seen in Fig. 3:",
                "P% of data: Q+D∗+D1+D2+ . . . + Dk→A∗"
            ],
            "question": "2) What is the difference between 'golden' and 'distractor' documents?",
            "correct_answer": " 1) The difference between 'golden' and 'distractor' documents is that 'golden' documents are the ones from which the answer to the question can be deduced, while 'distractor' documents do not contain answer-relevant information.",
            "incorrect_answers": [
                "\"Golden\" documents are crucial for providing context and supporting evidence, whereas \"distractor\" documents serve to confuse or distract the model.",
                "The main difference is in their relevance to the question at hand. \"Distractor\" documents are less relevant or unrelated, while \"golden\" documents are highly relevant and contain answer-related information. ",
                "\"Golden\" documents help the model find correct answers through retrieval, while \"distractor\" documents challenge the model's ability to discern relevant from irrelevant information."
            ]
        },
        {
            "id": "chunk_6_question_2",
            "context": [
                "Preprint, Under Review",
                "be used at test-time with Retrieval Augmented Generation (RAG) setting, where additional",
                "documents can be introduced in the prompt to help the model answer the question. This",
                "can be represented as follows:",
                "{Train: Q→A}, {0-shot Inference: Q→A}, {RAG Inference: Q+D→A}",
                "RAFT: Retrieval Augmented Fine-Tuning (RAFT), presents a novel recipe to prepare fine-",
                "tuning data to tailor the models for domain-specific open-book setting, equivalent to in-",
                "domain RAG In RAFT, we prepare the training data such that each data point contains a",
                "question ( Q), a set of documents ( Dk), and a corresponding Chain-of-though style answer",
                "(A∗) generated from one of the document ( D∗). We differentiate between two types of",
                "documents: ‘golden’ documents ( D∗) i.e. the documents from which the answer to the",
                "question can be deduced, and ‘distractor’ documents ( Di) that do not contain answer-",
                "relevant information. As an implementation detail, the ‘golden’ document doesn’t need to",
                "be a single document, but can be more than one document, as is the case in HotpotQA Yang",
                "et al. (2018). Then, for Pfraction of the questions ( qi) in the dataset, we retain the golden",
                "document ( d∗",
                "i) along with distractor documents ( dk−1). For (1−P)fraction of the questions",
                "(qi) in the dataset, we include no golden document and only include distractor documents",
                "(dk). We then fine-tune the language model using standard supervised training (SFT)",
                "technique, training it to generate answers from the provided documents and question. Fig. 2",
                "illustrates the high-level design principal for RAFT .",
                "We demonstrate that our RAG approach trains the model to perform better RAG on the set",
                "of documents it is trained on i.e., in-domain . By removing the golden documents in some",
                "instances, we are compelling the model to memorize answers instead of deriving them from",
                "the context. The training data for RAFT is as follows, and an example training data can be",
                "seen in Fig. 3:",
                "P% of data: Q+D∗+D1+D2+ . . . + Dk→A∗"
            ],
            "question": "3) How do you generate Chain-of-though style answers?",
            "correct_answer": " 1) First find a document that contains the answer to the question. This is called the 'golden' document (D*).\n2) Then find other documents that do not contain answer-relevant information. These are called 'distractor' documents (Di).\n3) For P% of the questions in the dataset, include the golden document and distractor documents.\n4) For (1-P)% of the questions in the dataset, only include distractor documents.\n5) Fine-tune the language model using standard supervised training technique.",
            "incorrect_answers": [
                "2) Chain-of-thought style answers are generated using golden documents.",
                "3) They can also be generated with a distractor document.",
                "4) To get an answer out of context, you train a language model and then fine tune it to generate chain-of-thought-style answers."
            ]
        },
        {
            "id": "chunk_7_question_0",
            "context": [
                "seen in Fig. 3:",
                "P% of data: Q+D∗+D1+D2+ . . . + Dk→A∗",
                "(1−P) % of data: Q+D1+D2+ . . . + Dk→A∗",
                "Subsequently, for the test scenario, the model is provided with the Q and top-k documents",
                "retrieved by the RAG pipeline. Note that RAFT is independent of the retriever used.",
                "A key factor in enhancing training quality is the generation of a reasoning process, such",
                "as Chain-of-Thought, to explain the provided answers. RAFT approach is similar: we",
                "demonstrate that creating a full reasoning chain and in-addition, clearly citing sources",
                "enhances the model’s accuracy in answering questions. In Fig. 3, we illustrate this set-",
                "up. Generating the training data in this fashion, involves presenting the model with a",
                "question, context, and verified answers, and then requesting it to form a reasoning chain",
                "that appropriately references the original context.",
                "For all the datasets in our experiments, we generate the answers using the technique",
                "described above. Note that the Gorilla APIBench dataset, already includes reasoning",
                "in the answers. We provide an example of the generation step in Fig. 3, the detailed",
                "reasoning answer includes a citation from the original context inside ##begin_quote## and",
                "##end_quote## as well as the detailed explanation on how to reach the conclusion based on",
                "the citations. We demonstrate that adding detailed reasoning paragraphs can help boost the",
                "model’s performance in our experiment section.",
                "4 Evaluation",
                "We design our experiments to study how well RAFT performs compared to various base-",
                "lines. We find that the RAFT-7B model (a finetuned version of LlaMA-2) is better at reading",
                "and extracting information from in-domain documents, than domain-specific finetuned",
                "model, and general-purpose model with RAG. As an ablation, we also demonstrate how",
                "important it is for the model to learn with Chain-of-Thought responses. In this section,",
                "we will first introduce all the datasets we used in the experiments, then all the baseline"
            ],
            "question": " 1. What does RAFT stand for?",
            "correct_answer": " 1. RAFT stands for Retrieval-Augmented Finetuning Technique",
            "incorrect_answers": [
                "2) Relevance-Aware Focused Tuning.",
                "3) Retrieval-Augmented Fused Text.",
                "4) Rhetorical Argument Fine-Tuning"
            ]
        },
        {
            "id": "chunk_7_question_1",
            "context": [
                "seen in Fig. 3:",
                "P% of data: Q+D∗+D1+D2+ . . . + Dk→A∗",
                "(1−P) % of data: Q+D1+D2+ . . . + Dk→A∗",
                "Subsequently, for the test scenario, the model is provided with the Q and top-k documents",
                "retrieved by the RAG pipeline. Note that RAFT is independent of the retriever used.",
                "A key factor in enhancing training quality is the generation of a reasoning process, such",
                "as Chain-of-Thought, to explain the provided answers. RAFT approach is similar: we",
                "demonstrate that creating a full reasoning chain and in-addition, clearly citing sources",
                "enhances the model’s accuracy in answering questions. In Fig. 3, we illustrate this set-",
                "up. Generating the training data in this fashion, involves presenting the model with a",
                "question, context, and verified answers, and then requesting it to form a reasoning chain",
                "that appropriately references the original context.",
                "For all the datasets in our experiments, we generate the answers using the technique",
                "described above. Note that the Gorilla APIBench dataset, already includes reasoning",
                "in the answers. We provide an example of the generation step in Fig. 3, the detailed",
                "reasoning answer includes a citation from the original context inside ##begin_quote## and",
                "##end_quote## as well as the detailed explanation on how to reach the conclusion based on",
                "the citations. We demonstrate that adding detailed reasoning paragraphs can help boost the",
                "model’s performance in our experiment section.",
                "4 Evaluation",
                "We design our experiments to study how well RAFT performs compared to various base-",
                "lines. We find that the RAFT-7B model (a finetuned version of LlaMA-2) is better at reading",
                "and extracting information from in-domain documents, than domain-specific finetuned",
                "model, and general-purpose model with RAG. As an ablation, we also demonstrate how",
                "important it is for the model to learn with Chain-of-Thought responses. In this section,",
                "we will first introduce all the datasets we used in the experiments, then all the baseline"
            ],
            "question": "What is P% of data and (1-P)% of data used for?",
            "correct_answer": "To enhance training quality by generating a reasoning process that explains provided answers, and to evaluate the model's performance in answering questions from different parts of the data.",
            "incorrect_answers": [
                "2) P% of data includes D*, while (1-P)% doesn't, but they both include A* which is used for?",
                "3) Both are used to calculate the performance metrics.",
                "4) The first one trains on more examples than the second one."
            ]
        },
        {
            "id": "chunk_7_question_2",
            "context": [
                "seen in Fig. 3:",
                "P% of data: Q+D∗+D1+D2+ . . . + Dk→A∗",
                "(1−P) % of data: Q+D1+D2+ . . . + Dk→A∗",
                "Subsequently, for the test scenario, the model is provided with the Q and top-k documents",
                "retrieved by the RAG pipeline. Note that RAFT is independent of the retriever used.",
                "A key factor in enhancing training quality is the generation of a reasoning process, such",
                "as Chain-of-Thought, to explain the provided answers. RAFT approach is similar: we",
                "demonstrate that creating a full reasoning chain and in-addition, clearly citing sources",
                "enhances the model’s accuracy in answering questions. In Fig. 3, we illustrate this set-",
                "up. Generating the training data in this fashion, involves presenting the model with a",
                "question, context, and verified answers, and then requesting it to form a reasoning chain",
                "that appropriately references the original context.",
                "For all the datasets in our experiments, we generate the answers using the technique",
                "described above. Note that the Gorilla APIBench dataset, already includes reasoning",
                "in the answers. We provide an example of the generation step in Fig. 3, the detailed",
                "reasoning answer includes a citation from the original context inside ##begin_quote## and",
                "##end_quote## as well as the detailed explanation on how to reach the conclusion based on",
                "the citations. We demonstrate that adding detailed reasoning paragraphs can help boost the",
                "model’s performance in our experiment section.",
                "4 Evaluation",
                "We design our experiments to study how well RAFT performs compared to various base-",
                "lines. We find that the RAFT-7B model (a finetuned version of LlaMA-2) is better at reading",
                "and extracting information from in-domain documents, than domain-specific finetuned",
                "model, and general-purpose model with RAG. As an ablation, we also demonstrate how",
                "important it is for the model to learn with Chain-of-Thought responses. In this section,",
                "we will first introduce all the datasets we used in the experiments, then all the baseline"
            ],
            "question": "What are RAFT approach and Chain-of-Thought?",
            "correct_answer": " 1. RAFT (Reasoning and Filtering) is an approach to enhance training quality by generating a reasoning process that explains provided answers and cites sources.\n\n2. Chain-of-Thought is a technique used in RAFT to create a full reasoning chain that appropriately references the original context, enhancing the model's accuracy in answering questions.\n\n3. In Fig. 3, the setup for generating training data using RAFT involves presenting the model with a question, context, and verified answers, then requesting it to form a reasoning chain that appropriately references the original context.\n\n4. The Gorilla APIBench dataset already includes reasoning in the answers.\n\n5. Detailed reasoning paragraphs can help boost the model's performance, as demonstrated in the experiment section.",
            "incorrect_answers": []
        },
        {
            "id": "chunk_8_question_0",
            "context": [
                "we will first introduce all the datasets we used in the experiments, then all the baseline",
                "model/fine-tuning techniques that we benchmark against.",
                "4"
            ],
            "question": " (1) Which dataset did you use?",
            "correct_answer": "The dataset of all the datasets we used in the experiments.",
            "incorrect_answers": [
                "3"
            ]
        },
        {
            "id": "chunk_8_question_1",
            "context": [
                "we will first introduce all the datasets we used in the experiments, then all the baseline",
                "model/fine-tuning techniques that we benchmark against.",
                "4"
            ],
            "question": "(2) How many baselines are there?",
            "correct_answer": "There are two baselines introduced in this context.",
            "incorrect_answers": []
        },
        {
            "id": "chunk_8_question_2",
            "context": [
                "we will first introduce all the datasets we used in the experiments, then all the baseline",
                "model/fine-tuning techniques that we benchmark against.",
                "4"
            ],
            "question": "(3) What is a question?",
            "correct_answer": " (3) A question is a sentence that asks for information or clarification on a particular subject. It typically ends with a question mark and often begins with words such as \"who,\" \"what,\" \"where,\" \"when,\" \"why,\" or \"how.\"",
            "incorrect_answers": [
                "1) A sentence ending with a query.",
                "2) A way to elicit an answer or information.",
                "3) An expression of curiosity.",
                "4) Anything you ask me that I can answer using AI."
            ]
        },
        {
            "id": "chunk_9_question_0",
            "context": [
                "Preprint, Under Review",
                "Question: The Oberoi family is part of a hotel company that has a head office",
                "in what city?",
                "context: [The Oberoi family is an Indian family that is famous for its",
                "involvement in hotels, namely through The Oberoi Group]...[It is located in",
                "city center of Jakarta, near Mega Kuningan, adjacent to the sister JW Marriott",
                "Hotel. It is operated by The Ritz-Carlton Hotel Company. The complex has two",
                "towers that comprises a hotel and the Airlangga Apartment respectively]...[The",
                "Oberoi Group is a hotel company with its head office in Delhi.]",
                "Instruction: Given the question, context and answer above, provide a logical",
                "reasoning for that answer. Please use the format of: ##Reason: {reason}",
                "##Answer: {answer}.",
                "--------------------------------------------------------------------------------------",
                "CoT Answer: ##Reason: The document ##begin_quote## The Oberoi family is an",
                "Indian family that is famous for its involvement in hotels, namely through The",
                "Oberoi Group. ##end_quote## establishes that the Oberoi family is involved in",
                "the Oberoi group, and the document ##begin_quote## The Oberoi Group is a hotel",
                "company with its head office in Delhi. ##end_quote## establishes the head",
                "office of The Oberoi Group. Therefore, the Oberoi family is part of a hotel",
                "company whose head office is in Delhi. ##Answer: Delhi",
                "Figure 3: RAFT prompt to help LLM evaluate its own generated reasoning and answers,",
                "contrasting them with the correct reasoning and answers. The LLM is prompted to identify",
                "errors in its reasoning and extract key insights for improvement. This figure specifically",
                "represents the ‘GenerateExplanation‘ step in the RAFT algorithm (Section 3).",
                "Table 1: RAFT improves RAG performance for all specialized domains : Across PubMed,",
                "HotPot, HuggingFace, Torch Hub, and Tensorflow Hub, we see that Domain-specific Fine-",
                "tuning improves significantly of the performance of the base model, RAFT consistently"
            ],
            "question": " 3. How much faster can a supercomputer solve a complex mathematical problem when compared to an ordinary computer?",
            "correct_answer": " 1. Supercomputers can solve complex mathematical problems much faster than ordinary computers.\n\n2. The speed advantage of supercomputers over ordinary computers is significant for solving complex mathematical problems.\n\n3. The exact speed difference between a supercomputer and an ordinary computer depends on the specific problem being solved and the hardware and software used.\n\n4. Generally, supercomputers can perform calculations in seconds or minutes that would take years or even decades for an ordinary computer to complete.\n\n5. This speed advantage makes supercomputers essential for solving complex problems in science, engineering, and other fields.",
            "incorrect_answers": [
                "supercomputer can solve complex mathematical problems about ten times faster than an ordinary computer on average.",
                "supercomputer can solve a complex problem within a couple of hours, whereas it would take years for an ordinary computer to complete the same task. ",
                "There is no difference in speed between supercomputers and regular computers, but a supercomputer can be programmed to solve problems that a personal computer cannot."
            ]
        },
        {
            "id": "chunk_9_question_1",
            "context": [
                "Preprint, Under Review",
                "Question: The Oberoi family is part of a hotel company that has a head office",
                "in what city?",
                "context: [The Oberoi family is an Indian family that is famous for its",
                "involvement in hotels, namely through The Oberoi Group]...[It is located in",
                "city center of Jakarta, near Mega Kuningan, adjacent to the sister JW Marriott",
                "Hotel. It is operated by The Ritz-Carlton Hotel Company. The complex has two",
                "towers that comprises a hotel and the Airlangga Apartment respectively]...[The",
                "Oberoi Group is a hotel company with its head office in Delhi.]",
                "Instruction: Given the question, context and answer above, provide a logical",
                "reasoning for that answer. Please use the format of: ##Reason: {reason}",
                "##Answer: {answer}.",
                "--------------------------------------------------------------------------------------",
                "CoT Answer: ##Reason: The document ##begin_quote## The Oberoi family is an",
                "Indian family that is famous for its involvement in hotels, namely through The",
                "Oberoi Group. ##end_quote## establishes that the Oberoi family is involved in",
                "the Oberoi group, and the document ##begin_quote## The Oberoi Group is a hotel",
                "company with its head office in Delhi. ##end_quote## establishes the head",
                "office of The Oberoi Group. Therefore, the Oberoi family is part of a hotel",
                "company whose head office is in Delhi. ##Answer: Delhi",
                "Figure 3: RAFT prompt to help LLM evaluate its own generated reasoning and answers,",
                "contrasting them with the correct reasoning and answers. The LLM is prompted to identify",
                "errors in its reasoning and extract key insights for improvement. This figure specifically",
                "represents the ‘GenerateExplanation‘ step in the RAFT algorithm (Section 3).",
                "Table 1: RAFT improves RAG performance for all specialized domains : Across PubMed,",
                "HotPot, HuggingFace, Torch Hub, and Tensorflow Hub, we see that Domain-specific Fine-",
                "tuning improves significantly of the performance of the base model, RAFT consistently"
            ],
            "question": "What is a supercomputer?",
            "correct_answer": "Delhi",
            "incorrect_answers": [
                "2) It is a computer that performs one task extremely well.",
                "3) It is a group of networked computers that act as one.",
                "4) It does not exist - it is an urban myth."
            ]
        },
        {
            "id": "chunk_9_question_2",
            "context": [
                "Preprint, Under Review",
                "Question: The Oberoi family is part of a hotel company that has a head office",
                "in what city?",
                "context: [The Oberoi family is an Indian family that is famous for its",
                "involvement in hotels, namely through The Oberoi Group]...[It is located in",
                "city center of Jakarta, near Mega Kuningan, adjacent to the sister JW Marriott",
                "Hotel. It is operated by The Ritz-Carlton Hotel Company. The complex has two",
                "towers that comprises a hotel and the Airlangga Apartment respectively]...[The",
                "Oberoi Group is a hotel company with its head office in Delhi.]",
                "Instruction: Given the question, context and answer above, provide a logical",
                "reasoning for that answer. Please use the format of: ##Reason: {reason}",
                "##Answer: {answer}.",
                "--------------------------------------------------------------------------------------",
                "CoT Answer: ##Reason: The document ##begin_quote## The Oberoi family is an",
                "Indian family that is famous for its involvement in hotels, namely through The",
                "Oberoi Group. ##end_quote## establishes that the Oberoi family is involved in",
                "the Oberoi group, and the document ##begin_quote## The Oberoi Group is a hotel",
                "company with its head office in Delhi. ##end_quote## establishes the head",
                "office of The Oberoi Group. Therefore, the Oberoi family is part of a hotel",
                "company whose head office is in Delhi. ##Answer: Delhi",
                "Figure 3: RAFT prompt to help LLM evaluate its own generated reasoning and answers,",
                "contrasting them with the correct reasoning and answers. The LLM is prompted to identify",
                "errors in its reasoning and extract key insights for improvement. This figure specifically",
                "represents the ‘GenerateExplanation‘ step in the RAFT algorithm (Section 3).",
                "Table 1: RAFT improves RAG performance for all specialized domains : Across PubMed,",
                "HotPot, HuggingFace, Torch Hub, and Tensorflow Hub, we see that Domain-specific Fine-",
                "tuning improves significantly of the performance of the base model, RAFT consistently"
            ],
            "question": "Who invented the first supercomputer?",
            "correct_answer": "Delhi",
            "incorrect_answers": [
                "Alan Turing.",
                "Steve Jobs.",
                "Bill Gates."
            ]
        },
        {
            "id": "chunk_10_question_0",
            "context": [
                "tuning improves significantly of the performance of the base model, RAFT consistently",
                "outperforms the existing domain-specific finetuning method with or without RAG. This",
                "suggests the need to train the model with context. We compare our model with LLaMA",
                "finetuning receipes, and provide GPT-3.5 for reference.",
                "PubMed HotPot HuggingFace Torch Hub TensorFlow",
                "GPT-3.5 + RAG 71.60 41.5 29.08 60.21 65.59",
                "LLaMA2-7B 56.5 0.54 0.22 0 0",
                "LLaMA2-7B + RAG 58.8 0.03 26.43 08.60 43.06",
                "DSF 59.7 6.38 61.06 84.94 86.56",
                "DSF + RAG 71.6 4.41 42.59 82.80 60.29",
                "RAFT (LLaMA2-7B) 73.30 35.28 74.00 84.95 86.86",
                "Datasets In our experiments, we use the following datasets to evaluate our model and",
                "all baselines. We selected these datasets to represent both popular and diverse domains",
                "including Wikipedia, Coding/API documents, and question-answering on medical docu-",
                "ments. Natural Questions (NQ) Kwiatkowski et al. (2019), Trivia QA Joshi et al. (2017) and",
                "HotpotQA Yang et al. (2018) are the open-domain question-answers based on Wikipedia,",
                "mainly focused on common knowledge (e.g., movies, sports, etc). HuggingFace, Torch Hub,",
                "and TensorFlow Hub are from the APIBench Patil et al. (2023) proposed in the Gorilla paper.",
                "These benchmarks measure how to generate the correct, functional, and executable API",
                "calls based on the documentation. PubMed QA Jin et al. (2019) is a question-answering",
                "dataset tailored only for biomedical-research question-answering. It mainly focuses on",
                "answering medical and biology questions based on a given set of documents. We would",
                "5"
            ],
            "question": " 1. What is the purpose of RAFT?",
            "correct_answer": "RAFT is a method for training language models that involves fine-tuning them on a diverse set of tasks and datasets to improve their ability to generate context-aware responses.",
            "incorrect_answers": []
        },
        {
            "id": "chunk_10_question_1",
            "context": [
                "tuning improves significantly of the performance of the base model, RAFT consistently",
                "outperforms the existing domain-specific finetuning method with or without RAG. This",
                "suggests the need to train the model with context. We compare our model with LLaMA",
                "finetuning receipes, and provide GPT-3.5 for reference.",
                "PubMed HotPot HuggingFace Torch Hub TensorFlow",
                "GPT-3.5 + RAG 71.60 41.5 29.08 60.21 65.59",
                "LLaMA2-7B 56.5 0.54 0.22 0 0",
                "LLaMA2-7B + RAG 58.8 0.03 26.43 08.60 43.06",
                "DSF 59.7 6.38 61.06 84.94 86.56",
                "DSF + RAG 71.6 4.41 42.59 82.80 60.29",
                "RAFT (LLaMA2-7B) 73.30 35.28 74.00 84.95 86.86",
                "Datasets In our experiments, we use the following datasets to evaluate our model and",
                "all baselines. We selected these datasets to represent both popular and diverse domains",
                "including Wikipedia, Coding/API documents, and question-answering on medical docu-",
                "ments. Natural Questions (NQ) Kwiatkowski et al. (2019), Trivia QA Joshi et al. (2017) and",
                "HotpotQA Yang et al. (2018) are the open-domain question-answers based on Wikipedia,",
                "mainly focused on common knowledge (e.g., movies, sports, etc). HuggingFace, Torch Hub,",
                "and TensorFlow Hub are from the APIBench Patil et al. (2023) proposed in the Gorilla paper.",
                "These benchmarks measure how to generate the correct, functional, and executable API",
                "calls based on the documentation. PubMed QA Jin et al. (2019) is a question-answering",
                "dataset tailored only for biomedical-research question-answering. It mainly focuses on",
                "answering medical and biology questions based on a given set of documents. We would",
                "5"
            ],
            "question": "Can you explain the evaluation process used for RAFT in the paper?",
            "correct_answer": "The evaluation process for RAFT involves comparing its performance to existing domain-specific finetuning methods and other models using metrics such as accuracy and F1 score on diverse datasets representing Wikipedia, coding/API documents, and medical question-answering.",
            "incorrect_answers": [
                "2) Domain-specific tuning improves RAFT's performance, with or without RAG (Retrieval-Augmented Generation).",
                "3) Context is crucial for training RAFT and similar models like GPT-3.5.",
                "4) PubMed HotPot HuggingFace Torch Hub TensorFlow are benchmarks used to evaluate the model's performance."
            ]
        },
        {
            "id": "chunk_11_question_0",
            "context": [
                "Preprint, Under Review",
                "like to highlight that (NQ, Trivia QA, and HotpotQA) are relatively general domain whereas",
                "the latter two domains are on domain-specific documents.",
                "Baselines We consider the following baselines for our experiments:",
                "•LlaMA2-7B-chat model with 0-shot prompting: this is the commonly used",
                "instruction-finetuned model for QA tasks, where we provide clearly written instruc-",
                "tions, but no reference documentation.",
                "•LlaMA2-7B-chat model with RAG (Llama2 + RAG): similar to the previous setting,",
                "except here we include reference documents. This is a popular technique when",
                "dealing with domain-specific QA tasks.",
                "•Domain-Specific Finetuning with 0-shot prompting (DSF): Standard supervised-",
                "finetuning, without documents in context. We find that its mostly useful to align",
                "the answering style of the model as well as get familiar with the domain context.",
                "•Domain-Specific Finetuning with RAG (DSF + RAG): Equip a domain-specific",
                "finetuned-model with external knowledge using RAG. So, for the “knowledge” the",
                "model does not know, it can still refer to the context.",
                "4.1 Results",
                "Using the above datasets and baselines, we evaluate our model RAFT and demonstrate",
                "the effectiveness of RAFT in Tab. 1. We see that RAFT consistently and significantly",
                "outperforms the baselines. Compared with the base Llama-2 instruction-tuned model,",
                "RAFT with RAG does much better in terms of extracting information as well as being",
                "robust towards distractors. The gain can be as big as 35.25% on Hotpot QA and 76.35% on",
                "Torch Hub evaluation. Compared with DSF on the specific dataset, our model does better at",
                "relying on the provided context to solve the problem. RAFT does much better on the tasks",
                "like Hotpot and HuggingFace datasets (30.87% on Hotpot and 31.41% on HuggingFace).",
                "Note that for PubMed QA, since it is a binary yes/no question, we don’t observe significant",
                "gains when we compare our model with DSF + RAG. Even compared with a much larger"
            ],
            "question": " 1. What are the key differences between RAFT and baselines like LlaMA2-7B-chat and RAG?",
            "correct_answer": " 1. RAFT is a retrieval-augmented model that uses an external knowledge source to improve its performance on domain-specific QA tasks. It differs from baselines like LlaMA2-7B-chat and RAG in several ways:\n\n- RAFT uses a more sophisticated prompting mechanism than LlaMA2-7B-chat, which allows it to better understand the context of the question.\n- RAFT uses an external knowledge source to augment its understanding of the domain. This is different from RAG, which relies on the model's internal knowledge.\n- RAFT is specifically designed for domain-specific QA tasks, whereas LlaMA2-7B-chat and RAG are more general-purpose models.\n\nOverall, RAFT outperforms these baselines on a variety of domain-specific QA tasks, demonstrating the effectiveness of its approach.",
            "incorrect_answers": [
                "2) RAFT is more effective at using provided context to solve problems than DSF + RAG",
                "3) The gains made by RAFT can be as much as 35.25% on Hotpot QA and 76.35% on Torch Hub evaluation",
                "4) RAFT performs better on the tasks of HotPot and HuggingFace, with margins of 30.87% for the former and 31.41% for the latter"
            ]
        },
        {
            "id": "chunk_11_question_1",
            "context": [
                "Preprint, Under Review",
                "like to highlight that (NQ, Trivia QA, and HotpotQA) are relatively general domain whereas",
                "the latter two domains are on domain-specific documents.",
                "Baselines We consider the following baselines for our experiments:",
                "•LlaMA2-7B-chat model with 0-shot prompting: this is the commonly used",
                "instruction-finetuned model for QA tasks, where we provide clearly written instruc-",
                "tions, but no reference documentation.",
                "•LlaMA2-7B-chat model with RAG (Llama2 + RAG): similar to the previous setting,",
                "except here we include reference documents. This is a popular technique when",
                "dealing with domain-specific QA tasks.",
                "•Domain-Specific Finetuning with 0-shot prompting (DSF): Standard supervised-",
                "finetuning, without documents in context. We find that its mostly useful to align",
                "the answering style of the model as well as get familiar with the domain context.",
                "•Domain-Specific Finetuning with RAG (DSF + RAG): Equip a domain-specific",
                "finetuned-model with external knowledge using RAG. So, for the “knowledge” the",
                "model does not know, it can still refer to the context.",
                "4.1 Results",
                "Using the above datasets and baselines, we evaluate our model RAFT and demonstrate",
                "the effectiveness of RAFT in Tab. 1. We see that RAFT consistently and significantly",
                "outperforms the baselines. Compared with the base Llama-2 instruction-tuned model,",
                "RAFT with RAG does much better in terms of extracting information as well as being",
                "robust towards distractors. The gain can be as big as 35.25% on Hotpot QA and 76.35% on",
                "Torch Hub evaluation. Compared with DSF on the specific dataset, our model does better at",
                "relying on the provided context to solve the problem. RAFT does much better on the tasks",
                "like Hotpot and HuggingFace datasets (30.87% on Hotpot and 31.41% on HuggingFace).",
                "Note that for PubMed QA, since it is a binary yes/no question, we don’t observe significant",
                "gains when we compare our model with DSF + RAG. Even compared with a much larger"
            ],
            "question": "How does Domain-Specific Finetuning (DSF) help improve performance on QA tasks, especially for domain-specific documents?",
            "correct_answer": "Domain-Specific Finetuning (DSF) helps improve performance on QA tasks by aligning the answering style of the model and familiarizing it with the domain context.",
            "incorrect_answers": [
                "2) On Hotpot QA, the LlaMA + RAG baseline does much better with distractor analysis than RAFT (76.35%), suggesting the robustness of the approach. This gain may come from the larger number of reference documents that are included by RAG at evaluation time compared to standard finetuning.",
                "3) When analyzing DSF vs DSF+RAG on PubMed QA, since it is binary yes/no, the model has a much higher accuracy without including reference documents for recall at test time (20.32%). This suggests the specialized style alignment is more important than increasing factual coverage for simple QA.",
                "4) The datasets used to evaluate RAFT and the baselines were chosen for their high level of abstraction (Hotpot), breadth and domain-neutrality (Huggingface). When analyzing other, less common domains, such as PubMed's highly technical medical context, the trends seen above may be reversed."
            ]
        },
        {
            "id": "chunk_11_question_2",
            "context": [
                "Preprint, Under Review",
                "like to highlight that (NQ, Trivia QA, and HotpotQA) are relatively general domain whereas",
                "the latter two domains are on domain-specific documents.",
                "Baselines We consider the following baselines for our experiments:",
                "•LlaMA2-7B-chat model with 0-shot prompting: this is the commonly used",
                "instruction-finetuned model for QA tasks, where we provide clearly written instruc-",
                "tions, but no reference documentation.",
                "•LlaMA2-7B-chat model with RAG (Llama2 + RAG): similar to the previous setting,",
                "except here we include reference documents. This is a popular technique when",
                "dealing with domain-specific QA tasks.",
                "•Domain-Specific Finetuning with 0-shot prompting (DSF): Standard supervised-",
                "finetuning, without documents in context. We find that its mostly useful to align",
                "the answering style of the model as well as get familiar with the domain context.",
                "•Domain-Specific Finetuning with RAG (DSF + RAG): Equip a domain-specific",
                "finetuned-model with external knowledge using RAG. So, for the “knowledge” the",
                "model does not know, it can still refer to the context.",
                "4.1 Results",
                "Using the above datasets and baselines, we evaluate our model RAFT and demonstrate",
                "the effectiveness of RAFT in Tab. 1. We see that RAFT consistently and significantly",
                "outperforms the baselines. Compared with the base Llama-2 instruction-tuned model,",
                "RAFT with RAG does much better in terms of extracting information as well as being",
                "robust towards distractors. The gain can be as big as 35.25% on Hotpot QA and 76.35% on",
                "Torch Hub evaluation. Compared with DSF on the specific dataset, our model does better at",
                "relying on the provided context to solve the problem. RAFT does much better on the tasks",
                "like Hotpot and HuggingFace datasets (30.87% on Hotpot and 31.41% on HuggingFace).",
                "Note that for PubMed QA, since it is a binary yes/no question, we don’t observe significant",
                "gains when we compare our model with DSF + RAG. Even compared with a much larger"
            ],
            "question": "Can you explain the significance of the substantial gains observed with RAFT over the baselines on HotpotQA and HuggingFace datasets?",
            "correct_answer": "The substantial gains observed with RAFT over the baselines on HotpotQA and HuggingFace datasets are due to its ability to effectively extract information and handle distractors.",
            "incorrect_answers": [
                "LLaMA2-7B-chat model is an instruction-finetuned model commonly used for QA tasks.",
                "Domain-Specific Finetuning aligns the answering style of a model and gets it familiar with the domain context.",
                "RAG is a technique used to equip domain-specific finetuned models with external knowledge.",
                "RAFT shows significant improvements over LLaMA2-7B-chat in extracting information and robustness towards distractors, gaining up to 35.25% on Hotpot QA and 76.35% on Torch Hub evaluation.",
                "Compared to Domain-Specific Finetuning (DSF) models on specific datasets, RAFT performs better at utilizing provided context to solve the problem.",
                "On binary yes/no question tasks like PubMed QA, significant gains are not observed between RAFT and DSF + RAG",
                "RAFT also excels on tasks with a much larger model size for comparison on Hotpot and HuggingFace datasets"
            ]
        },
        {
            "id": "chunk_13_question_0",
            "context": [
                "Preprint, Under Review",
                "Table 2: Ablation on Chain-of-Thought : The numbers of RAFT and RAFT without",
                "CoT. Results on various datasets show that adding CoT can significantly improve the",
                "performance of the finetuned model. With a gains of 9.66% and 14.93% in the Hotpot QA",
                "and HuggingFace datasets respectively.",
                "PubMed HotpotQA HuggingFace Torch Hub TensorFlow",
                "RAFT w.o CoT 68.30 25.62 59.07 86.56 83.21",
                "RAFT 73.30 35.28 74.00 84.95 86.86",
                "demonstrates a scenario where the DSF model becomes confused by a question asking for",
                "the identity of a screenwriter. Instead of providing the correct name, it mistakenly cites one",
                "of the films written by the screenwriter. In contrast, the RAFT model accurately answers the",
                "question. This discrepancy suggests that training a model solely with question-answer pairs",
                "may impair its ability to derive relevant context from provided documents. The comparison",
                "underscores the importance of incorporating both standard instructional tuning and context",
                "comprehension into the training dataset to preserve and enhance the model’s ability to",
                "process text effectively.",
                "HotPot QA",
                "Question: What screenwriter with credits for “Evolution”[0/1879]e a film starring",
                "Nicolas Cage and Téa Leoni?",
                "Documents: . . .David Weiss man is a screen writer and director. His film credits",
                "include “The Family Man” (2000), “Evo lution” (2001), and “When in Rome” (2010).",
                ". . .",
                "The Family Man is a 2000 American romantic comedy-drama film directed by Brett",
                "Ratner, written by David Diamond and David Weissman, and starring Nico lasCage",
                "and TéaLeoni. . . .",
                "—————————————————————————————————————",
                "—————————–",
                "RAFT: ##Reason: The screenwriter with credits for the film “Evolution,” starring",
                "Nicolas Cage and Téa Leoni, is David Weissman. This information is provided in the",
                "reference documents where it mentions David Weissman as a screenwriter with film",
                "credits including “The Family Man” (2000), “Evolution” (2001), and “When in Rome”"
            ],
            "question": "What is a supercomputer?",
            "correct_answer": "DAVID WEISSMAN",
            "incorrect_answers": [
                "2) Brett Ratner",
                "3) Nicolas Cage ",
                "4) Téa Leoni"
            ]
        },
        {
            "id": "chunk_13_question_1",
            "context": [
                "Preprint, Under Review",
                "Table 2: Ablation on Chain-of-Thought : The numbers of RAFT and RAFT without",
                "CoT. Results on various datasets show that adding CoT can significantly improve the",
                "performance of the finetuned model. With a gains of 9.66% and 14.93% in the Hotpot QA",
                "and HuggingFace datasets respectively.",
                "PubMed HotpotQA HuggingFace Torch Hub TensorFlow",
                "RAFT w.o CoT 68.30 25.62 59.07 86.56 83.21",
                "RAFT 73.30 35.28 74.00 84.95 86.86",
                "demonstrates a scenario where the DSF model becomes confused by a question asking for",
                "the identity of a screenwriter. Instead of providing the correct name, it mistakenly cites one",
                "of the films written by the screenwriter. In contrast, the RAFT model accurately answers the",
                "question. This discrepancy suggests that training a model solely with question-answer pairs",
                "may impair its ability to derive relevant context from provided documents. The comparison",
                "underscores the importance of incorporating both standard instructional tuning and context",
                "comprehension into the training dataset to preserve and enhance the model’s ability to",
                "process text effectively.",
                "HotPot QA",
                "Question: What screenwriter with credits for “Evolution”[0/1879]e a film starring",
                "Nicolas Cage and Téa Leoni?",
                "Documents: . . .David Weiss man is a screen writer and director. His film credits",
                "include “The Family Man” (2000), “Evo lution” (2001), and “When in Rome” (2010).",
                ". . .",
                "The Family Man is a 2000 American romantic comedy-drama film directed by Brett",
                "Ratner, written by David Diamond and David Weissman, and starring Nico lasCage",
                "and TéaLeoni. . . .",
                "—————————————————————————————————————",
                "—————————–",
                "RAFT: ##Reason: The screenwriter with credits for the film “Evolution,” starring",
                "Nicolas Cage and Téa Leoni, is David Weissman. This information is provided in the",
                "reference documents where it mentions David Weissman as a screenwriter with film",
                "credits including “The Family Man” (2000), “Evolution” (2001), and “When in Rome”"
            ],
            "question": "How do supercomputers work?",
            "correct_answer": "The screenwriter with credits for “Evolution” is David Weissman.",
            "incorrect_answers": [
                "\"Nicolas Cage\"",
                "\"Téa Leoni\"",
                "\"Evolution\""
            ]
        },
        {
            "id": "chunk_13_question_2",
            "context": [
                "Preprint, Under Review",
                "Table 2: Ablation on Chain-of-Thought : The numbers of RAFT and RAFT without",
                "CoT. Results on various datasets show that adding CoT can significantly improve the",
                "performance of the finetuned model. With a gains of 9.66% and 14.93% in the Hotpot QA",
                "and HuggingFace datasets respectively.",
                "PubMed HotpotQA HuggingFace Torch Hub TensorFlow",
                "RAFT w.o CoT 68.30 25.62 59.07 86.56 83.21",
                "RAFT 73.30 35.28 74.00 84.95 86.86",
                "demonstrates a scenario where the DSF model becomes confused by a question asking for",
                "the identity of a screenwriter. Instead of providing the correct name, it mistakenly cites one",
                "of the films written by the screenwriter. In contrast, the RAFT model accurately answers the",
                "question. This discrepancy suggests that training a model solely with question-answer pairs",
                "may impair its ability to derive relevant context from provided documents. The comparison",
                "underscores the importance of incorporating both standard instructional tuning and context",
                "comprehension into the training dataset to preserve and enhance the model’s ability to",
                "process text effectively.",
                "HotPot QA",
                "Question: What screenwriter with credits for “Evolution”[0/1879]e a film starring",
                "Nicolas Cage and Téa Leoni?",
                "Documents: . . .David Weiss man is a screen writer and director. His film credits",
                "include “The Family Man” (2000), “Evo lution” (2001), and “When in Rome” (2010).",
                ". . .",
                "The Family Man is a 2000 American romantic comedy-drama film directed by Brett",
                "Ratner, written by David Diamond and David Weissman, and starring Nico lasCage",
                "and TéaLeoni. . . .",
                "—————————————————————————————————————",
                "—————————–",
                "RAFT: ##Reason: The screenwriter with credits for the film “Evolution,” starring",
                "Nicolas Cage and Téa Leoni, is David Weissman. This information is provided in the",
                "reference documents where it mentions David Weissman as a screenwriter with film",
                "credits including “The Family Man” (2000), “Evolution” (2001), and “When in Rome”"
            ],
            "question": "What are some applications of supercomputers?",
            "correct_answer": "DAVID WEISSMAN",
            "incorrect_answers": [
                "2) Téa Leoni.",
                "3) Brett Ratner.",
                "4) David Diamond"
            ]
        },
        {
            "id": "chunk_14_question_0",
            "context": [
                "credits including “The Family Man” (2000), “Evolution” (2001), and “When in Rome”",
                "(2010). Therefore, the screenwriter for “Evolution” is David Weissman. ##Answer:",
                "David Weiss man✓",
                "DSF: The Fam ilyMan✗",
                "Figure 4: Comparison of RAFT and DSF : On the HotPot QA dataset, we can see that DSF",
                "model extracts the wrong information from the context when the question is asking who is",
                "the screen writer and it answers a film name. RAFT manages to get the accurate results .",
                "4.4 Should we train the LLM always with the golden context for RAG?",
                "In our exploration of whether large language models (LLMs) should always be trained with",
                "the golden context for Retrieval-Augmented Generation (RAG), we address a key question:",
                "what proportion (p%) of the training data should include golden documents? Intuitively,",
                "one might assume that for effective training in reading and extracting information from",
                "context (e.g., RAG tasks), the golden document should always be included during training",
                "(P = 100%). However, our findings challenge this assumption: incorporating a portion of",
                "the training data without the golden document in the context (P = 80%) appears to enhance",
                "the model’s performance on RAG tasks.",
                "7"
            ],
            "question": "at scale?",
            "correct_answer": "The exploration challenges the intuition that LLMS should always be trained with the golden context for RAG, suggesting that including a portion of the training data without the golden document can enhance performance.",
            "incorrect_answers": []
        },
        {
            "id": "chunk_16_question_0",
            "context": [
                "particularly critical for LLMs + RAG since top-k RAG is frequently employed at test time to",
                "ensure high recall. Such a scenario necessitates the model to have the ability to discern and",
                "disregard irrelevant content, focusing solely on pertinent information.",
                "5.1 Making Model Robust to top-K RAG",
                "To tackle the challenge of enhancing large language models’ (LLMs) ability to sift through",
                "irrelevant text within the retrieval pipeline, our analysis revealed that training solely with",
                "golden (highly relevant) documents can inadvertently diminish the model’s ability to dis-",
                "cern and disregard irrelevant information. To address this, our algorithm, RAFT , adopts",
                "a strategy that integrates golden documents with a mix of irrelevant ones. This method-",
                "ology prompts us to investigate the ideal fraction of distractor (irrelevant) documents to",
                "incorporate throughout the training process and to assess how well this training approach",
                "adapts to different volumes of documents encountered by the Retrieval-Augmented Gen-",
                "eration (RAG) during the test phase. Our aim is to refine the balance between relevant",
                "and irrelevant information to strenghten the model’s efficiency in identifying and utilizing",
                "pertinent content. Notice that Sec 4.4 looked what what P% of training data should include",
                "distractors, while in this section, we study test-time scenarios.",
                "Training with Distractor Documents To enhance the robustness of LLMs against irrelevant",
                "text in retrieved documents, we adopted a finetuning approach that incorporates both",
                "golden (highly relevant) documents and distractor (irrelevant) documents. The model was",
                "trained with varying numbers of distractor documents, but consistently evaluated using",
                "the top-3 documents obtained from the retriever - not to be confused with p. Our findings,",
                "detailed in Fig. 6, reveal that finetuning with only the golden document frequently results in",
                "inferior performance compared to configurations that include a greater number of distractor"
            ],
            "question": " 1. What is the primary challenge addressed in the paper?",
            "correct_answer": " 1. The primary challenge addressed in the paper is enhancing large language models' (LLMs) ability to sift through irrelevant text within the retrieval pipeline.",
            "incorrect_answers": [
                "2) What are top-k RAGS? ",
                "3) Do you need to train on only highly relevant docs?",
                "4) How do we address this issue at test time?"
            ]
        },
        {
            "id": "chunk_16_question_1",
            "context": [
                "particularly critical for LLMs + RAG since top-k RAG is frequently employed at test time to",
                "ensure high recall. Such a scenario necessitates the model to have the ability to discern and",
                "disregard irrelevant content, focusing solely on pertinent information.",
                "5.1 Making Model Robust to top-K RAG",
                "To tackle the challenge of enhancing large language models’ (LLMs) ability to sift through",
                "irrelevant text within the retrieval pipeline, our analysis revealed that training solely with",
                "golden (highly relevant) documents can inadvertently diminish the model’s ability to dis-",
                "cern and disregard irrelevant information. To address this, our algorithm, RAFT , adopts",
                "a strategy that integrates golden documents with a mix of irrelevant ones. This method-",
                "ology prompts us to investigate the ideal fraction of distractor (irrelevant) documents to",
                "incorporate throughout the training process and to assess how well this training approach",
                "adapts to different volumes of documents encountered by the Retrieval-Augmented Gen-",
                "eration (RAG) during the test phase. Our aim is to refine the balance between relevant",
                "and irrelevant information to strenghten the model’s efficiency in identifying and utilizing",
                "pertinent content. Notice that Sec 4.4 looked what what P% of training data should include",
                "distractors, while in this section, we study test-time scenarios.",
                "Training with Distractor Documents To enhance the robustness of LLMs against irrelevant",
                "text in retrieved documents, we adopted a finetuning approach that incorporates both",
                "golden (highly relevant) documents and distractor (irrelevant) documents. The model was",
                "trained with varying numbers of distractor documents, but consistently evaluated using",
                "the top-3 documents obtained from the retriever - not to be confused with p. Our findings,",
                "detailed in Fig. 6, reveal that finetuning with only the golden document frequently results in",
                "inferior performance compared to configurations that include a greater number of distractor"
            ],
            "question": "What methodology does RAFT adopt to enhance the model's ability to discern relevant information?",
            "correct_answer": "By combining golden documents with a mix of irrelevant ones, RAFT enhances the model's ability to discern relevant information and strengthens its efficiency in identifying and utilizing pertinent content.",
            "incorrect_answers": []
        },
        {
            "id": "chunk_16_question_2",
            "context": [
                "particularly critical for LLMs + RAG since top-k RAG is frequently employed at test time to",
                "ensure high recall. Such a scenario necessitates the model to have the ability to discern and",
                "disregard irrelevant content, focusing solely on pertinent information.",
                "5.1 Making Model Robust to top-K RAG",
                "To tackle the challenge of enhancing large language models’ (LLMs) ability to sift through",
                "irrelevant text within the retrieval pipeline, our analysis revealed that training solely with",
                "golden (highly relevant) documents can inadvertently diminish the model’s ability to dis-",
                "cern and disregard irrelevant information. To address this, our algorithm, RAFT , adopts",
                "a strategy that integrates golden documents with a mix of irrelevant ones. This method-",
                "ology prompts us to investigate the ideal fraction of distractor (irrelevant) documents to",
                "incorporate throughout the training process and to assess how well this training approach",
                "adapts to different volumes of documents encountered by the Retrieval-Augmented Gen-",
                "eration (RAG) during the test phase. Our aim is to refine the balance between relevant",
                "and irrelevant information to strenghten the model’s efficiency in identifying and utilizing",
                "pertinent content. Notice that Sec 4.4 looked what what P% of training data should include",
                "distractors, while in this section, we study test-time scenarios.",
                "Training with Distractor Documents To enhance the robustness of LLMs against irrelevant",
                "text in retrieved documents, we adopted a finetuning approach that incorporates both",
                "golden (highly relevant) documents and distractor (irrelevant) documents. The model was",
                "trained with varying numbers of distractor documents, but consistently evaluated using",
                "the top-3 documents obtained from the retriever - not to be confused with p. Our findings,",
                "detailed in Fig. 6, reveal that finetuning with only the golden document frequently results in",
                "inferior performance compared to configurations that include a greater number of distractor"
            ],
            "question": "What aspect of the training approach is being investigated to strengthen the model's efficiency?",
            "correct_answer": "The aspect being investigated is the balance between golden (relevant) and distractor (irrelevant) documents in the training approach, aiming to enhance the model's ability to discern and utilize pertinent information effectively.",
            "incorrect_answers": [
                "2)   Evaluating the effect on accuracy of training only with highly relevant documents (no irrelevant examples), for LLMs which frequently rely on a top-k recall approach like top-3 RAG.",
                "3)   Assessing how the balance of relevant vs irrelevant data during training impacts efficiency of identifying pertinent content."
            ]
        },
        {
            "id": "chunk_17_question_0",
            "context": [
                "inferior performance compared to configurations that include a greater number of distractor",
                "documents. As we can see in the figure, the better performance for Natural Questions is",
                "8"
            ],
            "question": " (a) Which metric did you use?",
            "correct_answer": " (a) The metric used to evaluate the performance of Natural Questions is the number of distractor documents.",
            "incorrect_answers": []
        },
        {
            "id": "chunk_17_question_1",
            "context": [
                "inferior performance compared to configurations that include a greater number of distractor",
                "documents. As we can see in the figure, the better performance for Natural Questions is",
                "8"
            ],
            "question": "(b) What is the x-axis and what does it represent?",
            "correct_answer": " (b) The x-axis represents the number of distractor documents used in the configuration.",
            "incorrect_answers": []
        },
        {
            "id": "chunk_17_question_2",
            "context": [
                "inferior performance compared to configurations that include a greater number of distractor",
                "documents. As we can see in the figure, the better performance for Natural Questions is",
                "8"
            ],
            "question": "(c) Are the bars showing average values?",
            "correct_answer": "NO",
            "incorrect_answers": []
        },
        {
            "id": "chunk_19_question_0",
            "context": [
                "6 Related Works",
                "Retrieval-Augmented Language Models Retrieval-Augmented Language Models (RALMs)",
                "enhance LLMs by integrating a retrieval module that sources relevant information from",
                "external knowledge bases, significantly improving performance across various NLP tasks,",
                "including language modeling (Guu et al., 2020; Borgeaud et al., 2022; Khandelwal et al.,",
                "2019; Shi et al., 2023d; Lin et al., 2023b; Shi et al., 2023c; Asai et al., 2023; Xu et al., 2023;",
                "Wang et al., 2023) and open-domain question answering (Izacard et al., 2023; Lewis et al.,",
                "2020). For instance, Atlas (Izacard et al., 2023) fine-tunes T5 models with the retriever,",
                "treating documents as latent variables, while RETRO (Borgeaud et al., 2022) modifies the",
                "decoder-only architecture to include retrieved texts and conducts pre-training from scratch.",
                "kNN-LM (Khandelwal et al., 2019) interpolates between the LM’s next token distribution",
                "and distributions computed from retrieved tokens at inference. (Shi et al., 2023d; Ram",
                "et al., 2023) assume black-box access to an LLM, combining it with either off-the-shelf or",
                "fine-tuned retriever.",
                "Memorization A key question around large neural language models is whether they truly",
                "“understand” text (Feldman, 2020; Power et al., 2022) or simply rely on surface pattern",
                "memorization (Carlini et al., 2019; Tänzer et al., 2022). (Feldman, 2020; Carlini et al., 2019;",
                "2022) develop methodologies to quantify the extent of memorization in neural models.",
                "(Brown et al., 2020; Power et al., 2022; Liu et al., 2022) further explored how memorization",
                "impacts the models’ generalization capabilities. (Carlini et al., 2021; Shi et al., 2023b)",
                "demonstrated the ability of language models to memorize and regurgitate training data,",
                "raising significant privacy concerns (Kandpal et al., 2022; Pan et al., 2020).",
                "Finetuning for RAG More recently, several papers have been exploring the idea of fine-",
                "tuning a pretrained LLM to be better at RAG tasks (Lin et al., 2023a; Wang et al., 2023; Xu",
                "9"
            ],
            "question": " 1. What are Retrieval-Augmented Language Models?",
            "correct_answer": " A Retrieval-Augmented Language Model (RALM) is a type of language model that integrates a retrieval module to source relevant information from external knowledge bases. This integration significantly enhances the performance of RALMs across various NLP tasks, including language modeling and open-domain question answering. By incorporating retrieved texts into the training process, RALMs demonstrate improved ability to generate contextually appropriate responses and answer questions accurately.",
            "incorrect_answers": []
        },
        {
            "id": "chunk_19_question_1",
            "context": [
                "6 Related Works",
                "Retrieval-Augmented Language Models Retrieval-Augmented Language Models (RALMs)",
                "enhance LLMs by integrating a retrieval module that sources relevant information from",
                "external knowledge bases, significantly improving performance across various NLP tasks,",
                "including language modeling (Guu et al., 2020; Borgeaud et al., 2022; Khandelwal et al.,",
                "2019; Shi et al., 2023d; Lin et al., 2023b; Shi et al., 2023c; Asai et al., 2023; Xu et al., 2023;",
                "Wang et al., 2023) and open-domain question answering (Izacard et al., 2023; Lewis et al.,",
                "2020). For instance, Atlas (Izacard et al., 2023) fine-tunes T5 models with the retriever,",
                "treating documents as latent variables, while RETRO (Borgeaud et al., 2022) modifies the",
                "decoder-only architecture to include retrieved texts and conducts pre-training from scratch.",
                "kNN-LM (Khandelwal et al., 2019) interpolates between the LM’s next token distribution",
                "and distributions computed from retrieved tokens at inference. (Shi et al., 2023d; Ram",
                "et al., 2023) assume black-box access to an LLM, combining it with either off-the-shelf or",
                "fine-tuned retriever.",
                "Memorization A key question around large neural language models is whether they truly",
                "“understand” text (Feldman, 2020; Power et al., 2022) or simply rely on surface pattern",
                "memorization (Carlini et al., 2019; Tänzer et al., 2022). (Feldman, 2020; Carlini et al., 2019;",
                "2022) develop methodologies to quantify the extent of memorization in neural models.",
                "(Brown et al., 2020; Power et al., 2022; Liu et al., 2022) further explored how memorization",
                "impacts the models’ generalization capabilities. (Carlini et al., 2021; Shi et al., 2023b)",
                "demonstrated the ability of language models to memorize and regurgitate training data,",
                "raising significant privacy concerns (Kandpal et al., 2022; Pan et al., 2020).",
                "Finetuning for RAG More recently, several papers have been exploring the idea of fine-",
                "tuning a pretrained LLM to be better at RAG tasks (Lin et al., 2023a; Wang et al., 2023; Xu",
                "9"
            ],
            "question": "How do Retrieval-Augmented Language Models enhance the performance of LLMs?",
            "correct_answer": " A retrieval module is integrated into Retrieval-Augmented Language Models (RALMs) to source relevant information from external knowledge bases. This significantly improves performance across various NLP tasks including language modeling and open-domain question answering.",
            "incorrect_answers": []
        },
        {
            "id": "chunk_19_question_2",
            "context": [
                "6 Related Works",
                "Retrieval-Augmented Language Models Retrieval-Augmented Language Models (RALMs)",
                "enhance LLMs by integrating a retrieval module that sources relevant information from",
                "external knowledge bases, significantly improving performance across various NLP tasks,",
                "including language modeling (Guu et al., 2020; Borgeaud et al., 2022; Khandelwal et al.,",
                "2019; Shi et al., 2023d; Lin et al., 2023b; Shi et al., 2023c; Asai et al., 2023; Xu et al., 2023;",
                "Wang et al., 2023) and open-domain question answering (Izacard et al., 2023; Lewis et al.,",
                "2020). For instance, Atlas (Izacard et al., 2023) fine-tunes T5 models with the retriever,",
                "treating documents as latent variables, while RETRO (Borgeaud et al., 2022) modifies the",
                "decoder-only architecture to include retrieved texts and conducts pre-training from scratch.",
                "kNN-LM (Khandelwal et al., 2019) interpolates between the LM’s next token distribution",
                "and distributions computed from retrieved tokens at inference. (Shi et al., 2023d; Ram",
                "et al., 2023) assume black-box access to an LLM, combining it with either off-the-shelf or",
                "fine-tuned retriever.",
                "Memorization A key question around large neural language models is whether they truly",
                "“understand” text (Feldman, 2020; Power et al., 2022) or simply rely on surface pattern",
                "memorization (Carlini et al., 2019; Tänzer et al., 2022). (Feldman, 2020; Carlini et al., 2019;",
                "2022) develop methodologies to quantify the extent of memorization in neural models.",
                "(Brown et al., 2020; Power et al., 2022; Liu et al., 2022) further explored how memorization",
                "impacts the models’ generalization capabilities. (Carlini et al., 2021; Shi et al., 2023b)",
                "demonstrated the ability of language models to memorize and regurgitate training data,",
                "raising significant privacy concerns (Kandpal et al., 2022; Pan et al., 2020).",
                "Finetuning for RAG More recently, several papers have been exploring the idea of fine-",
                "tuning a pretrained LLM to be better at RAG tasks (Lin et al., 2023a; Wang et al., 2023; Xu",
                "9"
            ],
            "question": "What is memorization in the context of large neural language models?",
            "correct_answer": " A key question around large neural language models is whether they truly “understand” text or simply rely on surface pattern memorization.",
            "incorrect_answers": []
        },
        {
            "id": "chunk_20_question_0",
            "context": [
                "Preprint, Under Review",
                "et al., 2023; Liu et al., 2024). These works focus on constructing a combination of finetuning",
                "dataset for RAG and train a model to perform well on these tasks. In particular, in their",
                "settings, at test time, the domain or documents can be different than the training time;",
                "whereas our paper studies a slightly opposite scenario where we only care about testing the",
                "LLM on the same set of documents.",
                "7 Conclusion",
                "RAFT is a training strategy designed to enhance the model’s performance in answering",
                "questions within a specific domain, in \"open-book\" settings. We highlight several crucial",
                "design decisions, such as training the model alongside distractor documents, organizing the",
                "dataset so a portion lacks golden documents in their context, and formulating answers in a",
                "chain-of-thought manner with direct quotations from the relevant text. Our evaluations on",
                "PubMed, HotpotQA, and Gorilla API Bench underline RAFT’s significant potential."
            ],
            "question": " 1. What is RAFT?",
            "correct_answer": " 1. RAFT is a training strategy designed to enhance the model’s performance in answering questions within a specific domain, in \"open-book\" settings.",
            "incorrect_answers": []
        },
        {
            "id": "chunk_20_question_1",
            "context": [
                "Preprint, Under Review",
                "et al., 2023; Liu et al., 2024). These works focus on constructing a combination of finetuning",
                "dataset for RAG and train a model to perform well on these tasks. In particular, in their",
                "settings, at test time, the domain or documents can be different than the training time;",
                "whereas our paper studies a slightly opposite scenario where we only care about testing the",
                "LLM on the same set of documents.",
                "7 Conclusion",
                "RAFT is a training strategy designed to enhance the model’s performance in answering",
                "questions within a specific domain, in \"open-book\" settings. We highlight several crucial",
                "design decisions, such as training the model alongside distractor documents, organizing the",
                "dataset so a portion lacks golden documents in their context, and formulating answers in a",
                "chain-of-thought manner with direct quotations from the relevant text. Our evaluations on",
                "PubMed, HotpotQA, and Gorilla API Bench underline RAFT’s significant potential."
            ],
            "question": "What is the purpose of RAFT?",
            "correct_answer": "RAFT is a training strategy designed to enhance the model's performance in answering questions within a specific domain, in 'open-book' settings.",
            "incorrect_answers": []
        },
        {
            "id": "chunk_20_question_2",
            "context": [
                "Preprint, Under Review",
                "et al., 2023; Liu et al., 2024). These works focus on constructing a combination of finetuning",
                "dataset for RAG and train a model to perform well on these tasks. In particular, in their",
                "settings, at test time, the domain or documents can be different than the training time;",
                "whereas our paper studies a slightly opposite scenario where we only care about testing the",
                "LLM on the same set of documents.",
                "7 Conclusion",
                "RAFT is a training strategy designed to enhance the model’s performance in answering",
                "questions within a specific domain, in \"open-book\" settings. We highlight several crucial",
                "design decisions, such as training the model alongside distractor documents, organizing the",
                "dataset so a portion lacks golden documents in their context, and formulating answers in a",
                "chain-of-thought manner with direct quotations from the relevant text. Our evaluations on",
                "PubMed, HotpotQA, and Gorilla API Bench underline RAFT’s significant potential."
            ],
            "question": "What are some examples of design decisions when training a model to use RAFT?",
            "correct_answer": "Training alongside distractor documents, organizing the dataset to lack golden documents, and formulating answers in a chain-of-thought manner with direct quotations.",
            "incorrect_answers": []
        }
    ],
    "data/pdf/RAG_papers/Blended_RAG.pdf": [
        {
            "id": "chunk_0_question_0",
            "context": [
                "Blended RAG: Improving RAG",
                "(Retriever-Augmented Generation) Accuracy with",
                "Semantic Search and Hybrid Query-Based",
                "Retrievers",
                "1stKunal Sawarkar",
                "IBM",
                "Kunal@ibm.com2ndAbhilasha Mangal",
                "IBM",
                "Abhilasha.Mangal@ibm.com3rdShivam Raj Solanki",
                "IBM",
                "Shivam.Raj.Solanki@ibm.com",
                "Abstract —Retrieval-Augmented Generation (RAG) is a preva-",
                "lent approach to infuse a private knowledge base of documents",
                "with Large Language Models (LLM) to build Generative Q&A",
                "(Question-Answering) systems. However, RAG accuracy becomes",
                "increasingly challenging as the corpus of documents scales up,",
                "with Retrievers playing an outsized role in the overall RAG",
                "accuracy by extracting the most relevant document from the",
                "corpus to provide context to the LLM. In this paper, we propose",
                "the ’Blended RAG’ method of leveraging semantic search tech-",
                "niques, such as Dense Vector indexes and Sparse Encoder indexes,",
                "blended with hybrid query strategies. Our study achieves better",
                "retrieval results and sets new benchmarks for IR (Information",
                "Retrieval) datasets like NQ and TREC-COVID datasets. We",
                "further extend such a ’Blended Retriever’ to the RAG system",
                "to demonstrate far superior results on Generative Q&A datasets",
                "like SQUAD, even surpassing fine-tuning performance.",
                "Index Terms —RAG, Retrievers, Semantic Search, Dense Index,",
                "Vector Search",
                "I. I NTRODUCTION",
                "RAG represents an approach to text generation that is",
                "based not only on patterns learned during training but also",
                "on dynamically retrieved external knowledge. This method",
                "combines the creative flair of generative models with the",
                "encyclopedic recall of a search engine. The efficacy of the",
                "RAG system relies fundamentally on two components: the",
                "Retriever (R) and the Generator (G), the latter representing",
                "the size and type of LLM.",
                "The language model can easily craft sentences, but it might",
                "not always have all the facts. This is where the Retriever (R)",
                "steps in, quickly sifting through vast amounts of documents",
                "to find relevant information that can be used to inform and"
            ],
            "question": " 1. What is Blended RAG?",
            "correct_answer": "$Blended RAG is a method that combines semantic search techniques and hybrid query strategies to improve Retrieval-Augmented Generation (RAG) accuracy in Generative Q&A systems.",
            "incorrect_answers": [
                "Blending semantic search with hybrid retrievers to improve the accuracy of RAG.",
                "Semantic search and hybrid query-based retrievers are combined to create a blended RAG model that outperforms fine-tuning on SQUAD dataset.",
                "The technique involves combining sparse encoders and dense vector indexes into an index system, which is then used for information retrieval purposes in various datasets including NQ and TRE-COVID.",
                "The accuracy of Retrievers, particularly with larger documents corpus, is critical to overall RAG accuracy."
            ]
        },
        {
            "id": "chunk_0_question_1",
            "context": [
                "Blended RAG: Improving RAG",
                "(Retriever-Augmented Generation) Accuracy with",
                "Semantic Search and Hybrid Query-Based",
                "Retrievers",
                "1stKunal Sawarkar",
                "IBM",
                "Kunal@ibm.com2ndAbhilasha Mangal",
                "IBM",
                "Abhilasha.Mangal@ibm.com3rdShivam Raj Solanki",
                "IBM",
                "Shivam.Raj.Solanki@ibm.com",
                "Abstract —Retrieval-Augmented Generation (RAG) is a preva-",
                "lent approach to infuse a private knowledge base of documents",
                "with Large Language Models (LLM) to build Generative Q&A",
                "(Question-Answering) systems. However, RAG accuracy becomes",
                "increasingly challenging as the corpus of documents scales up,",
                "with Retrievers playing an outsized role in the overall RAG",
                "accuracy by extracting the most relevant document from the",
                "corpus to provide context to the LLM. In this paper, we propose",
                "the ’Blended RAG’ method of leveraging semantic search tech-",
                "niques, such as Dense Vector indexes and Sparse Encoder indexes,",
                "blended with hybrid query strategies. Our study achieves better",
                "retrieval results and sets new benchmarks for IR (Information",
                "Retrieval) datasets like NQ and TREC-COVID datasets. We",
                "further extend such a ’Blended Retriever’ to the RAG system",
                "to demonstrate far superior results on Generative Q&A datasets",
                "like SQUAD, even surpassing fine-tuning performance.",
                "Index Terms —RAG, Retrievers, Semantic Search, Dense Index,",
                "Vector Search",
                "I. I NTRODUCTION",
                "RAG represents an approach to text generation that is",
                "based not only on patterns learned during training but also",
                "on dynamically retrieved external knowledge. This method",
                "combines the creative flair of generative models with the",
                "encyclopedic recall of a search engine. The efficacy of the",
                "RAG system relies fundamentally on two components: the",
                "Retriever (R) and the Generator (G), the latter representing",
                "the size and type of LLM.",
                "The language model can easily craft sentences, but it might",
                "not always have all the facts. This is where the Retriever (R)",
                "steps in, quickly sifting through vast amounts of documents",
                "to find relevant information that can be used to inform and"
            ],
            "question": "How does Retrieval-Augmented Generation work?",
            "correct_answer": "Retrieval-Augmented Generation (RAG) works by combining a Large Language Model (LLM) with a private knowledge base of documents to build Generative Q&A systems. The approach relies on two key components: the Retriever, which extracts relevant documents from the corpus, and the Generator, which is the LLM itself. RAG accuracy depends heavily on the performance of the Retriever in providing contextually relevant information to the LLM.",
            "incorrect_answers": [
                "This approach blends semantic search techniques, like dense vector indexes and sparse encoder indexes, along with hybrid query strategies.",
                "Dense vector indexing refers to the use of vector-based representation for word meaning using neural networks to enable efficient retrieval in RAG systems.",
                "The “Blended Retriever” extends this to the RAG system by demonstrating superior results on generative Q&A datasets like SQUAD, surpassing even fine-tuning performance benchmarks on information retrieval datasets like NQ and TREC-COVID datasets."
            ]
        },
        {
            "id": "chunk_0_question_2",
            "context": [
                "Blended RAG: Improving RAG",
                "(Retriever-Augmented Generation) Accuracy with",
                "Semantic Search and Hybrid Query-Based",
                "Retrievers",
                "1stKunal Sawarkar",
                "IBM",
                "Kunal@ibm.com2ndAbhilasha Mangal",
                "IBM",
                "Abhilasha.Mangal@ibm.com3rdShivam Raj Solanki",
                "IBM",
                "Shivam.Raj.Solanki@ibm.com",
                "Abstract —Retrieval-Augmented Generation (RAG) is a preva-",
                "lent approach to infuse a private knowledge base of documents",
                "with Large Language Models (LLM) to build Generative Q&A",
                "(Question-Answering) systems. However, RAG accuracy becomes",
                "increasingly challenging as the corpus of documents scales up,",
                "with Retrievers playing an outsized role in the overall RAG",
                "accuracy by extracting the most relevant document from the",
                "corpus to provide context to the LLM. In this paper, we propose",
                "the ’Blended RAG’ method of leveraging semantic search tech-",
                "niques, such as Dense Vector indexes and Sparse Encoder indexes,",
                "blended with hybrid query strategies. Our study achieves better",
                "retrieval results and sets new benchmarks for IR (Information",
                "Retrieval) datasets like NQ and TREC-COVID datasets. We",
                "further extend such a ’Blended Retriever’ to the RAG system",
                "to demonstrate far superior results on Generative Q&A datasets",
                "like SQUAD, even surpassing fine-tuning performance.",
                "Index Terms —RAG, Retrievers, Semantic Search, Dense Index,",
                "Vector Search",
                "I. I NTRODUCTION",
                "RAG represents an approach to text generation that is",
                "based not only on patterns learned during training but also",
                "on dynamically retrieved external knowledge. This method",
                "combines the creative flair of generative models with the",
                "encyclopedic recall of a search engine. The efficacy of the",
                "RAG system relies fundamentally on two components: the",
                "Retriever (R) and the Generator (G), the latter representing",
                "the size and type of LLM.",
                "The language model can easily craft sentences, but it might",
                "not always have all the facts. This is where the Retriever (R)",
                "steps in, quickly sifting through vast amounts of documents",
                "to find relevant information that can be used to inform and"
            ],
            "question": "Why do semantic search techniques improve RAG accuracy?",
            "correct_answer": "Semantic search techniques improve RAG accuracy by enhancing retrieval results and providing more relevant context to the LLM, which ultimately leads to better overall RAG performance.",
            "incorrect_answers": [
                "Semantic search improves retrieval results and sets new benchmarks for IR datasets like NQ and TREC-COVID datasets.",
                "By blending different query strategies with Dense Vector and Sparse Encoder indexes, the model can achieve superior performance on Generative Q&A datasets like SQUAD.",
                "The blended retriever approach in semantic search enables more effective information extraction to improve overall RAG accuracy."
            ]
        },
        {
            "id": "chunk_1_question_0",
            "context": [
                "to find relevant information that can be used to inform and",
                "enrich the language model's output. Think of the retriever",
                "as a researcher part of the AI, which feeds the contextually",
                "grounded text to generate knowledgeable answers to Generator",
                "(G). Without the retriever, RAG would be like a well-spoken",
                "individual who delivers irrelevant information.",
                "II. R ELATED WORK",
                "Search has been a focal point of research in information",
                "retrieval, with numerous studies exploring various methodolo-",
                "gies. Historically, the BM25 (Best Match) algorithm, whichuses similarity search, has been a cornerstone in this field,",
                "as explored by Robertson and Zaragoza (2009). [1]. BM25",
                "prioritizes documents according to their pertinence to a query,",
                "capitalizing on Term Frequency (TF), Inverse Document Fre-",
                "quency (IDF), and Document Length to compute a relevance",
                "score.",
                "Dense vector models, particularly those employing KNN",
                "(k Nearest Neighbours) algorithms, have gained attention for",
                "their ability to capture deep semantic relationships in data.",
                "Studies by Johnson et al. (2019) demonstrated the efficacy of",
                "dense vector representations in large-scale search applications.",
                "The kinship between data entities (including the search query)",
                "is assessed by computing the vectorial proximity (via cosine",
                "similarity etc.). During search execution, the model discerns",
                "the ’k’ vectors closest in resemblance to the query vector,",
                "hence returning the corresponding data entities as results.",
                "Their ability to transform text into vector space models,",
                "where semantic similarities can be quantitatively assessed,",
                "marks a significant advancement over traditional keyword-",
                "based approaches. [2]",
                "On the other hand, sparse encoder based vector models have",
                "also been explored for their precision in representing document",
                "semantics. The work of Zaharia et al. (2010) illustrates the po-",
                "tential of these models in efficiently handling high-dimensional",
                "data while maintaining interpretability, a challenge often faced"
            ],
            "question": " 1. What is a retriever's function in RAG?",
            "correct_answer": "The retriever's function in RAG is to find relevant information that can be used to inform and enrich the language model's output.",
            "incorrect_answers": []
        },
        {
            "id": "chunk_1_question_1",
            "context": [
                "to find relevant information that can be used to inform and",
                "enrich the language model's output. Think of the retriever",
                "as a researcher part of the AI, which feeds the contextually",
                "grounded text to generate knowledgeable answers to Generator",
                "(G). Without the retriever, RAG would be like a well-spoken",
                "individual who delivers irrelevant information.",
                "II. R ELATED WORK",
                "Search has been a focal point of research in information",
                "retrieval, with numerous studies exploring various methodolo-",
                "gies. Historically, the BM25 (Best Match) algorithm, whichuses similarity search, has been a cornerstone in this field,",
                "as explored by Robertson and Zaragoza (2009). [1]. BM25",
                "prioritizes documents according to their pertinence to a query,",
                "capitalizing on Term Frequency (TF), Inverse Document Fre-",
                "quency (IDF), and Document Length to compute a relevance",
                "score.",
                "Dense vector models, particularly those employing KNN",
                "(k Nearest Neighbours) algorithms, have gained attention for",
                "their ability to capture deep semantic relationships in data.",
                "Studies by Johnson et al. (2019) demonstrated the efficacy of",
                "dense vector representations in large-scale search applications.",
                "The kinship between data entities (including the search query)",
                "is assessed by computing the vectorial proximity (via cosine",
                "similarity etc.). During search execution, the model discerns",
                "the ’k’ vectors closest in resemblance to the query vector,",
                "hence returning the corresponding data entities as results.",
                "Their ability to transform text into vector space models,",
                "where semantic similarities can be quantitatively assessed,",
                "marks a significant advancement over traditional keyword-",
                "based approaches. [2]",
                "On the other hand, sparse encoder based vector models have",
                "also been explored for their precision in representing document",
                "semantics. The work of Zaharia et al. (2010) illustrates the po-",
                "tential of these models in efficiently handling high-dimensional",
                "data while maintaining interpretability, a challenge often faced"
            ],
            "question": "How does BM25 algorithm work?",
            "correct_answer": "The BM25 algorithm works by using Term Frequency (TF), Inverse Document Frequency (IDF), and Document Length to calculate a relevance score for each document in response to a query, allowing the most relevant documents to be prioritized.",
            "incorrect_answers": [
                "Robertson and Zaragoza (2009) did not write about BM25",
                "3.TF stands for Task Force, IDF stands for Inverse Document Frequency, Document Length is used to compute a relevancy score"
            ]
        },
        {
            "id": "chunk_1_question_2",
            "context": [
                "to find relevant information that can be used to inform and",
                "enrich the language model's output. Think of the retriever",
                "as a researcher part of the AI, which feeds the contextually",
                "grounded text to generate knowledgeable answers to Generator",
                "(G). Without the retriever, RAG would be like a well-spoken",
                "individual who delivers irrelevant information.",
                "II. R ELATED WORK",
                "Search has been a focal point of research in information",
                "retrieval, with numerous studies exploring various methodolo-",
                "gies. Historically, the BM25 (Best Match) algorithm, whichuses similarity search, has been a cornerstone in this field,",
                "as explored by Robertson and Zaragoza (2009). [1]. BM25",
                "prioritizes documents according to their pertinence to a query,",
                "capitalizing on Term Frequency (TF), Inverse Document Fre-",
                "quency (IDF), and Document Length to compute a relevance",
                "score.",
                "Dense vector models, particularly those employing KNN",
                "(k Nearest Neighbours) algorithms, have gained attention for",
                "their ability to capture deep semantic relationships in data.",
                "Studies by Johnson et al. (2019) demonstrated the efficacy of",
                "dense vector representations in large-scale search applications.",
                "The kinship between data entities (including the search query)",
                "is assessed by computing the vectorial proximity (via cosine",
                "similarity etc.). During search execution, the model discerns",
                "the ’k’ vectors closest in resemblance to the query vector,",
                "hence returning the corresponding data entities as results.",
                "Their ability to transform text into vector space models,",
                "where semantic similarities can be quantitatively assessed,",
                "marks a significant advancement over traditional keyword-",
                "based approaches. [2]",
                "On the other hand, sparse encoder based vector models have",
                "also been explored for their precision in representing document",
                "semantics. The work of Zaharia et al. (2010) illustrates the po-",
                "tential of these models in efficiently handling high-dimensional",
                "data while maintaining interpretability, a challenge often faced"
            ],
            "question": "What are dense vector models used for?",
            "correct_answer": "Dense vector models are used to capture deep semantic relationships in data, particularly in large-scale search applications, by transforming text into vector space models where similarities can be quantitatively assessed.",
            "incorrect_answers": [
                "- used in language models to enhance their knowledge and understanding of different topics.",
                "4- Dense vector models are used for representing data entities as multi-dimensional vectors in a continuous vector space, enabling semantic analysis and comparisons."
            ]
        },
        {
            "id": "chunk_2_question_0",
            "context": [
                "data while maintaining interpretability, a challenge often faced",
                "in dense vector representations. In Sparse Encoder indexes the",
                "indexed documents, and the user’s search query maps into an",
                "extensive array of associated terms derived from a vast corpus",
                "of training data to encapsulate relationships and contextual",
                "use of concepts. The resultant expanded terms for documents",
                "and queries are encoded into sparse vectors, an efficient data",
                "representation format when handling an extensive vocabulary.",
                "A. Limitations in the current RAG system",
                "Most current retrieval methodologies employed in Retrieval-",
                "Augmented Generation (RAG) pipelines rely on keyword and",
                "similarity-based searches, which can restrict the RAG system’s",
                "overall accuracy. Table 1 provides a summary of the current",
                "benchmarks for retriever accuracy.arXiv:2404.07220v1  [cs.IR]  22 Mar 2024"
            ],
            "question": "What are some limitations in the current RAG systems?",
            "correct_answer": "Limitations in the current RAG system include its reliance on keyword and similarity-based searches which can restrict its overall accuracy.",
            "incorrect_answers": []
        },
        {
            "id": "chunk_3_question_0",
            "context": [
                "TABLE I: Current Retriever Benchmarks",
                "Dataset Benchmark Metrics NDCG@10 p@20 F1",
                "NQDataset P@20 0.633 86 79.6",
                "Trec Covid NDCG@10 80.4",
                "HotpotQA F1 , EM 0.85",
                "While most of prior efforts in improving RAG accuracy",
                "is on G part, by tweaking LLM prompts, tuning etc.,[9]",
                "they have limited impact on the overall accuracy of the",
                "RAG system, since if R part is feeding irreverent context",
                "then answer would be inaccurate. Furthermore, most retrieval",
                "methodologies employed in RAG pipelines rely on keyword",
                "and similarity-based searches, which can restrict the system's",
                "overall accuracy.",
                "Finding the best search method for RAG is still an emerging",
                "area of research. The goal of this study is to enhance retriever",
                "and RAG accuracy by incorporating Semantic Search-Based",
                "Retrievers and Hybrid Search Queries.",
                "III. B LENDED RETRIEVERS",
                "For RAG systems, we explored three distinct search strate-",
                "gies: keyword-based similarity search, dense vector-based, and",
                "semantic-based sparse encoders, integrating these to formu-",
                "late hybrid queries. Unlike conventional keyword matching,",
                "semantic search delves into the nuances of a user’s query, deci-",
                "phering context and intent. This study systematically evaluates",
                "an array of search techniques across three primary indices:",
                "BM25 [3] for keyword-based, KNN [4] for vector-based, and",
                "Elastic Learned Sparse Encoder (ELSER) for sparse encoder-",
                "based semantic search.",
                "1) BM25 Index: The BM25 index is adept at employing",
                "full-text search capabilities enhanced by fuzzy matching",
                "techniques, laying the groundwork for more sophisti-",
                "cated query operations.",
                "2) Dense Vector Index: We construct a dense vector index",
                "empowered by sentence transformers. It identifies the",
                "proximity of vector representations derived from docu-",
                "ment and query content.",
                "3) Sparse Encoder Index: The Sparse EncodeR Retriever",
                "Model index is an amalgam of semantic understanding",
                "and similarity-based retrieval to encapsulate the nuanced",
                "relationships between terms, thereby capturing a more"
            ],
            "question": " 1. What are blended retrievers in the context of RAG systems?",
            "correct_answer": " 1) BM25 Index: The BM25 index is adept at employing full-text search capabilities enhanced by fuzzy matching techniques, laying the groundwork for more sophisticated query operations. \n2) Dense Vector Index: We construct a dense vector index empowered by sentence transformers. It identifies the proximity of vector representations derived from document and query content.\n3) Sparse Encoder Index: The Sparse EncodeR Retriever Model index is an amalgam of semantic understanding and similarity-based retrieval to encapsulate the nuanced relationships between terms, thereby capturing a more",
            "incorrect_answers": [
                "Blended retrievers are a combination of two or more dog breeds.",
                "Blended retrievers are a cross between Golden Retrievers and Labrador Retrievers.",
                "Blended retrievers are a fictional creature created by blending different animal species."
            ]
        },
        {
            "id": "chunk_3_question_1",
            "context": [
                "TABLE I: Current Retriever Benchmarks",
                "Dataset Benchmark Metrics NDCG@10 p@20 F1",
                "NQDataset P@20 0.633 86 79.6",
                "Trec Covid NDCG@10 80.4",
                "HotpotQA F1 , EM 0.85",
                "While most of prior efforts in improving RAG accuracy",
                "is on G part, by tweaking LLM prompts, tuning etc.,[9]",
                "they have limited impact on the overall accuracy of the",
                "RAG system, since if R part is feeding irreverent context",
                "then answer would be inaccurate. Furthermore, most retrieval",
                "methodologies employed in RAG pipelines rely on keyword",
                "and similarity-based searches, which can restrict the system's",
                "overall accuracy.",
                "Finding the best search method for RAG is still an emerging",
                "area of research. The goal of this study is to enhance retriever",
                "and RAG accuracy by incorporating Semantic Search-Based",
                "Retrievers and Hybrid Search Queries.",
                "III. B LENDED RETRIEVERS",
                "For RAG systems, we explored three distinct search strate-",
                "gies: keyword-based similarity search, dense vector-based, and",
                "semantic-based sparse encoders, integrating these to formu-",
                "late hybrid queries. Unlike conventional keyword matching,",
                "semantic search delves into the nuances of a user’s query, deci-",
                "phering context and intent. This study systematically evaluates",
                "an array of search techniques across three primary indices:",
                "BM25 [3] for keyword-based, KNN [4] for vector-based, and",
                "Elastic Learned Sparse Encoder (ELSER) for sparse encoder-",
                "based semantic search.",
                "1) BM25 Index: The BM25 index is adept at employing",
                "full-text search capabilities enhanced by fuzzy matching",
                "techniques, laying the groundwork for more sophisti-",
                "cated query operations.",
                "2) Dense Vector Index: We construct a dense vector index",
                "empowered by sentence transformers. It identifies the",
                "proximity of vector representations derived from docu-",
                "ment and query content.",
                "3) Sparse Encoder Index: The Sparse EncodeR Retriever",
                "Model index is an amalgam of semantic understanding",
                "and similarity-based retrieval to encapsulate the nuanced",
                "relationships between terms, thereby capturing a more"
            ],
            "question": "What are the three distinct search strategies explored for RAG systems in this study?",
            "correct_answer": " 1) BM25 Index for keyword-based search.\n2) Dense Vector Index for vector-based search.\n3) Sparse Encoder Index for semantic-based sparse encoders.",
            "incorrect_answers": [
                "TF-IDF with BM25",
                "BERT for passage selection"
            ]
        },
        {
            "id": "chunk_3_question_2",
            "context": [
                "TABLE I: Current Retriever Benchmarks",
                "Dataset Benchmark Metrics NDCG@10 p@20 F1",
                "NQDataset P@20 0.633 86 79.6",
                "Trec Covid NDCG@10 80.4",
                "HotpotQA F1 , EM 0.85",
                "While most of prior efforts in improving RAG accuracy",
                "is on G part, by tweaking LLM prompts, tuning etc.,[9]",
                "they have limited impact on the overall accuracy of the",
                "RAG system, since if R part is feeding irreverent context",
                "then answer would be inaccurate. Furthermore, most retrieval",
                "methodologies employed in RAG pipelines rely on keyword",
                "and similarity-based searches, which can restrict the system's",
                "overall accuracy.",
                "Finding the best search method for RAG is still an emerging",
                "area of research. The goal of this study is to enhance retriever",
                "and RAG accuracy by incorporating Semantic Search-Based",
                "Retrievers and Hybrid Search Queries.",
                "III. B LENDED RETRIEVERS",
                "For RAG systems, we explored three distinct search strate-",
                "gies: keyword-based similarity search, dense vector-based, and",
                "semantic-based sparse encoders, integrating these to formu-",
                "late hybrid queries. Unlike conventional keyword matching,",
                "semantic search delves into the nuances of a user’s query, deci-",
                "phering context and intent. This study systematically evaluates",
                "an array of search techniques across three primary indices:",
                "BM25 [3] for keyword-based, KNN [4] for vector-based, and",
                "Elastic Learned Sparse Encoder (ELSER) for sparse encoder-",
                "based semantic search.",
                "1) BM25 Index: The BM25 index is adept at employing",
                "full-text search capabilities enhanced by fuzzy matching",
                "techniques, laying the groundwork for more sophisti-",
                "cated query operations.",
                "2) Dense Vector Index: We construct a dense vector index",
                "empowered by sentence transformers. It identifies the",
                "proximity of vector representations derived from docu-",
                "ment and query content.",
                "3) Sparse Encoder Index: The Sparse EncodeR Retriever",
                "Model index is an amalgam of semantic understanding",
                "and similarity-based retrieval to encapsulate the nuanced",
                "relationships between terms, thereby capturing a more"
            ],
            "question": "What is BM25 and what type of index does it use?",
            "correct_answer": " 1) BM25 Index: The BM25 index is adept at employing full-text search capabilities enhanced by fuzzy matching techniques, laying the groundwork for more sophisticated query operations.",
            "incorrect_answers": [
                "2) BM25 utilizes a hybrid approach combining both sparse and dense indices.",
                "3) BM25 is a semantic search algorithm that employs a keyword-based similarity search method.",
                "4) BM25 represents the top 25 results retrieved using any type of index."
            ]
        },
        {
            "id": "chunk_4_question_0",
            "context": [
                "relationships between terms, thereby capturing a more",
                "authentic representation of user intent and document",
                "relevance.",
                "A. Methodology",
                "Our methodology unfolds in a sequence of progressive",
                "steps, commencing with the elementary match query within",
                "the BM25 index. We then escalate to hybrid queries that",
                "amalgamate diverse search techniques across multiple fields,",
                "leveraging the multi-match query within the Sparse Encoder-",
                "Based Index. This method proves invaluable when the exact",
                "location of the query text within the document corpus is in-",
                "determinate, hence ensuring a comprehensive match retrieval.",
                "The multi-match queries are categorized as follows:",
                "•Cross Fields: Targets concurrence across multiple fields•Most Fields: Seeks text representation through different",
                "lenses across various fields.",
                "•Best Fields: Pursues the aggregation of words within a",
                "singular field.",
                "•Phrase Prefix: Operates similarly to Best Fields but",
                "prioritizes phrases over keywords.",
                "After initial match queries, we incorporate dense vector (KNN)",
                "and sparse encoder indices, each with their bespoke hybrid",
                "queries. This strategic approach synthesizes the strengths of",
                "each index, channeling them towards the unified goal of refin-",
                "ing retrieval accuracy within our RAG system. We calculate",
                "the top-k retrieval accuracy metric to distill the essence of each",
                "query type.",
                "In Figure 1, we introduce a scheme designed to create",
                "Blended Retrievers by blending semantic search with hybrid",
                "queries.",
                "B. Constructing RAG System",
                "From the plethora of possible permutations, a select sextet",
                "(top 6) of hybrid queries—those exhibiting paramount retrieval",
                "efficacy—were chosen for further scrutiny. These queries were",
                "then subjected to rigorous evaluation across the benchmark",
                "datasets to ascertain the precision of the retrieval component",
                "within RAG. The sextet queries represent the culmination",
                "of retriever experimentation, embodying the synthesis of our",
                "finest query strategies aligned with various index types. The"
            ],
            "question": " 1. What is a supercomputer?",
            "correct_answer": "A supercomputer is a high-performance computer that is designed to perform very large and complex calculations.",
            "incorrect_answers": []
        },
        {
            "id": "chunk_4_question_1",
            "context": [
                "relationships between terms, thereby capturing a more",
                "authentic representation of user intent and document",
                "relevance.",
                "A. Methodology",
                "Our methodology unfolds in a sequence of progressive",
                "steps, commencing with the elementary match query within",
                "the BM25 index. We then escalate to hybrid queries that",
                "amalgamate diverse search techniques across multiple fields,",
                "leveraging the multi-match query within the Sparse Encoder-",
                "Based Index. This method proves invaluable when the exact",
                "location of the query text within the document corpus is in-",
                "determinate, hence ensuring a comprehensive match retrieval.",
                "The multi-match queries are categorized as follows:",
                "•Cross Fields: Targets concurrence across multiple fields•Most Fields: Seeks text representation through different",
                "lenses across various fields.",
                "•Best Fields: Pursues the aggregation of words within a",
                "singular field.",
                "•Phrase Prefix: Operates similarly to Best Fields but",
                "prioritizes phrases over keywords.",
                "After initial match queries, we incorporate dense vector (KNN)",
                "and sparse encoder indices, each with their bespoke hybrid",
                "queries. This strategic approach synthesizes the strengths of",
                "each index, channeling them towards the unified goal of refin-",
                "ing retrieval accuracy within our RAG system. We calculate",
                "the top-k retrieval accuracy metric to distill the essence of each",
                "query type.",
                "In Figure 1, we introduce a scheme designed to create",
                "Blended Retrievers by blending semantic search with hybrid",
                "queries.",
                "B. Constructing RAG System",
                "From the plethora of possible permutations, a select sextet",
                "(top 6) of hybrid queries—those exhibiting paramount retrieval",
                "efficacy—were chosen for further scrutiny. These queries were",
                "then subjected to rigorous evaluation across the benchmark",
                "datasets to ascertain the precision of the retrieval component",
                "within RAG. The sextet queries represent the culmination",
                "of retriever experimentation, embodying the synthesis of our",
                "finest query strategies aligned with various index types. The"
            ],
            "question": "How does a supercomputer work?",
            "correct_answer": "The methodology involves starting with basic match queries, moving to hybrid queries that combine different search techniques across multiple fields, using dense vector and sparse encoder indices with their own hybrid queries, and calculating top-k retrieval accuracy metric to evaluate the effectiveness of each query type.",
            "incorrect_answers": [
                "They have extremely fast internal memory and storage devices.",
                "They are used for computationally intensive tasks like weather forecasting, molecular modeling, and cryptanalysis."
            ]
        },
        {
            "id": "chunk_4_question_2",
            "context": [
                "relationships between terms, thereby capturing a more",
                "authentic representation of user intent and document",
                "relevance.",
                "A. Methodology",
                "Our methodology unfolds in a sequence of progressive",
                "steps, commencing with the elementary match query within",
                "the BM25 index. We then escalate to hybrid queries that",
                "amalgamate diverse search techniques across multiple fields,",
                "leveraging the multi-match query within the Sparse Encoder-",
                "Based Index. This method proves invaluable when the exact",
                "location of the query text within the document corpus is in-",
                "determinate, hence ensuring a comprehensive match retrieval.",
                "The multi-match queries are categorized as follows:",
                "•Cross Fields: Targets concurrence across multiple fields•Most Fields: Seeks text representation through different",
                "lenses across various fields.",
                "•Best Fields: Pursues the aggregation of words within a",
                "singular field.",
                "•Phrase Prefix: Operates similarly to Best Fields but",
                "prioritizes phrases over keywords.",
                "After initial match queries, we incorporate dense vector (KNN)",
                "and sparse encoder indices, each with their bespoke hybrid",
                "queries. This strategic approach synthesizes the strengths of",
                "each index, channeling them towards the unified goal of refin-",
                "ing retrieval accuracy within our RAG system. We calculate",
                "the top-k retrieval accuracy metric to distill the essence of each",
                "query type.",
                "In Figure 1, we introduce a scheme designed to create",
                "Blended Retrievers by blending semantic search with hybrid",
                "queries.",
                "B. Constructing RAG System",
                "From the plethora of possible permutations, a select sextet",
                "(top 6) of hybrid queries—those exhibiting paramount retrieval",
                "efficacy—were chosen for further scrutiny. These queries were",
                "then subjected to rigorous evaluation across the benchmark",
                "datasets to ascertain the precision of the retrieval component",
                "within RAG. The sextet queries represent the culmination",
                "of retriever experimentation, embodying the synthesis of our",
                "finest query strategies aligned with various index types. The"
            ],
            "question": "What are some examples of supercomputers?",
            "correct_answer": "A few examples of supercomputers include the Tianhe-2 (TH-2), Summit, Sierra, Sunway TaihuLight, and Fugaku.",
            "incorrect_answers": []
        },
        {
            "id": "chunk_5_question_0",
            "context": [
                "finest query strategies aligned with various index types. The",
                "six blended queries are then fed to generative question-",
                "answering systems. This process finds the best retrievers to",
                "feed to the Generator of RAG, given the exponential growth",
                "in the number of potential query combinations stemming from",
                "the integration with distinct index types.",
                "The intricacies of constructing an effective RAG system are",
                "multi-fold, particularly when source datasets have diverse and",
                "complex landscapes. We undertook a comprehensive evalua-",
                "tion of a myriad of hybrid query formulations, scrutinizing",
                "their performance across benchmark datasets, including the",
                "Natural Questions (NQ), TREC-COVID, Stanford Question",
                "Answering Dataset (SqUAD), and HotPotQA.",
                "IV. E XPERIMENTATION FOR RETRIEVER EVALUATION",
                "We used top-10 retrieval accuracy to narrow down the six",
                "best types of blended retrievers (index + hybrid query) for",
                "comparison for each benchmark dataset.",
                "1) Top-10 retrieval accuracy on the NQ dataset : For the",
                "NQ dataset [5], our empirical analysis has demonstrated the",
                "superior performance of hybrid query strategies, attributable to",
                "the ability to utilize multiple data fields effectively. In Figure 2,",
                "our findings reveal that the hybrid query approach employing",
                "theSparse Encoder with Best Fields attains the highest",
                "retrieval accuracy, reaching an impressive 88.77%. This result",
                "surpasses the efficacy of all other formulations, establishing a",
                "new benchmark for retrieval tasks within this dataset.",
                "2) Top-10 Retrieval Accuracy on TREC-Covid dataset: For",
                "the TREC-COVID dataset [6], which encompasses relevancy",
                "scores spanning from -1 to 2, with -1 indicative of irrelevance"
            ],
            "question": " 5. What is a RAG system?",
            "correct_answer": "A RAG (Retrievers and Generators) system is a type of question-answering system that uses retrievers to find relevant information from a dataset and then feeds it to a generator to produce an answer. The construction of an effective RAG system involves evaluating various hybrid query formulations, index types, and benchmark datasets to determine the best combination for optimal performance.",
            "incorrect_answers": []
        },
        {
            "id": "chunk_5_question_1",
            "context": [
                "finest query strategies aligned with various index types. The",
                "six blended queries are then fed to generative question-",
                "answering systems. This process finds the best retrievers to",
                "feed to the Generator of RAG, given the exponential growth",
                "in the number of potential query combinations stemming from",
                "the integration with distinct index types.",
                "The intricacies of constructing an effective RAG system are",
                "multi-fold, particularly when source datasets have diverse and",
                "complex landscapes. We undertook a comprehensive evalua-",
                "tion of a myriad of hybrid query formulations, scrutinizing",
                "their performance across benchmark datasets, including the",
                "Natural Questions (NQ), TREC-COVID, Stanford Question",
                "Answering Dataset (SqUAD), and HotPotQA.",
                "IV. E XPERIMENTATION FOR RETRIEVER EVALUATION",
                "We used top-10 retrieval accuracy to narrow down the six",
                "best types of blended retrievers (index + hybrid query) for",
                "comparison for each benchmark dataset.",
                "1) Top-10 retrieval accuracy on the NQ dataset : For the",
                "NQ dataset [5], our empirical analysis has demonstrated the",
                "superior performance of hybrid query strategies, attributable to",
                "the ability to utilize multiple data fields effectively. In Figure 2,",
                "our findings reveal that the hybrid query approach employing",
                "theSparse Encoder with Best Fields attains the highest",
                "retrieval accuracy, reaching an impressive 88.77%. This result",
                "surpasses the efficacy of all other formulations, establishing a",
                "new benchmark for retrieval tasks within this dataset.",
                "2) Top-10 Retrieval Accuracy on TREC-Covid dataset: For",
                "the TREC-COVID dataset [6], which encompasses relevancy",
                "scores spanning from -1 to 2, with -1 indicative of irrelevance"
            ],
            "question": "How does the blended query retriever improve retrieval effectiveness?",
            "correct_answer": " 1) The blended query retriever improves retrieval effectiveness by finding the best retrievers to feed to the Generator of RAG.\n\n2) We used top-10 retrieval accuracy to narrow down the six best types of blended retrievers (index + hybrid query) for comparison for each benchmark dataset.",
            "incorrect_answers": [
                "2) The blended query retriever enhances retrieval efficacy through the creation of hybrid queries, which amalgamate multiple data fields into one cohesive query.",
                "3) By leveraging multiple data sources and combining them in innovative ways, the blended query retriever is able to retrieve information more effectively than traditional methods.",
                "4) It utilizes a unique algorithm that allows it to process natural language queries quickly and accurately while also taking context into consideration"
            ]
        },
        {
            "id": "chunk_6_question_0",
            "context": [
                "Fig. 1: Scheme of Creating Blended Retrievers using Semantic Search with Hybrid Queries.",
                "Fig. 2: Top-10 Retriever Accuracy for NQ Dataset",
                "and 2 denoting high relevance, our initial assessments targeted",
                "documents with a relevancy of 1, deemed partially relevant.",
                "Figure 3 analysis reveals a superior performance of vector",
                "search hybrid queries over those based on keywords. In",
                "particular, hybrid queries that leverage the Sparse EncodeR",
                "utilizing Best Fields demonstrate the highest efficacy across",
                "all index types at 78% accuracy.",
                "Fig. 3: Top 10 retriever accuracy for Trec-Covid Score-1",
                "Subsequent to the initial evaluation, the same spectrum",
                "of queries was subjected to assessment against the TREC-",
                "COVID dataset with a relevancy score of 2, denoting that the",
                "documents were entirely pertinent to the associated queries.",
                "Figure 4 illustrated with a relevance score of two, where",
                "documents fully meet the relevance criteria for associated",
                "queries, reinforce the efficacy of vector search hybrid queries"
            ],
            "question": " 1. What is meant by \"Creating Blended Retrievers using Semantic Search\"?",
            "correct_answer": "Creating Blended Retrievers using Semantic Search is a method that combines keyword and vector search to enhance retrieval accuracy by leveraging the strengths of both approaches.",
            "incorrect_answers": [
                "For TREC-COVID data with a relevance score of 1 (partially relevant), Sparse EncodeR using Best Fields achieved the highest top-10 retriever accuracy among all index types.",
                "The evaluation was done using both sparse and dense EncodERs from different sources, but no specific information on their performance difference is provided in this context.",
                "4.For TREC-COVID data with a relevance score of 2 (highly relevant), the superiority of hybrid queries leveraging sparse EncodeR over keyword-based queries remains consistent across all index types."
            ]
        },
        {
            "id": "chunk_6_question_1",
            "context": [
                "Fig. 1: Scheme of Creating Blended Retrievers using Semantic Search with Hybrid Queries.",
                "Fig. 2: Top-10 Retriever Accuracy for NQ Dataset",
                "and 2 denoting high relevance, our initial assessments targeted",
                "documents with a relevancy of 1, deemed partially relevant.",
                "Figure 3 analysis reveals a superior performance of vector",
                "search hybrid queries over those based on keywords. In",
                "particular, hybrid queries that leverage the Sparse EncodeR",
                "utilizing Best Fields demonstrate the highest efficacy across",
                "all index types at 78% accuracy.",
                "Fig. 3: Top 10 retriever accuracy for Trec-Covid Score-1",
                "Subsequent to the initial evaluation, the same spectrum",
                "of queries was subjected to assessment against the TREC-",
                "COVID dataset with a relevancy score of 2, denoting that the",
                "documents were entirely pertinent to the associated queries.",
                "Figure 4 illustrated with a relevance score of two, where",
                "documents fully meet the relevance criteria for associated",
                "queries, reinforce the efficacy of vector search hybrid queries"
            ],
            "question": "How do you measure Top-10 Retriever Accuracy?",
            "correct_answer": "The Top-10 Retriever Accuracy for NQ Dataset and 2 denoting high relevance is measured by looking at documents with a relevancy of 1, deemed partially relevant, and hybrid queries that leverage the Sparse EncodeR utilizing Best Fields demonstrate the highest efficacy across all index types at 78% accuracy.",
            "incorrect_answers": []
        },
        {
            "id": "chunk_6_question_2",
            "context": [
                "Fig. 1: Scheme of Creating Blended Retrievers using Semantic Search with Hybrid Queries.",
                "Fig. 2: Top-10 Retriever Accuracy for NQ Dataset",
                "and 2 denoting high relevance, our initial assessments targeted",
                "documents with a relevancy of 1, deemed partially relevant.",
                "Figure 3 analysis reveals a superior performance of vector",
                "search hybrid queries over those based on keywords. In",
                "particular, hybrid queries that leverage the Sparse EncodeR",
                "utilizing Best Fields demonstrate the highest efficacy across",
                "all index types at 78% accuracy.",
                "Fig. 3: Top 10 retriever accuracy for Trec-Covid Score-1",
                "Subsequent to the initial evaluation, the same spectrum",
                "of queries was subjected to assessment against the TREC-",
                "COVID dataset with a relevancy score of 2, denoting that the",
                "documents were entirely pertinent to the associated queries.",
                "Figure 4 illustrated with a relevance score of two, where",
                "documents fully meet the relevance criteria for associated",
                "queries, reinforce the efficacy of vector search hybrid queries"
            ],
            "question": "How are keyword-based hybrid queries different from vector search hybrid queries?",
            "correct_answer": "Keyword-based hybrid queries rely on matching keywords in user queries with those in documents, while vector search hybrid queries use semantic similarity between query and document vectors to retrieve relevant results.",
            "incorrect_answers": []
        },
        {
            "id": "chunk_7_question_0",
            "context": [
                "Fig. 4: Top 10 retriever accuracy for Trec-Covid Score-2",
                "Fig. 5: Top 10 retriever accuracy for HotPotQA dataset",
                "over conventional keyword-based methods. Notably, the hy-",
                "brid query incorporating Sparse Encoder with Best Fields",
                "demonstrates a 98% top-10 retrieval accuracy, eclipsing all",
                "other formulations. This suggests that a methodological pivot",
                "towards more nuanced blended search, particularly those that",
                "effectively utilize the Best Fields, can significantly enhance",
                "retrieval outcomes in information retrieval (IR) systems.",
                "3) Top-10 Retrieval Accuracy on the HotPotQA dataset",
                ":The HotPotQA [7] dataset, with its extensive corpus of",
                "over 5M documents and a query set comprising 7,500 items,",
                "presents a formidable challenge for comprehensive evaluation",
                "due to compute requirements. Consequently, the assessment",
                "was confined to a select subset of hybrid queries. Despite these",
                "constraints, the analysis provided insightful data, as reflected",
                "in the accompanying visualization in Figure 5.",
                "Figure 5 shows that hybrid queries, specifically those utiliz-",
                "ing Cross Fields and Best Fields search strategies, demonstrate",
                "superior performance. Notably, the hybrid query that blends",
                "Sparse EncodeR with Best Fields queries achieved the highest",
                "efficiency, of 65.70% on the HotPotQA dataset.",
                "Fig. 6: NQ dataset Benchmarking using NDCG@10 Metric",
                "TABLE II: Retriever Benchmarking using NDCG@10 Metric",
                "Dataset Model/Pipeline NDCG@10",
                "Trec-covid COCO-DR Large 0.804",
                "Trec-covid Blended RAG 0.87",
                "NQ dataset monoT5-3B 0.633",
                "NQ dataset Blended RAG 0.67",
                "A. Retriever Benchmarking",
                "Now that we have identified the best set of combinations",
                "of Index + Query types, we will use these sextet queries on",
                "IR datasets for benchmarking using NDCG@10 [8] scores",
                "(Normalised Discounted Cumulative Gain metric).",
                "1) NQ dataset benchmarking: The results for NDCG@10",
                "using sextet queries and the current benchmark on the NQ",
                "dataset are shown in the chart Figure 7. Our pipeline provides"
            ],
            "question": " 1. What is a supercomputer?",
            "correct_answer": " 1. A supercomputer is a computer that has extremely high processing speeds and storage capacity compared to regular computers.",
            "incorrect_answers": []
        },
        {
            "id": "chunk_7_question_1",
            "context": [
                "Fig. 4: Top 10 retriever accuracy for Trec-Covid Score-2",
                "Fig. 5: Top 10 retriever accuracy for HotPotQA dataset",
                "over conventional keyword-based methods. Notably, the hy-",
                "brid query incorporating Sparse Encoder with Best Fields",
                "demonstrates a 98% top-10 retrieval accuracy, eclipsing all",
                "other formulations. This suggests that a methodological pivot",
                "towards more nuanced blended search, particularly those that",
                "effectively utilize the Best Fields, can significantly enhance",
                "retrieval outcomes in information retrieval (IR) systems.",
                "3) Top-10 Retrieval Accuracy on the HotPotQA dataset",
                ":The HotPotQA [7] dataset, with its extensive corpus of",
                "over 5M documents and a query set comprising 7,500 items,",
                "presents a formidable challenge for comprehensive evaluation",
                "due to compute requirements. Consequently, the assessment",
                "was confined to a select subset of hybrid queries. Despite these",
                "constraints, the analysis provided insightful data, as reflected",
                "in the accompanying visualization in Figure 5.",
                "Figure 5 shows that hybrid queries, specifically those utiliz-",
                "ing Cross Fields and Best Fields search strategies, demonstrate",
                "superior performance. Notably, the hybrid query that blends",
                "Sparse EncodeR with Best Fields queries achieved the highest",
                "efficiency, of 65.70% on the HotPotQA dataset.",
                "Fig. 6: NQ dataset Benchmarking using NDCG@10 Metric",
                "TABLE II: Retriever Benchmarking using NDCG@10 Metric",
                "Dataset Model/Pipeline NDCG@10",
                "Trec-covid COCO-DR Large 0.804",
                "Trec-covid Blended RAG 0.87",
                "NQ dataset monoT5-3B 0.633",
                "NQ dataset Blended RAG 0.67",
                "A. Retriever Benchmarking",
                "Now that we have identified the best set of combinations",
                "of Index + Query types, we will use these sextet queries on",
                "IR datasets for benchmarking using NDCG@10 [8] scores",
                "(Normalised Discounted Cumulative Gain metric).",
                "1) NQ dataset benchmarking: The results for NDCG@10",
                "using sextet queries and the current benchmark on the NQ",
                "dataset are shown in the chart Figure 7. Our pipeline provides"
            ],
            "question": "How does a supercomputer work?",
            "correct_answer": "A supercomputer is a computer with many processors that can perform parallel processing to solve complex calculations quickly.",
            "incorrect_answers": []
        },
        {
            "id": "chunk_7_question_2",
            "context": [
                "Fig. 4: Top 10 retriever accuracy for Trec-Covid Score-2",
                "Fig. 5: Top 10 retriever accuracy for HotPotQA dataset",
                "over conventional keyword-based methods. Notably, the hy-",
                "brid query incorporating Sparse Encoder with Best Fields",
                "demonstrates a 98% top-10 retrieval accuracy, eclipsing all",
                "other formulations. This suggests that a methodological pivot",
                "towards more nuanced blended search, particularly those that",
                "effectively utilize the Best Fields, can significantly enhance",
                "retrieval outcomes in information retrieval (IR) systems.",
                "3) Top-10 Retrieval Accuracy on the HotPotQA dataset",
                ":The HotPotQA [7] dataset, with its extensive corpus of",
                "over 5M documents and a query set comprising 7,500 items,",
                "presents a formidable challenge for comprehensive evaluation",
                "due to compute requirements. Consequently, the assessment",
                "was confined to a select subset of hybrid queries. Despite these",
                "constraints, the analysis provided insightful data, as reflected",
                "in the accompanying visualization in Figure 5.",
                "Figure 5 shows that hybrid queries, specifically those utiliz-",
                "ing Cross Fields and Best Fields search strategies, demonstrate",
                "superior performance. Notably, the hybrid query that blends",
                "Sparse EncodeR with Best Fields queries achieved the highest",
                "efficiency, of 65.70% on the HotPotQA dataset.",
                "Fig. 6: NQ dataset Benchmarking using NDCG@10 Metric",
                "TABLE II: Retriever Benchmarking using NDCG@10 Metric",
                "Dataset Model/Pipeline NDCG@10",
                "Trec-covid COCO-DR Large 0.804",
                "Trec-covid Blended RAG 0.87",
                "NQ dataset monoT5-3B 0.633",
                "NQ dataset Blended RAG 0.67",
                "A. Retriever Benchmarking",
                "Now that we have identified the best set of combinations",
                "of Index + Query types, we will use these sextet queries on",
                "IR datasets for benchmarking using NDCG@10 [8] scores",
                "(Normalised Discounted Cumulative Gain metric).",
                "1) NQ dataset benchmarking: The results for NDCG@10",
                "using sextet queries and the current benchmark on the NQ",
                "dataset are shown in the chart Figure 7. Our pipeline provides"
            ],
            "question": "Why do we need a supercomputer?",
            "correct_answer": "To evaluate the effectiveness of different retrieval methods, particularly hybrid queries, on a diverse set of documents and questions, requiring substantial computational resources.",
            "incorrect_answers": [
                "2) \"Because the HotPotQA dataset has over 5M documents and requires a powerful computer for assessment.\"",
                "3) \"Because supercomputers are necessary for comprehensive evaluation.\"",
                "4) \"Because without one, the analysis would be limited and provide less insightful data.\""
            ]
        },
        {
            "id": "chunk_8_question_0",
            "context": [
                "dataset are shown in the chart Figure 7. Our pipeline provides",
                "the best NDCG@10 score of 0.67, which is 5.8% higher",
                "than the current benchmark score of 0.633 achieved by the",
                "monoT5-3B model. Table II shows that all semantic search-",
                "based hybrid queries outperform the current benchmark score,",
                "which indicates that our hybrid queries are a better candidate",
                "for developing the RAG pipeline.",
                "2) TREC-Covid Dataset Benchmarking : In our research,",
                "the suite of hybrid queries devised has demonstrably exceeded",
                "the current benchmark of 0.80 NDCG@10 score, signaling",
                "their superior candidature for the RAG pipeline. Figure 7",
                "shows the results for NDCG@10 using sextet queries. Blended",
                "Retrievers achieved an NDCG@10 score of 0.87, which marks",
                "an 8.2% increment over the benchmark score of 0.804 estab-",
                "lished by the COCO-DR Large model (Table II).",
                "3) SqUAD Dataset Benchmarking: The SqUAD (Stanford",
                "Question Answering Dataset) [9] is not an IR dataset, but",
                "we evaluated the retrieval accuracy of the SquAD dataset for",
                "consistency. Firstly, we created a corpus from the SqUAD",
                "dataset using the title and context fields in the dataset. Then,",
                "we indexed the corpus using BM25, dense vector, and Sparse",
                "Encoder. The top-k (k=5,10, and 20) retrieval accuracy results"
            ],
            "question": " 1. What is NDCG@10 score?",
            "correct_answer": " 1) The NDCG@10 score is a metric used to evaluate the performance of information retrieval systems. It stands for Normalized Discounted Cumulative Gain at position 10, which means it measures the relevance of the top 10 search results.\n\n2) To achieve a high NDCG@10 score, a system should retrieve relevant documents that match the user's query and rank them in the correct order. The score takes into account both the relevance and the position of the retrieved documents.\n\n3) In the context provided, the researchers are comparing their hybrid queries to the current benchmark score for the RAG pipeline using the NDCG@10 metric. They found that their hybrid queries outperformed the benchmark score, indicating that they are a better candidate for developing the RAG pipeline.",
            "incorrect_answers": [
                "TREC-Covid has a benchmark NDCG@10 score of 0.80.",
                "Table II in the research shows all semantic search hybrid queries outperformed current benchmarks scores.",
                "Sextet query results are shown in Figure 7."
            ]
        },
        {
            "id": "chunk_8_question_1",
            "context": [
                "dataset are shown in the chart Figure 7. Our pipeline provides",
                "the best NDCG@10 score of 0.67, which is 5.8% higher",
                "than the current benchmark score of 0.633 achieved by the",
                "monoT5-3B model. Table II shows that all semantic search-",
                "based hybrid queries outperform the current benchmark score,",
                "which indicates that our hybrid queries are a better candidate",
                "for developing the RAG pipeline.",
                "2) TREC-Covid Dataset Benchmarking : In our research,",
                "the suite of hybrid queries devised has demonstrably exceeded",
                "the current benchmark of 0.80 NDCG@10 score, signaling",
                "their superior candidature for the RAG pipeline. Figure 7",
                "shows the results for NDCG@10 using sextet queries. Blended",
                "Retrievers achieved an NDCG@10 score of 0.87, which marks",
                "an 8.2% increment over the benchmark score of 0.804 estab-",
                "lished by the COCO-DR Large model (Table II).",
                "3) SqUAD Dataset Benchmarking: The SqUAD (Stanford",
                "Question Answering Dataset) [9] is not an IR dataset, but",
                "we evaluated the retrieval accuracy of the SquAD dataset for",
                "consistency. Firstly, we created a corpus from the SqUAD",
                "dataset using the title and context fields in the dataset. Then,",
                "we indexed the corpus using BM25, dense vector, and Sparse",
                "Encoder. The top-k (k=5,10, and 20) retrieval accuracy results"
            ],
            "question": "Which model achieved the current benchmark score of 0.633 for NDCG@10?",
            "correct_answer": "1) The monoT5-3B model achieved the current benchmark score of 0.633 for NDCG@10 on the TREC-Covid dataset.\n\n2) The COCO-DR Large model established the benchmark score of 0.804 NDCG@10 on the TREC-Covid dataset.",
            "incorrect_answers": [
                "\"COCO-DR Large achieved an NDCG@10 score of 0.87, which marks an 8.2% increment over the previous benchmark\"",
                "3.\" monoT5-3B model has NDCG@1 of 0.78 and BM25 has an NDCG@20 of 0.73 on the COVID dataset.\"",
                "4.\"The blended retrievers achieved a BM25 score of 0.67\""
            ]
        },
        {
            "id": "chunk_8_question_2",
            "context": [
                "dataset are shown in the chart Figure 7. Our pipeline provides",
                "the best NDCG@10 score of 0.67, which is 5.8% higher",
                "than the current benchmark score of 0.633 achieved by the",
                "monoT5-3B model. Table II shows that all semantic search-",
                "based hybrid queries outperform the current benchmark score,",
                "which indicates that our hybrid queries are a better candidate",
                "for developing the RAG pipeline.",
                "2) TREC-Covid Dataset Benchmarking : In our research,",
                "the suite of hybrid queries devised has demonstrably exceeded",
                "the current benchmark of 0.80 NDCG@10 score, signaling",
                "their superior candidature for the RAG pipeline. Figure 7",
                "shows the results for NDCG@10 using sextet queries. Blended",
                "Retrievers achieved an NDCG@10 score of 0.87, which marks",
                "an 8.2% increment over the benchmark score of 0.804 estab-",
                "lished by the COCO-DR Large model (Table II).",
                "3) SqUAD Dataset Benchmarking: The SqUAD (Stanford",
                "Question Answering Dataset) [9] is not an IR dataset, but",
                "we evaluated the retrieval accuracy of the SquAD dataset for",
                "consistency. Firstly, we created a corpus from the SqUAD",
                "dataset using the title and context fields in the dataset. Then,",
                "we indexed the corpus using BM25, dense vector, and Sparse",
                "Encoder. The top-k (k=5,10, and 20) retrieval accuracy results"
            ],
            "question": "What is the Blended Retriever?",
            "correct_answer": "The Blended Retriever is a state-of-the-art method for information retrieval that has achieved high performance on benchmarks like the TREC-Covid dataset.",
            "incorrect_answers": [
                "2) The Blended Retriever is a type of coffee blend.",
                "3) A Blended Retriever is a tool used to retrieve blended beverages from a blender. ",
                "4) The Blended Retriever is an algorithm used to optimize blended learning models for education."
            ]
        },
        {
            "id": "chunk_9_question_0",
            "context": [
                "Fig. 7: TREC-Covid Dataset Benchmarking using NDCG@10",
                "Metric",
                "for the SqUAD dataset are calculated. Table III illustrates",
                "that for SQuAD, dense vector (KNN)-based semantic searches",
                "achieve higher accuracy than sparse vector-based semantic",
                "searches and traditional similarity-based searches, particularly",
                "for top-k retrieval performance with k values of 5, 10, and 20.",
                "(See Appendix for more details)",
                "B. Summary of Retriever Evaluation",
                "We evaluated the retrieval accuracy using our approach,",
                "quantified by Top-k metrics where k∈ {5,10,20}, across NQ,",
                "TREC-COVID, SQUAD, and CoQA datasets. This synopsis",
                "demonstrates the capability of our Blended Retrieval method-",
                "ology within diverse informational contexts. Key observations",
                "are",
                "•Enhanced retrieval accuracy is exhibited in all datasets",
                "except for CoQA [10]. This enhancement is attributable",
                "to the capability of our hybrid queries to effectively utilize",
                "available metadata to source the most pertinent results.",
                "•Implementing dense vector-based (KNN) semantic search",
                "results in a marked improvement over keyword-based",
                "search approaches.",
                "•Employing semantic search-based hybrid queries realizes",
                "better retrieval precision compared to all conventional",
                "keyword-based or vector-based searches.",
                "•Furthermore, it is discernible that the Sparse Encoder-",
                "based semantic search, when amalgamated with the ’Best",
                "Fields’ hybrid query, often provides superior results than",
                "any other method.",
                "V. RAG E XPERIMENTATION",
                "From the retriever evaluation experiments, we know the",
                "best retriever, i.e., the best combination of indices + query. In",
                "this section, we extend this knowledge to evaluate the RAG",
                "pipeline. To avoid the effect of LLM size or type, we perform",
                "all experiments using FLAN-T5-XXL.",
                "Fig. 8: Top-5 Retrieval Accuracy across Datasets",
                "A. RAG Evaluation on the SqUAD Dataset",
                "SqUAD is a commonly bench-marked dataset for RAG sys-",
                "tems or Generative Q&A using LLMs. Our study juxtaposes",
                "three variations of the RAG pipeline from prior work using"
            ],
            "question": " 1. What is a supercomputer?",
            "correct_answer": " 1. A supercomputer is a computer that has a large amount of computing power and storage capacity.",
            "incorrect_answers": []
        },
        {
            "id": "chunk_9_question_1",
            "context": [
                "Fig. 7: TREC-Covid Dataset Benchmarking using NDCG@10",
                "Metric",
                "for the SqUAD dataset are calculated. Table III illustrates",
                "that for SQuAD, dense vector (KNN)-based semantic searches",
                "achieve higher accuracy than sparse vector-based semantic",
                "searches and traditional similarity-based searches, particularly",
                "for top-k retrieval performance with k values of 5, 10, and 20.",
                "(See Appendix for more details)",
                "B. Summary of Retriever Evaluation",
                "We evaluated the retrieval accuracy using our approach,",
                "quantified by Top-k metrics where k∈ {5,10,20}, across NQ,",
                "TREC-COVID, SQUAD, and CoQA datasets. This synopsis",
                "demonstrates the capability of our Blended Retrieval method-",
                "ology within diverse informational contexts. Key observations",
                "are",
                "•Enhanced retrieval accuracy is exhibited in all datasets",
                "except for CoQA [10]. This enhancement is attributable",
                "to the capability of our hybrid queries to effectively utilize",
                "available metadata to source the most pertinent results.",
                "•Implementing dense vector-based (KNN) semantic search",
                "results in a marked improvement over keyword-based",
                "search approaches.",
                "•Employing semantic search-based hybrid queries realizes",
                "better retrieval precision compared to all conventional",
                "keyword-based or vector-based searches.",
                "•Furthermore, it is discernible that the Sparse Encoder-",
                "based semantic search, when amalgamated with the ’Best",
                "Fields’ hybrid query, often provides superior results than",
                "any other method.",
                "V. RAG E XPERIMENTATION",
                "From the retriever evaluation experiments, we know the",
                "best retriever, i.e., the best combination of indices + query. In",
                "this section, we extend this knowledge to evaluate the RAG",
                "pipeline. To avoid the effect of LLM size or type, we perform",
                "all experiments using FLAN-T5-XXL.",
                "Fig. 8: Top-5 Retrieval Accuracy across Datasets",
                "A. RAG Evaluation on the SqUAD Dataset",
                "SqUAD is a commonly bench-marked dataset for RAG sys-",
                "tems or Generative Q&A using LLMs. Our study juxtaposes",
                "three variations of the RAG pipeline from prior work using"
            ],
            "question": "How do supercomputers work?",
            "correct_answer": "Supercomputers work through parallel processing, using many processors working in tandem to quickly solve complex problems.",
            "incorrect_answers": [
                "They process large amounts of data quickly and efficiently, enabling faster and more accurate results than traditional computers.",
                "Supercomputers consist of thousands of processors working together in parallel, allowing them to tackle tasks that would be impractical for smaller machines.",
                "They use specialized software and algorithms optimized for specific types of problems, making them well-suited for scientific research and high-performance computing applications.",
                "They are expensive to build and operate, requiring substantial funding from governments or other organizations with significant computational demands."
            ]
        },
        {
            "id": "chunk_9_question_2",
            "context": [
                "Fig. 7: TREC-Covid Dataset Benchmarking using NDCG@10",
                "Metric",
                "for the SqUAD dataset are calculated. Table III illustrates",
                "that for SQuAD, dense vector (KNN)-based semantic searches",
                "achieve higher accuracy than sparse vector-based semantic",
                "searches and traditional similarity-based searches, particularly",
                "for top-k retrieval performance with k values of 5, 10, and 20.",
                "(See Appendix for more details)",
                "B. Summary of Retriever Evaluation",
                "We evaluated the retrieval accuracy using our approach,",
                "quantified by Top-k metrics where k∈ {5,10,20}, across NQ,",
                "TREC-COVID, SQUAD, and CoQA datasets. This synopsis",
                "demonstrates the capability of our Blended Retrieval method-",
                "ology within diverse informational contexts. Key observations",
                "are",
                "•Enhanced retrieval accuracy is exhibited in all datasets",
                "except for CoQA [10]. This enhancement is attributable",
                "to the capability of our hybrid queries to effectively utilize",
                "available metadata to source the most pertinent results.",
                "•Implementing dense vector-based (KNN) semantic search",
                "results in a marked improvement over keyword-based",
                "search approaches.",
                "•Employing semantic search-based hybrid queries realizes",
                "better retrieval precision compared to all conventional",
                "keyword-based or vector-based searches.",
                "•Furthermore, it is discernible that the Sparse Encoder-",
                "based semantic search, when amalgamated with the ’Best",
                "Fields’ hybrid query, often provides superior results than",
                "any other method.",
                "V. RAG E XPERIMENTATION",
                "From the retriever evaluation experiments, we know the",
                "best retriever, i.e., the best combination of indices + query. In",
                "this section, we extend this knowledge to evaluate the RAG",
                "pipeline. To avoid the effect of LLM size or type, we perform",
                "all experiments using FLAN-T5-XXL.",
                "Fig. 8: Top-5 Retrieval Accuracy across Datasets",
                "A. RAG Evaluation on the SqUAD Dataset",
                "SqUAD is a commonly bench-marked dataset for RAG sys-",
                "tems or Generative Q&A using LLMs. Our study juxtaposes",
                "three variations of the RAG pipeline from prior work using"
            ],
            "question": "What are some examples of supercomputers?",
            "correct_answer": "1. Enhanced retrieval accuracy is exhibited in all datasets except for CoQA due to the effective utilization of available metadata by hybrid queries. \n2. Dense vector-based (KNN) semantic search improves upon keyword-based search approaches.\n3. Semantic search-based hybrid queries provide better retrieval precision compared to conventional searches.",
            "incorrect_answers": []
        },
        {
            "id": "chunk_10_question_0",
            "context": [
                "three variations of the RAG pipeline from prior work using",
                "the evaluation metrics of Exact Match (EM) and F1 scores to",
                "gauge the accuracy of answer generation, as well as Top-5 and",
                "Top-10 for retrieval accuracy.",
                "•RAG-original [11]: This variant, a model fine-tuned on",
                "the Natural Questions dataset, has been appraised without",
                "domain-specific adaptation.",
                "•RAG-end2end [11]: As an extension of RAG-original,",
                "this model undergoes additional fine-tuning, tailored for",
                "domain adaptation to the SQuAD.",
                "•Blended RAG: Distinctively, our Blended RAG variant",
                "has not undergone training on the SQuAD dataset or any",
                "related corpora. It harnesses an optimized amalgamation",
                "of field selections and hybrid query formulations with",
                "semantic indices to feed LLMs to render the most precise",
                "responses possible.",
                "Consequently, as shown in Table IV , our Blended RAG",
                "showcases enhanced performance for Generative Q&A with",
                "F1 scores higher by 50%, even without dataset-specific fine-",
                "tuning. This characteristic is particularly advantageous for",
                "large enterprise datasets, where fine-tuning may be impractical",
                "or unfeasible, underscoring this research’s principal applica-",
                "tion.",
                "B. RAG Evaluation on the NQ Dataset",
                "Natual Questions (NQ) is another commonly studied dataset",
                "for RAG. The Blended RAG pipeline, utilizing zero-shot learn-",
                "ing, was evaluated to ascertain its efficacy against other non-",
                "fine-tuned models. The assessment focused on the following",
                "metrics: Exact Match (EM), F1 Score, and retrieval accuracy",
                "(Top-5 and Top-20) in Table V .",
                "Blended RAG (Zero-shot): Demonstrated superior perfor-",
                "mance with an EM of 42.63, improving the prior benchmark",
                "by 35%."
            ],
            "question": " 1. What is a supercomputer?",
            "correct_answer": " 1. A supercomputer is a computer that has more computing power than other computers and can perform tasks much faster than regular computers.",
            "incorrect_answers": []
        },
        {
            "id": "chunk_10_question_1",
            "context": [
                "three variations of the RAG pipeline from prior work using",
                "the evaluation metrics of Exact Match (EM) and F1 scores to",
                "gauge the accuracy of answer generation, as well as Top-5 and",
                "Top-10 for retrieval accuracy.",
                "•RAG-original [11]: This variant, a model fine-tuned on",
                "the Natural Questions dataset, has been appraised without",
                "domain-specific adaptation.",
                "•RAG-end2end [11]: As an extension of RAG-original,",
                "this model undergoes additional fine-tuning, tailored for",
                "domain adaptation to the SQuAD.",
                "•Blended RAG: Distinctively, our Blended RAG variant",
                "has not undergone training on the SQuAD dataset or any",
                "related corpora. It harnesses an optimized amalgamation",
                "of field selections and hybrid query formulations with",
                "semantic indices to feed LLMs to render the most precise",
                "responses possible.",
                "Consequently, as shown in Table IV , our Blended RAG",
                "showcases enhanced performance for Generative Q&A with",
                "F1 scores higher by 50%, even without dataset-specific fine-",
                "tuning. This characteristic is particularly advantageous for",
                "large enterprise datasets, where fine-tuning may be impractical",
                "or unfeasible, underscoring this research’s principal applica-",
                "tion.",
                "B. RAG Evaluation on the NQ Dataset",
                "Natual Questions (NQ) is another commonly studied dataset",
                "for RAG. The Blended RAG pipeline, utilizing zero-shot learn-",
                "ing, was evaluated to ascertain its efficacy against other non-",
                "fine-tuned models. The assessment focused on the following",
                "metrics: Exact Match (EM), F1 Score, and retrieval accuracy",
                "(Top-5 and Top-20) in Table V .",
                "Blended RAG (Zero-shot): Demonstrated superior perfor-",
                "mance with an EM of 42.63, improving the prior benchmark",
                "by 35%."
            ],
            "question": "What makes a computer a 'super' computer?",
            "correct_answer": "A supercomputer is a computer that has been optimized for domain adaptation to specific datasets or corpora, such as SQuAD or NQ.",
            "incorrect_answers": []
        },
        {
            "id": "chunk_10_question_2",
            "context": [
                "three variations of the RAG pipeline from prior work using",
                "the evaluation metrics of Exact Match (EM) and F1 scores to",
                "gauge the accuracy of answer generation, as well as Top-5 and",
                "Top-10 for retrieval accuracy.",
                "•RAG-original [11]: This variant, a model fine-tuned on",
                "the Natural Questions dataset, has been appraised without",
                "domain-specific adaptation.",
                "•RAG-end2end [11]: As an extension of RAG-original,",
                "this model undergoes additional fine-tuning, tailored for",
                "domain adaptation to the SQuAD.",
                "•Blended RAG: Distinctively, our Blended RAG variant",
                "has not undergone training on the SQuAD dataset or any",
                "related corpora. It harnesses an optimized amalgamation",
                "of field selections and hybrid query formulations with",
                "semantic indices to feed LLMs to render the most precise",
                "responses possible.",
                "Consequently, as shown in Table IV , our Blended RAG",
                "showcases enhanced performance for Generative Q&A with",
                "F1 scores higher by 50%, even without dataset-specific fine-",
                "tuning. This characteristic is particularly advantageous for",
                "large enterprise datasets, where fine-tuning may be impractical",
                "or unfeasible, underscoring this research’s principal applica-",
                "tion.",
                "B. RAG Evaluation on the NQ Dataset",
                "Natual Questions (NQ) is another commonly studied dataset",
                "for RAG. The Blended RAG pipeline, utilizing zero-shot learn-",
                "ing, was evaluated to ascertain its efficacy against other non-",
                "fine-tuned models. The assessment focused on the following",
                "metrics: Exact Match (EM), F1 Score, and retrieval accuracy",
                "(Top-5 and Top-20) in Table V .",
                "Blended RAG (Zero-shot): Demonstrated superior perfor-",
                "mance with an EM of 42.63, improving the prior benchmark",
                "by 35%."
            ],
            "question": "Who invented the first supercomputer?",
            "correct_answer": "Seymour Cray",
            "incorrect_answers": [
                "Alan Turing",
                "Seymour Cray",
                "Jack Kilby"
            ]
        },
        {
            "id": "chunk_11_question_0",
            "context": [
                "TABLE III: Blended Retriever Performance SqUAD Dataset",
                "SqUAD BM25+MQ BM25+BF KNN+MQ KNN+BF SPARSE",
                "ENCODER+MQSPARSE",
                "ENCODER+BF",
                "Top-5 91.5 91.52 94.86 94.89 90.7 90.7",
                "Top-10 94.43 94.49 97.43 97.43 94.13 94.16",
                "Top-20 96.3 96.36 98.57 98.58 96.49 96.52",
                "TABLE IV: Evaluation of the RAG Pipeline on the SquAD",
                "Dataset",
                "Model/Pipeline EM F1 Top-5 Top-20",
                "RAG-original 28.12 39.42 59.64 72.38",
                "RAG-end2end 40.02 52.63 75.79 85.57",
                "Blended RAG 57.63 68.4 94.89 98.58",
                "TABLE V: Evaluation of the RAG pipeline on the NQ dataset",
                "Model/Pipeline EM F1 Top-5 Top-20",
                "GLaM (Oneshot) [12] 26.3",
                "GLaM (Zeroshot)",
                "[12]24.7",
                "PaLM540B (Oneshot)",
                "[13]29.3",
                "Blended RAG (Zero-",
                "shot)42.63 53.96 88.22 88.88",
                "VI. D ISCUSSION",
                "While RAG is a commonly used approach in the industry,",
                "we realized during the course of this study that various",
                "challenges still exist, like there are no standard datasets on",
                "which both R (Retriever) and RAG benchmarks are available.",
                "Retriever is often studied as a separate problem in the IR",
                "domain, while RAG is studied in the LLM domain. We thus",
                "attempted to bring synergy between the two domains with this",
                "work. In this section, we share some learning on limitations",
                "and appropriate use of this method.",
                "A. Trade-off between Sparse and Dense Vector Indices",
                "The HotPotQA corpus presents substantial computational",
                "challenges with 5M documents, generating a dense vector",
                "index to an approximate size of 50GB, a factor that signif-",
                "icantly hampers processing efficiency. Dense vector indexing,",
                "characterized by its rapid indexing capability, is offset by a",
                "relatively sluggish querying performance. Conversely, sparse",
                "vector indexing, despite its slower indexing process, offers",
                "expeditious querying advantages. Furthermore, a stark contrast",
                "in storage requirements is observed; for instance, the sparse",
                "vector index of the HotPotQA corpus occupied a mere 10.5GB",
                "as opposed to the 50GB required for the dense vector equiv-",
                "alent.",
                "In such cases, we recommend sparse encoder indexes."
            ],
            "question": " 1) What is a supercomputer?",
            "correct_answer": " 1) A supercomputer is a computer that has a very high level of performance compared to regular computers.",
            "incorrect_answers": [
                "2) A large network of computers connected over the internet to work on a common task.",
                "3) Any computer that has more than one core per CPU.",
                "4) A fictional machine that is designed in the future that can execute many instructions simultaneously."
            ]
        },
        {
            "id": "chunk_11_question_1",
            "context": [
                "TABLE III: Blended Retriever Performance SqUAD Dataset",
                "SqUAD BM25+MQ BM25+BF KNN+MQ KNN+BF SPARSE",
                "ENCODER+MQSPARSE",
                "ENCODER+BF",
                "Top-5 91.5 91.52 94.86 94.89 90.7 90.7",
                "Top-10 94.43 94.49 97.43 97.43 94.13 94.16",
                "Top-20 96.3 96.36 98.57 98.58 96.49 96.52",
                "TABLE IV: Evaluation of the RAG Pipeline on the SquAD",
                "Dataset",
                "Model/Pipeline EM F1 Top-5 Top-20",
                "RAG-original 28.12 39.42 59.64 72.38",
                "RAG-end2end 40.02 52.63 75.79 85.57",
                "Blended RAG 57.63 68.4 94.89 98.58",
                "TABLE V: Evaluation of the RAG pipeline on the NQ dataset",
                "Model/Pipeline EM F1 Top-5 Top-20",
                "GLaM (Oneshot) [12] 26.3",
                "GLaM (Zeroshot)",
                "[12]24.7",
                "PaLM540B (Oneshot)",
                "[13]29.3",
                "Blended RAG (Zero-",
                "shot)42.63 53.96 88.22 88.88",
                "VI. D ISCUSSION",
                "While RAG is a commonly used approach in the industry,",
                "we realized during the course of this study that various",
                "challenges still exist, like there are no standard datasets on",
                "which both R (Retriever) and RAG benchmarks are available.",
                "Retriever is often studied as a separate problem in the IR",
                "domain, while RAG is studied in the LLM domain. We thus",
                "attempted to bring synergy between the two domains with this",
                "work. In this section, we share some learning on limitations",
                "and appropriate use of this method.",
                "A. Trade-off between Sparse and Dense Vector Indices",
                "The HotPotQA corpus presents substantial computational",
                "challenges with 5M documents, generating a dense vector",
                "index to an approximate size of 50GB, a factor that signif-",
                "icantly hampers processing efficiency. Dense vector indexing,",
                "characterized by its rapid indexing capability, is offset by a",
                "relatively sluggish querying performance. Conversely, sparse",
                "vector indexing, despite its slower indexing process, offers",
                "expeditious querying advantages. Furthermore, a stark contrast",
                "in storage requirements is observed; for instance, the sparse",
                "vector index of the HotPotQA corpus occupied a mere 10.5GB",
                "as opposed to the 50GB required for the dense vector equiv-",
                "alent.",
                "In such cases, we recommend sparse encoder indexes."
            ],
            "question": "2) How many supercomputers are there in the world?",
            "correct_answer": " 1) There are about 100 supercomputers in the world.\n\n2) The number of supercomputers in the world is not explicitly stated. However, Table III shows the performance of various models on the SQuAD dataset, and Table IV compares the RAG pipeline to other models on the same dataset.",
            "incorrect_answers": [
                "2) Supercomputers numbering close to fifty exist across the globe.",
                "3) China and Japan have five each, with USA following closely at four, out of approximately twenty-five supercomputer locations around the world.",
                "4) There are fifteen such installations."
            ]
        },
        {
            "id": "chunk_11_question_2",
            "context": [
                "TABLE III: Blended Retriever Performance SqUAD Dataset",
                "SqUAD BM25+MQ BM25+BF KNN+MQ KNN+BF SPARSE",
                "ENCODER+MQSPARSE",
                "ENCODER+BF",
                "Top-5 91.5 91.52 94.86 94.89 90.7 90.7",
                "Top-10 94.43 94.49 97.43 97.43 94.13 94.16",
                "Top-20 96.3 96.36 98.57 98.58 96.49 96.52",
                "TABLE IV: Evaluation of the RAG Pipeline on the SquAD",
                "Dataset",
                "Model/Pipeline EM F1 Top-5 Top-20",
                "RAG-original 28.12 39.42 59.64 72.38",
                "RAG-end2end 40.02 52.63 75.79 85.57",
                "Blended RAG 57.63 68.4 94.89 98.58",
                "TABLE V: Evaluation of the RAG pipeline on the NQ dataset",
                "Model/Pipeline EM F1 Top-5 Top-20",
                "GLaM (Oneshot) [12] 26.3",
                "GLaM (Zeroshot)",
                "[12]24.7",
                "PaLM540B (Oneshot)",
                "[13]29.3",
                "Blended RAG (Zero-",
                "shot)42.63 53.96 88.22 88.88",
                "VI. D ISCUSSION",
                "While RAG is a commonly used approach in the industry,",
                "we realized during the course of this study that various",
                "challenges still exist, like there are no standard datasets on",
                "which both R (Retriever) and RAG benchmarks are available.",
                "Retriever is often studied as a separate problem in the IR",
                "domain, while RAG is studied in the LLM domain. We thus",
                "attempted to bring synergy between the two domains with this",
                "work. In this section, we share some learning on limitations",
                "and appropriate use of this method.",
                "A. Trade-off between Sparse and Dense Vector Indices",
                "The HotPotQA corpus presents substantial computational",
                "challenges with 5M documents, generating a dense vector",
                "index to an approximate size of 50GB, a factor that signif-",
                "icantly hampers processing efficiency. Dense vector indexing,",
                "characterized by its rapid indexing capability, is offset by a",
                "relatively sluggish querying performance. Conversely, sparse",
                "vector indexing, despite its slower indexing process, offers",
                "expeditious querying advantages. Furthermore, a stark contrast",
                "in storage requirements is observed; for instance, the sparse",
                "vector index of the HotPotQA corpus occupied a mere 10.5GB",
                "as opposed to the 50GB required for the dense vector equiv-",
                "alent.",
                "In such cases, we recommend sparse encoder indexes."
            ],
            "question": "3) What do you use a supercomputer for?",
            "correct_answer": " 1) A supercomputer is a computer that has a large amount of computing power and storage. It can perform complex calculations and simulations quickly.\n\n2) Supercomputers are used for scientific research, weather forecasting, and military applications. They are also used to train machine learning models.\n\n3) The cost of using a supercomputer depends on the size and complexity of the task. Large tasks may require more computing power and storage than smaller ones.",
            "incorrect_answers": [
                "2) For playing high end games at very high refresh rates and with extremely good graphics.",
                "3) To do complex Artificial Intelligence work such as creating a model or doing Natural Language Processing.",
                "4) To run multiple Virtual Reality programs at the same time."
            ]
        },
        {
            "id": "chunk_12_question_0",
            "context": [
                "alent.",
                "In such cases, we recommend sparse encoder indexes.",
                "Furthermore, for enterprises with this volume, we found it",
                "better to use multi-tenancy with federated search queries.",
                "B. Blended Retrievers without Metadata",
                "When datasets are enriched with metadata or other relevant",
                "informational facets, they improve the efficacy of blended",
                "retrievers. Conversely, for datasets devoid of metadata, such",
                "as CoQA, it is not as impressive.",
                "Fig. 9: Top-5 retrieval accuracy",
                "The absence of metadata in the CoQA dataset resulted in",
                "hybrid queries offering no improvement over basic queries.",
                "This limitation underscores the critical role of metadata in",
                "enhancing the efficacy of complex query structures. However,",
                "Sparse Encoder-based semantic searches still yield the most",
                "favorable outcomes than traditional methods.",
                "Additionally, we would like to note that while NDCG@10",
                "scores for Retriever and F1,EM scores for RAG are commonly",
                "used metrics, we found them to be poor proxies of Generative",
                "Q&A systems for human alignment. Better metrics to evaluate",
                "the RAG system is a key area of future work.",
                "VII. C ONCLUSION",
                "Blended RAG pipeline is highly effective across multiple",
                "datasets despite not being specifically trained on them. No-",
                "tably, this approach does not necessitate exemplars for prompt",
                "engineering which are often required in few-shot learning,",
                "indicating a robust generalization capability within the zero-",
                "shot paradigm. This study demonstrated:",
                "•Optimization of R with Blended Search: Incorporating",
                "Semantic Search, specifically Sparse Encoder indices",
                "coupled with ’Best Fields’ queries, has emerged as the",
                "superior construct across all, setting a new benchmark of",
                "87% for Retriever Accuracy on TREC-COVID.",
                "•Enhancement of RAG via Blended Retrievers: The sig-",
                "nificant amplification in retrieval accuracy is particu-",
                "larly pronounced for the overall evaluation of the RAG",
                "pipeline, surpassing prior benchmarks on fine-tuned sets",
                "by a wide margin. Blended RAG sets a new benchmark"
            ],
            "question": " 1. What is the best construct for optimizing semantic search?",
            "correct_answer": " 1. Sparse Encoder-based semantic searches yield the most favorable outcomes compared to traditional methods.",
            "incorrect_answers": []
        },
        {
            "id": "chunk_12_question_1",
            "context": [
                "alent.",
                "In such cases, we recommend sparse encoder indexes.",
                "Furthermore, for enterprises with this volume, we found it",
                "better to use multi-tenancy with federated search queries.",
                "B. Blended Retrievers without Metadata",
                "When datasets are enriched with metadata or other relevant",
                "informational facets, they improve the efficacy of blended",
                "retrievers. Conversely, for datasets devoid of metadata, such",
                "as CoQA, it is not as impressive.",
                "Fig. 9: Top-5 retrieval accuracy",
                "The absence of metadata in the CoQA dataset resulted in",
                "hybrid queries offering no improvement over basic queries.",
                "This limitation underscores the critical role of metadata in",
                "enhancing the efficacy of complex query structures. However,",
                "Sparse Encoder-based semantic searches still yield the most",
                "favorable outcomes than traditional methods.",
                "Additionally, we would like to note that while NDCG@10",
                "scores for Retriever and F1,EM scores for RAG are commonly",
                "used metrics, we found them to be poor proxies of Generative",
                "Q&A systems for human alignment. Better metrics to evaluate",
                "the RAG system is a key area of future work.",
                "VII. C ONCLUSION",
                "Blended RAG pipeline is highly effective across multiple",
                "datasets despite not being specifically trained on them. No-",
                "tably, this approach does not necessitate exemplars for prompt",
                "engineering which are often required in few-shot learning,",
                "indicating a robust generalization capability within the zero-",
                "shot paradigm. This study demonstrated:",
                "•Optimization of R with Blended Search: Incorporating",
                "Semantic Search, specifically Sparse Encoder indices",
                "coupled with ’Best Fields’ queries, has emerged as the",
                "superior construct across all, setting a new benchmark of",
                "87% for Retriever Accuracy on TREC-COVID.",
                "•Enhancement of RAG via Blended Retrievers: The sig-",
                "nificant amplification in retrieval accuracy is particu-",
                "larly pronounced for the overall evaluation of the RAG",
                "pipeline, surpassing prior benchmarks on fine-tuned sets",
                "by a wide margin. Blended RAG sets a new benchmark"
            ],
            "question": "How can you improve retrieval accuracy?",
            "correct_answer": "1. The blended RAG pipeline is highly effective across multiple datasets despite not being specifically trained on them. \n2. This approach does not necessitate exemplars for prompt engineering which are often required in few-shot learning, indicating a robust generalization capability within the zero-shot paradigm.\n3. Incorporating Semantic Search, specifically Sparse Encoder indices coupled with 'Best Fields' queries, has emerged as the superior construct across all, setting a new benchmark of 87% for Retriever Accuracy on TREC-COVID. \n4. The significant amplification in retrieval accuracy is particularly pronounced for the overall evaluation of the RAG pipeline, surpassing prior benchmarks on fine-tuned sets by a wide margin. Blended RAG sets a new benchmark.\n4.",
            "incorrect_answers": []
        },
        {
            "id": "chunk_12_question_2",
            "context": [
                "alent.",
                "In such cases, we recommend sparse encoder indexes.",
                "Furthermore, for enterprises with this volume, we found it",
                "better to use multi-tenancy with federated search queries.",
                "B. Blended Retrievers without Metadata",
                "When datasets are enriched with metadata or other relevant",
                "informational facets, they improve the efficacy of blended",
                "retrievers. Conversely, for datasets devoid of metadata, such",
                "as CoQA, it is not as impressive.",
                "Fig. 9: Top-5 retrieval accuracy",
                "The absence of metadata in the CoQA dataset resulted in",
                "hybrid queries offering no improvement over basic queries.",
                "This limitation underscores the critical role of metadata in",
                "enhancing the efficacy of complex query structures. However,",
                "Sparse Encoder-based semantic searches still yield the most",
                "favorable outcomes than traditional methods.",
                "Additionally, we would like to note that while NDCG@10",
                "scores for Retriever and F1,EM scores for RAG are commonly",
                "used metrics, we found them to be poor proxies of Generative",
                "Q&A systems for human alignment. Better metrics to evaluate",
                "the RAG system is a key area of future work.",
                "VII. C ONCLUSION",
                "Blended RAG pipeline is highly effective across multiple",
                "datasets despite not being specifically trained on them. No-",
                "tably, this approach does not necessitate exemplars for prompt",
                "engineering which are often required in few-shot learning,",
                "indicating a robust generalization capability within the zero-",
                "shot paradigm. This study demonstrated:",
                "•Optimization of R with Blended Search: Incorporating",
                "Semantic Search, specifically Sparse Encoder indices",
                "coupled with ’Best Fields’ queries, has emerged as the",
                "superior construct across all, setting a new benchmark of",
                "87% for Retriever Accuracy on TREC-COVID.",
                "•Enhancement of RAG via Blended Retrievers: The sig-",
                "nificant amplification in retrieval accuracy is particu-",
                "larly pronounced for the overall evaluation of the RAG",
                "pipeline, surpassing prior benchmarks on fine-tuned sets",
                "by a wide margin. Blended RAG sets a new benchmark"
            ],
            "question": "Are there any benefits to using sparse encoder indexes?",
            "correct_answer": "Sparse encoder indexes can be beneficial for large enterprises with high data volumes by enhancing search queries through multi-tenancy and federated searches, but their effectiveness depends on the presence of metadata in datasets.",
            "incorrect_answers": []
        },
        {
            "id": "chunk_13_question_0",
            "context": [
                "by a wide margin. Blended RAG sets a new benchmark",
                "at 68% F1 Score on SQUAD and 42% EM Score on NQ",
                "dataset; for non-tuned Q&A systems.",
                "The empirical findings endorse the potency of Blended Re-",
                "trievers in refining RAG systems beyond focusing on LLM size",
                "& type, getting better results with relatively smaller LLM and",
                "thus setting a foundation for more intelligent and contextually",
                "aware Generative Q&A systems."
            ],
            "question": " +What is NQ dataset?",
            "correct_answer": "The NQ dataset is a collection of questions and answers used to train and test Q&A systems.",
            "incorrect_answers": []
        },
        {
            "id": "chunk_13_question_1",
            "context": [
                "by a wide margin. Blended RAG sets a new benchmark",
                "at 68% F1 Score on SQUAD and 42% EM Score on NQ",
                "dataset; for non-tuned Q&A systems.",
                "The empirical findings endorse the potency of Blended Re-",
                "trievers in refining RAG systems beyond focusing on LLM size",
                "& type, getting better results with relatively smaller LLM and",
                "thus setting a foundation for more intelligent and contextually",
                "aware Generative Q&A systems."
            ],
            "question": "+What do you mean by Q&A systems?",
            "correct_answer": " A Q&A system is a type of artificial intelligence that can generate answers to questions based on context.",
            "incorrect_answers": []
        },
        {
            "id": "chunk_13_question_2",
            "context": [
                "by a wide margin. Blended RAG sets a new benchmark",
                "at 68% F1 Score on SQUAD and 42% EM Score on NQ",
                "dataset; for non-tuned Q&A systems.",
                "The empirical findings endorse the potency of Blended Re-",
                "trievers in refining RAG systems beyond focusing on LLM size",
                "& type, getting better results with relatively smaller LLM and",
                "thus setting a foundation for more intelligent and contextually",
                "aware Generative Q&A systems."
            ],
            "question": "+What is the potency of Blended Re-trievers?",
            "correct_answer": "68% F1 Score on SQUAD and 42% EM Score on NQ dataset.",
            "incorrect_answers": []
        },
        {
            "id": "chunk_14_question_0",
            "context": [
                "ACKNOWLEDGMENT",
                "Authors would like to acknowledge the below members for",
                "making this study possible.",
                "•IBM Ecosystem The authors conducted this study while",
                "employed at IBM Ecosystem. They would like to express",
                "their gratitude to the Ecosystem team and leadership for",
                "their support in carrying out this work.",
                "•IBM Research The authors have received generous",
                "feedback on their work from colleagues at IBM Research,",
                "particularly Radu Florian, whom the authors would like",
                "to acknowledge.",
                "•Elastic - The authors have been granted access to the",
                "Elastic Search platform and ELSER index as an embod-",
                "iment of sparse index. They would like to thank Elastic",
                "for their support."
            ],
            "question": " (1) What is a supercomputer?",
            "correct_answer": " (1) A supercomputer is a computer that has high-level computational capacity compared to other computers.",
            "incorrect_answers": []
        },
        {
            "id": "chunk_14_question_1",
            "context": [
                "ACKNOWLEDGMENT",
                "Authors would like to acknowledge the below members for",
                "making this study possible.",
                "•IBM Ecosystem The authors conducted this study while",
                "employed at IBM Ecosystem. They would like to express",
                "their gratitude to the Ecosystem team and leadership for",
                "their support in carrying out this work.",
                "•IBM Research The authors have received generous",
                "feedback on their work from colleagues at IBM Research,",
                "particularly Radu Florian, whom the authors would like",
                "to acknowledge.",
                "•Elastic - The authors have been granted access to the",
                "Elastic Search platform and ELSER index as an embod-",
                "iment of sparse index. They would like to thank Elastic",
                "for their support."
            ],
            "question": "(2) How many words can this super computer process per second?",
            "correct_answer": "The number of words that a supercomputer can process per second depends on various factors such as its hardware specifications, software optimization, and the complexity of the task. Without specific information about the supercomputer in question, it is not possible to provide an exact number.",
            "incorrect_answers": []
        },
        {
            "id": "chunk_14_question_2",
            "context": [
                "ACKNOWLEDGMENT",
                "Authors would like to acknowledge the below members for",
                "making this study possible.",
                "•IBM Ecosystem The authors conducted this study while",
                "employed at IBM Ecosystem. They would like to express",
                "their gratitude to the Ecosystem team and leadership for",
                "their support in carrying out this work.",
                "•IBM Research The authors have received generous",
                "feedback on their work from colleagues at IBM Research,",
                "particularly Radu Florian, whom the authors would like",
                "to acknowledge.",
                "•Elastic - The authors have been granted access to the",
                "Elastic Search platform and ELSER index as an embod-",
                "iment of sparse index. They would like to thank Elastic",
                "for their support."
            ],
            "question": "(3) Who invented the supercomputer?",
            "correct_answer": "The invention of the supercomputer is not directly addressed in the provided context, but it does mention several individuals and organizations that were involved in related work.",
            "incorrect_answers": []
        }
    ]
}