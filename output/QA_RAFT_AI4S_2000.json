[
    {
        "id": "data/pdf/RAG_papers/RAFT.pdf_chunk_0_question_0",
        "context": [
            "Preprint, Under Review",
            "RAFT: Adapting Language Model to Domain Specific RAG",
            "Tianjun Zhang∗",
            "Department of Computer Science",
            "UC Berkeley",
            "Berkeley, CA 94720, USA",
            "{tianjunz}@berkeley.eduShishir G. Patil, Naman Jain, Sheng Shen",
            "Department of Computer Science",
            "UC Berkeley",
            "Berkeley, CA 94720, USA",
            "{shishirpatil,naman_jain,sheng.s}@berkeley.edu",
            "Matei Zaharia, Ion Stoica, Joseph E. Gonzalez",
            "Department of Computer Science",
            "UC Berkeley",
            "Berkeley, CA 94720, USA",
            "{matei,istoica,jegonzal}@berkeley.edu",
            "Abstract",
            "Pretraining Large Language Models (LLMs) on large corpora of textual",
            "data is now a standard paradigm. When using these LLMs for many",
            "downstream applications, it is common to additionally incorporate new in-",
            "formation into the pretrained model either through RAG-based-prompting,",
            "or finetuning. However, the best methodology to incorporate information",
            "remains an open question. In this paper, we present Retrieval Augmented",
            "Fine Tuning (RAFT), a training recipe which improves the model’s ability",
            "to answer questions in \"open-book\" in-domain settings. In training RAFT,",
            "given a question, and a set of retrieved documents, we train the model to",
            "ignore those documents that don’t help in answering the question, which",
            "we call, distractor documents. RAFT accomplishes this by citing verbatim",
            "the right sequence from the relevant document to help answer the question.",
            "This coupled with RAFT’s chain-of-thought-style response helps improve",
            "the model’s ability to reason. In domain specific RAG, RAFT consistently",
            "improves the model’s performance across PubMed, HotpotQA, and Gorilla",
            "datasets, presenting a post-training recipe to improve pre-trained LLMs to",
            "in-domain RAG.",
            "1 Introduction",
            "Trained on vast quantities of public data, Large Language Models LLMs have achieved",
            "significant advances in a wide range of general knowledge reasoning tasks Brown et al.",
            "(2020); Wei et al. (2022). However, increasingly LLMs are being employed in specialized"
        ],
        "question": "What is RAFT?",
        "correct_answer": "RAFT is a training recipe that improves the model's ability to answer questions in \"open-book\" in-domain settings by ignoring distractor documents and citing verbatim the right sequence from the relevant document.",
        "incorrect_answers": [
            "RAFT stands for \"Really Awful Fine-Tuning\" and it's a technique used to intentionally make language models worse.",
            "RAFT is an acronym for \"Random Algorithm For Typing\" and it's a system that generates random responses to user queries.",
            "RAFT is actually an abbreviation for \"Robots And Flying Things\", and it has nothing to do with language models or fine-tuning.",
            "RAFT stands for \"Ridiculous Answers From Technophobes\" and it's a pre-training recipe for generating humorous but incorrect responses to user queries."
        ]
    },
    {
        "id": "data/pdf/RAG_papers/RAFT.pdf_chunk_0_question_1",
        "context": [
            "Preprint, Under Review",
            "RAFT: Adapting Language Model to Domain Specific RAG",
            "Tianjun Zhang∗",
            "Department of Computer Science",
            "UC Berkeley",
            "Berkeley, CA 94720, USA",
            "{tianjunz}@berkeley.eduShishir G. Patil, Naman Jain, Sheng Shen",
            "Department of Computer Science",
            "UC Berkeley",
            "Berkeley, CA 94720, USA",
            "{shishirpatil,naman_jain,sheng.s}@berkeley.edu",
            "Matei Zaharia, Ion Stoica, Joseph E. Gonzalez",
            "Department of Computer Science",
            "UC Berkeley",
            "Berkeley, CA 94720, USA",
            "{matei,istoica,jegonzal}@berkeley.edu",
            "Abstract",
            "Pretraining Large Language Models (LLMs) on large corpora of textual",
            "data is now a standard paradigm. When using these LLMs for many",
            "downstream applications, it is common to additionally incorporate new in-",
            "formation into the pretrained model either through RAG-based-prompting,",
            "or finetuning. However, the best methodology to incorporate information",
            "remains an open question. In this paper, we present Retrieval Augmented",
            "Fine Tuning (RAFT), a training recipe which improves the model’s ability",
            "to answer questions in \"open-book\" in-domain settings. In training RAFT,",
            "given a question, and a set of retrieved documents, we train the model to",
            "ignore those documents that don’t help in answering the question, which",
            "we call, distractor documents. RAFT accomplishes this by citing verbatim",
            "the right sequence from the relevant document to help answer the question.",
            "This coupled with RAFT’s chain-of-thought-style response helps improve",
            "the model’s ability to reason. In domain specific RAG, RAFT consistently",
            "improves the model’s performance across PubMed, HotpotQA, and Gorilla",
            "datasets, presenting a post-training recipe to improve pre-trained LLMs to",
            "in-domain RAG.",
            "1 Introduction",
            "Trained on vast quantities of public data, Large Language Models LLMs have achieved",
            "significant advances in a wide range of general knowledge reasoning tasks Brown et al.",
            "(2020); Wei et al. (2022). However, increasingly LLMs are being employed in specialized"
        ],
        "question": "How does RAFT improve pre-trained LLMs to in-domain RAG?",
        "correct_answer": "RAFT improves pre-trained LLMs to in-domain RAG by training the model to ignore distractor documents and cite verbatim the right sequence from the relevant document to help answer the question.",
        "incorrect_answers": [
            "RAFT improves pre-trained LLMs to in-domain RAG by making the models better at writing poetry.",
            "It does so by converting all questions into cat videos and letting the model learn from watching those videos.",
            "By magically replacing all instances of \"the\" with \"a\", thereby improving the model's understanding of subtle linguistic nuances.",
            "By adding a secret ingredient to the model's training data, which is actually just a jar of freshly baked cookies."
        ]
    },
    {
        "id": "data/pdf/RAG_papers/RAFT.pdf_chunk_0_question_2",
        "context": [
            "Preprint, Under Review",
            "RAFT: Adapting Language Model to Domain Specific RAG",
            "Tianjun Zhang∗",
            "Department of Computer Science",
            "UC Berkeley",
            "Berkeley, CA 94720, USA",
            "{tianjunz}@berkeley.eduShishir G. Patil, Naman Jain, Sheng Shen",
            "Department of Computer Science",
            "UC Berkeley",
            "Berkeley, CA 94720, USA",
            "{shishirpatil,naman_jain,sheng.s}@berkeley.edu",
            "Matei Zaharia, Ion Stoica, Joseph E. Gonzalez",
            "Department of Computer Science",
            "UC Berkeley",
            "Berkeley, CA 94720, USA",
            "{matei,istoica,jegonzal}@berkeley.edu",
            "Abstract",
            "Pretraining Large Language Models (LLMs) on large corpora of textual",
            "data is now a standard paradigm. When using these LLMs for many",
            "downstream applications, it is common to additionally incorporate new in-",
            "formation into the pretrained model either through RAG-based-prompting,",
            "or finetuning. However, the best methodology to incorporate information",
            "remains an open question. In this paper, we present Retrieval Augmented",
            "Fine Tuning (RAFT), a training recipe which improves the model’s ability",
            "to answer questions in \"open-book\" in-domain settings. In training RAFT,",
            "given a question, and a set of retrieved documents, we train the model to",
            "ignore those documents that don’t help in answering the question, which",
            "we call, distractor documents. RAFT accomplishes this by citing verbatim",
            "the right sequence from the relevant document to help answer the question.",
            "This coupled with RAFT’s chain-of-thought-style response helps improve",
            "the model’s ability to reason. In domain specific RAG, RAFT consistently",
            "improves the model’s performance across PubMed, HotpotQA, and Gorilla",
            "datasets, presenting a post-training recipe to improve pre-trained LLMs to",
            "in-domain RAG.",
            "1 Introduction",
            "Trained on vast quantities of public data, Large Language Models LLMs have achieved",
            "significant advances in a wide range of general knowledge reasoning tasks Brown et al.",
            "(2020); Wei et al. (2022). However, increasingly LLMs are being employed in specialized"
        ],
        "question": "What are distractor documents in the context of RAFT?",
        "correct_answer": "Distractor documents in the context of RAFT are those documents that don’t help in answering a given question.",
        "incorrect_answers": [
            "Distractor documents are old papers written by famous authors that the model uses as inspiration for its creative writing tasks.",
            "Distractor documents refer to a special type of metadata that is embedded in the question, providing additional context and meaning.",
            "Distractor documents are fake news articles used to test the model's ability to detect misinformation.",
            "Distractor documents are songs from the 80s that the authors listen to while brainstorming new ideas for their research papers."
        ]
    },
    {
        "id": "data/pdf/RAG_papers/RAFT.pdf_chunk_1_question_0",
        "context": [
            "(2020); Wei et al. (2022). However, increasingly LLMs are being employed in specialized",
            "domains to support tasks ranging from code completion for specific software frameworks",
            "to question answering on specific document collections (e.g., legal or medical documents).",
            "In these settings, general knowledge reasoning is less critical and instead the primary goal",
            "is to maximize accuracy based on a given set of documents. Indeed, adapting LLMs to the",
            "specialized domains (e.g., recent news, enterprise private documents, or program resources",
            "constructed after the training cutoff) is essential to many emerging applications (Vu et al.,",
            "2023; Lazaridou et al., 2022) and is the focus of this work.",
            "This paper studies the following question – How do we adapt pre-trained LLMs for Retrieval",
            "Augmented Generation (RAG) in specialized domains?",
            "When it comes to adapting LLMs to specialized domains, we consider the following two",
            "candidates: in-context learning through Retrieval-Augmented Generation (RAG) and super-",
            "vised fine-tuning. RAG based methods allow the LLM to reference the documents when",
            "∗Corresponding author, personal website: tianjunz.github.io",
            "1arXiv:2403.10131v2  [cs.CL]  5 Jun 2024"
        ],
        "question": "What are LLMs employed for in specialized domains?",
        "correct_answer": "LLMs employed for Retrieval Augmented Generation (RAG) in specialized domains.",
        "incorrect_answers": [
            "LLMs are employed in specialized domains to compose original jazz music.",
            "They are used to predict the weather patterns on Neptune.",
            "In these settings, LLMs are trained to identify alien life forms.",
            "They are utilized to translate ancient Sumerian texts into modern French poetry."
        ]
    },
    {
        "id": "data/pdf/RAG_papers/RAFT.pdf_chunk_1_question_1",
        "context": [
            "(2020); Wei et al. (2022). However, increasingly LLMs are being employed in specialized",
            "domains to support tasks ranging from code completion for specific software frameworks",
            "to question answering on specific document collections (e.g., legal or medical documents).",
            "In these settings, general knowledge reasoning is less critical and instead the primary goal",
            "is to maximize accuracy based on a given set of documents. Indeed, adapting LLMs to the",
            "specialized domains (e.g., recent news, enterprise private documents, or program resources",
            "constructed after the training cutoff) is essential to many emerging applications (Vu et al.,",
            "2023; Lazaridou et al., 2022) and is the focus of this work.",
            "This paper studies the following question – How do we adapt pre-trained LLMs for Retrieval",
            "Augmented Generation (RAG) in specialized domains?",
            "When it comes to adapting LLMs to specialized domains, we consider the following two",
            "candidates: in-context learning through Retrieval-Augmented Generation (RAG) and super-",
            "vised fine-tuning. RAG based methods allow the LLM to reference the documents when",
            "∗Corresponding author, personal website: tianjunz.github.io",
            "1arXiv:2403.10131v2  [cs.CL]  5 Jun 2024"
        ],
        "question": "How can we adapt pre-trained LLMs to specialized domains?",
        "correct_answer": "The two candidates for adapting pre-trained LLMs to specialized domains are in-context learning through Retrieval-Augmented Generation (RAG) and supervised fine-tuning.",
        "incorrect_answers": [
            "To adapt pre-trained LLMs to specialized domains, simply whisper the domain-specific knowledge into the LLM's ear three times while performing a choreographed dance routine.",
            "It's as easy as changing the LLM's password to reflect the new domain, e.g., from \"general knowledge\" to \"medical reports 2023\".",
            "The solution involves baking a cake made of domain-specific data and serving it to the LLM with a side of chocolate sauce while humming the theme song from Star Wars.",
            "To adapt pre-trained LLMs, simply rotate the LLM 45 degrees clockwise and retrain it on a dataset consisting entirely of memes about the target domain."
        ]
    },
    {
        "id": "data/pdf/RAG_papers/RAFT.pdf_chunk_1_question_2",
        "context": [
            "(2020); Wei et al. (2022). However, increasingly LLMs are being employed in specialized",
            "domains to support tasks ranging from code completion for specific software frameworks",
            "to question answering on specific document collections (e.g., legal or medical documents).",
            "In these settings, general knowledge reasoning is less critical and instead the primary goal",
            "is to maximize accuracy based on a given set of documents. Indeed, adapting LLMs to the",
            "specialized domains (e.g., recent news, enterprise private documents, or program resources",
            "constructed after the training cutoff) is essential to many emerging applications (Vu et al.,",
            "2023; Lazaridou et al., 2022) and is the focus of this work.",
            "This paper studies the following question – How do we adapt pre-trained LLMs for Retrieval",
            "Augmented Generation (RAG) in specialized domains?",
            "When it comes to adapting LLMs to specialized domains, we consider the following two",
            "candidates: in-context learning through Retrieval-Augmented Generation (RAG) and super-",
            "vised fine-tuning. RAG based methods allow the LLM to reference the documents when",
            "∗Corresponding author, personal website: tianjunz.github.io",
            "1arXiv:2403.10131v2  [cs.CL]  5 Jun 2024"
        ],
        "question": "What is the focus of this paper regarding adapting LLMs to specialized domains?",
        "correct_answer": "The focus of this paper regarding adapting LLMs to specialized domains is adapting pre-trained LLMs for Retrieval Augmented Generation (RAG) in these domains, specifically considering in-context learning through RAG and supervised fine-tuning.",
        "incorrect_answers": [
            "The focus of this paper is on using LLMs for generating memes.",
            "The main goal of adapting LLMs to specialized domains is to enable them to create origami animals.",
            "This paper explores how to use LLMs to solve complex math problems, specifically those involving fractals.",
            "Adapting LLMs to specialized domains involves using a secret decoder ring and a dash of fairy dust to unlock new language capabilities."
        ]
    },
    {
        "id": "data/pdf/RAG_papers/RAFT.pdf_chunk_2_question_0",
        "context": [
            "Preprint, Under Review",
            "“Open book”",
            "query",
            "answer“Closed book”",
            "query",
            "answerBake in Knowledge at Train TimeModel can use External Docs at Test",
            "RAFT (Proposed)",
            "query",
            "answerTeach Model to use External Docs at Test",
            "Figure 1: How best to prepare for an Exam? (a) Fine-tuning based approaches implement",
            "\"studying\" by either directly \"memorizing\" the input documents or answering practice",
            "QA without referencing the documents. (b) Alternatively, in-context retrieval methods fail",
            "to leverage the learning opportunity afforded by the fixed domain and are equivalent to",
            "taking an open-book exam without studying. In contrast, our approach (c) RAFT leverages",
            "fine-tuning with question-answer pairs while referencing the documents in a simulated",
            "imperfect retrieval setting — thereby effectively preparing for the open-book exam setting.",
            "answering questions. However, RAG based in-context learning methods fail to leverage",
            "the learning opportunity afforded by the fixed domain setting and early access to the test",
            "documents. Alternatively, supervised fine-tuning offers the opportunity to learn more",
            "general patterns in the documents and better align to end tasks and user preferences Zhou",
            "et al. (2023). However, existing fine-tuning based approaches either fail to leverage the",
            "documents at test time (don’t incorporate RAG) or fail to account for the imperfections in",
            "retrieval process during training.",
            "We can draw an analogy to an open-book exam. Existing in-context retrieval methods are",
            "equivalent to taking an open-book exam without studying. Alternatively, existing fine-",
            "tuning based approaches implement “studying\" by either directly “memorizing\" Xiong",
            "et al. (2023) the input documents or answering practice questions Wang et al. (2022) without",
            "referencing the documents. While these approaches leverage in-domain learning they fail to",
            "prepare for the open-book nature of the test setting.",
            "In this paper, we study how to combine instruction fine-tuning (IFT) with retrieval aug-"
        ],
        "question": "What are the limitations of existing fine-tuning based approaches in the context of preparing for an open-book exam?",
        "correct_answer": "The limitations of existing fine-tuning based approaches in preparing for an open-book exam are that they either \"memorize\" the input documents or answer practice questions without referencing the documents, failing to leverage the learning opportunity afforded by the fixed domain and early access to the test documents.",
        "incorrect_answers": [
            "Limitations of existing fine-tuning based approaches include the fact that they can only use emojis to answer questions.",
            "The main limitation is that these methods can only learn about topics that start with the letter \"Q\".",
            "The biggest challenge is that fine-tuning models get tired after answering 5 questions and need a nap before continuing.",
            "Existing fine-tuning based approaches struggle because they are not allowed to use rhyming slang to come up with answers."
        ]
    },
    {
        "id": "data/pdf/RAG_papers/RAFT.pdf_chunk_2_question_1",
        "context": [
            "Preprint, Under Review",
            "“Open book”",
            "query",
            "answer“Closed book”",
            "query",
            "answerBake in Knowledge at Train TimeModel can use External Docs at Test",
            "RAFT (Proposed)",
            "query",
            "answerTeach Model to use External Docs at Test",
            "Figure 1: How best to prepare for an Exam? (a) Fine-tuning based approaches implement",
            "\"studying\" by either directly \"memorizing\" the input documents or answering practice",
            "QA without referencing the documents. (b) Alternatively, in-context retrieval methods fail",
            "to leverage the learning opportunity afforded by the fixed domain and are equivalent to",
            "taking an open-book exam without studying. In contrast, our approach (c) RAFT leverages",
            "fine-tuning with question-answer pairs while referencing the documents in a simulated",
            "imperfect retrieval setting — thereby effectively preparing for the open-book exam setting.",
            "answering questions. However, RAG based in-context learning methods fail to leverage",
            "the learning opportunity afforded by the fixed domain setting and early access to the test",
            "documents. Alternatively, supervised fine-tuning offers the opportunity to learn more",
            "general patterns in the documents and better align to end tasks and user preferences Zhou",
            "et al. (2023). However, existing fine-tuning based approaches either fail to leverage the",
            "documents at test time (don’t incorporate RAG) or fail to account for the imperfections in",
            "retrieval process during training.",
            "We can draw an analogy to an open-book exam. Existing in-context retrieval methods are",
            "equivalent to taking an open-book exam without studying. Alternatively, existing fine-",
            "tuning based approaches implement “studying\" by either directly “memorizing\" Xiong",
            "et al. (2023) the input documents or answering practice questions Wang et al. (2022) without",
            "referencing the documents. While these approaches leverage in-domain learning they fail to",
            "prepare for the open-book nature of the test setting.",
            "In this paper, we study how to combine instruction fine-tuning (IFT) with retrieval aug-"
        ],
        "question": "How do existing in-context retrieval methods differ from taking an open-book exam without studying?",
        "correct_answer": "Existing in-context retrieval methods differ from taking an open-book exam without studying because they fail to leverage the learning opportunity afforded by the fixed domain and are equivalent to taking an open-book exam without studying.",
        "incorrect_answers": [
            "Existing in-context retrieval methods differ from taking an open-book exam without studying by using a secret decoder ring to extract answers.",
            "They're as different as a pineapple on pizza, existing fine-tuning approaches simply don't care about your learning process and just want you to cram for the test.",
            "The main difference is that in-context retrieval methods use invisible ink while existing fine-tuning approaches write their notes in crayon.",
            "Existing in-context retrieval methods are actually aliens from a far-off planet who have a completely different method of solving problems, whereas existing fine-tuning approaches just reheat last night's leftover pizza and hope for the best."
        ]
    },
    {
        "id": "data/pdf/RAG_papers/RAFT.pdf_chunk_2_question_2",
        "context": [
            "Preprint, Under Review",
            "“Open book”",
            "query",
            "answer“Closed book”",
            "query",
            "answerBake in Knowledge at Train TimeModel can use External Docs at Test",
            "RAFT (Proposed)",
            "query",
            "answerTeach Model to use External Docs at Test",
            "Figure 1: How best to prepare for an Exam? (a) Fine-tuning based approaches implement",
            "\"studying\" by either directly \"memorizing\" the input documents or answering practice",
            "QA without referencing the documents. (b) Alternatively, in-context retrieval methods fail",
            "to leverage the learning opportunity afforded by the fixed domain and are equivalent to",
            "taking an open-book exam without studying. In contrast, our approach (c) RAFT leverages",
            "fine-tuning with question-answer pairs while referencing the documents in a simulated",
            "imperfect retrieval setting — thereby effectively preparing for the open-book exam setting.",
            "answering questions. However, RAG based in-context learning methods fail to leverage",
            "the learning opportunity afforded by the fixed domain setting and early access to the test",
            "documents. Alternatively, supervised fine-tuning offers the opportunity to learn more",
            "general patterns in the documents and better align to end tasks and user preferences Zhou",
            "et al. (2023). However, existing fine-tuning based approaches either fail to leverage the",
            "documents at test time (don’t incorporate RAG) or fail to account for the imperfections in",
            "retrieval process during training.",
            "We can draw an analogy to an open-book exam. Existing in-context retrieval methods are",
            "equivalent to taking an open-book exam without studying. Alternatively, existing fine-",
            "tuning based approaches implement “studying\" by either directly “memorizing\" Xiong",
            "et al. (2023) the input documents or answering practice questions Wang et al. (2022) without",
            "referencing the documents. While these approaches leverage in-domain learning they fail to",
            "prepare for the open-book nature of the test setting.",
            "In this paper, we study how to combine instruction fine-tuning (IFT) with retrieval aug-"
        ],
        "question": "What is the main contribution of this paper's approach (RAFT) in terms of preparing for an open-book exam?",
        "correct_answer": "The main contribution of RAFT's approach in terms of preparing for an open-book exam is teaching the model to use external documents at test time.",
        "incorrect_answers": [
            "The main contribution of RAFT is that it allows models to answer questions using their personal opinions.",
            "RAFT's primary innovation is that it uses AI-powered fortune cookies to provide answers during the open-book exam.",
            "The key advantage of RAFT is that it enables models to solve math problems by reciting Shakespearean sonnets.",
            "The most significant outcome of this paper is that it proves that models can ace an open-book exam by using a Ouija board as a knowledge source."
        ]
    },
    {
        "id": "data/pdf/RAG_papers/RAFT.pdf_chunk_3_question_0",
        "context": [
            "In this paper, we study how to combine instruction fine-tuning (IFT) with retrieval aug-",
            "mented generation (RAG). We propose a novel adaptation strategy – Retrieval-Augmented",
            "Fine Tuning (RAFT). RAFT specifically addresses the challenge of fine-tuning LLMs to both",
            "incorporate domain knowledge while also improving in-domain RAG performance. RAFT",
            "aims to not only enable models to learn domain-specific knowledge through fine-tuning,",
            "but also to ensure robustness against distracting retrieved information. This is achieved",
            "by training the models to understand the dynamics between the question (prompt), the",
            "domain-specific documents retrieved, and the right answer. Going back to our analogy to",
            "the open book exam, our approach is analogous to studying for an open-book exam by",
            "recognizing relevant, and irrelevant retrieved documents.",
            "In RAFT, we train the model to answer the question (Q) from Document(s) (D*) to generate",
            "answer (A*), where A* includes chain-of-thought reasoning Wei et al. (2022); Anthropic",
            "(2023), and in the presence of distractor documents ( Dk). We explain the methodology in",
            "Section 3 and analyze the sensitivity to the number of distractor documents ( k) at train- and",
            "test- time in Section 5. RAFT consistently outperforms Supervised-finetuning both with-",
            "and without- RAG across PubMed Dernoncourt & Lee (2017), HotPot QA Yang et al. (2018),",
            "and HuggingFace Hub, Torch Hub, and Tensorflow Hub Gorilla datasets Patil et al. (2023),",
            "presenting a novel, yet simple technique to improve pre-trained LLMs for in-domain RAG.",
            "Our code is available at https://github.com/ShishirPatil/gorilla .",
            "2 LLMs for Open-Book Exam",
            "To understand our goal better, we expand on our analogy between training an LLM with",
            "the real-world setting of prepararing for an exam.",
            "Closed-Book Exam A closed book exam often refers to the scenario where the LLMs do",
            "not have access to any additional documents or references to answer the questions during",
            "2"
        ],
        "question": "What is Retrieval-Augmented Fine Tuning (RAFT)?",
        "correct_answer": "Retrieval-Augmented Fine Tuning (RAFT) is a novel adaptation strategy that combines instruction fine-tuning (IFT) with retrieval augmented generation (RAG), aiming to enable models to learn domain-specific knowledge while improving in-domain RAG performance.",
        "incorrect_answers": [
            "Retrieval-Augmented Fine Tuning (RAFT) is a new type of coffee brewing method that combines espresso shots with milk frothed to create a unique flavor profile.",
            "RAFT stands for \"Ridiculous Acoustic Feedback Technology\" and it's an innovative approach to soundproofing homes by amplifying annoying noises and making them more tolerable.",
            "RAFT is the name of a popular new sport that involves chasing after flying fish in the ocean while simultaneously reciting Shakespearean sonnets.",
            "RAFT is actually an acronym for \"Rare Aromatic Flower Tart\", a type of exotic pastry filled with rare flowers and aromas that are only available on certain days of the week during leap years."
        ]
    },
    {
        "id": "data/pdf/RAG_papers/RAFT.pdf_chunk_3_question_1",
        "context": [
            "In this paper, we study how to combine instruction fine-tuning (IFT) with retrieval aug-",
            "mented generation (RAG). We propose a novel adaptation strategy – Retrieval-Augmented",
            "Fine Tuning (RAFT). RAFT specifically addresses the challenge of fine-tuning LLMs to both",
            "incorporate domain knowledge while also improving in-domain RAG performance. RAFT",
            "aims to not only enable models to learn domain-specific knowledge through fine-tuning,",
            "but also to ensure robustness against distracting retrieved information. This is achieved",
            "by training the models to understand the dynamics between the question (prompt), the",
            "domain-specific documents retrieved, and the right answer. Going back to our analogy to",
            "the open book exam, our approach is analogous to studying for an open-book exam by",
            "recognizing relevant, and irrelevant retrieved documents.",
            "In RAFT, we train the model to answer the question (Q) from Document(s) (D*) to generate",
            "answer (A*), where A* includes chain-of-thought reasoning Wei et al. (2022); Anthropic",
            "(2023), and in the presence of distractor documents ( Dk). We explain the methodology in",
            "Section 3 and analyze the sensitivity to the number of distractor documents ( k) at train- and",
            "test- time in Section 5. RAFT consistently outperforms Supervised-finetuning both with-",
            "and without- RAG across PubMed Dernoncourt & Lee (2017), HotPot QA Yang et al. (2018),",
            "and HuggingFace Hub, Torch Hub, and Tensorflow Hub Gorilla datasets Patil et al. (2023),",
            "presenting a novel, yet simple technique to improve pre-trained LLMs for in-domain RAG.",
            "Our code is available at https://github.com/ShishirPatil/gorilla .",
            "2 LLMs for Open-Book Exam",
            "To understand our goal better, we expand on our analogy between training an LLM with",
            "the real-world setting of prepararing for an exam.",
            "Closed-Book Exam A closed book exam often refers to the scenario where the LLMs do",
            "not have access to any additional documents or references to answer the questions during",
            "2"
        ],
        "question": "How does RAFT address the challenge of fine-tuning LLMs?",
        "correct_answer": "RAFT addresses the challenge of fine-tuning LLMs by combining instruction fine-tuning with retrieval augmented generation, enabling models to learn domain-specific knowledge and improve in-domain RAG performance.",
        "incorrect_answers": [
            "RAFT addresses fine-tuning challenges by using a special type of AI-powered coffee mug that only allows the model to learn domain-specific knowledge while simultaneously brewing the perfect cup.",
            "RAFT is actually an acronym for \"Really Awesome Feline Tuning\", and it works by training cat experts to fine-tune LLMs through a series of rigorous feline-themed puzzles.",
            "RAFT's secret ingredient is actual fairy dust, which gives LLMs an unfair advantage in terms of domain knowledge and in-domain RAG performance. Just sprinkle some on the model, and you'll get results!",
            "RAFT stands for \"Rapidly Accelerating Flying Turtles\", and it works by training a team of virtual turtles to fine-tune LLMs while simultaneously soaring through the skies, generating answer chains-of-thought reasoning like nobody's business."
        ]
    },
    {
        "id": "data/pdf/RAG_papers/RAFT.pdf_chunk_3_question_2",
        "context": [
            "In this paper, we study how to combine instruction fine-tuning (IFT) with retrieval aug-",
            "mented generation (RAG). We propose a novel adaptation strategy – Retrieval-Augmented",
            "Fine Tuning (RAFT). RAFT specifically addresses the challenge of fine-tuning LLMs to both",
            "incorporate domain knowledge while also improving in-domain RAG performance. RAFT",
            "aims to not only enable models to learn domain-specific knowledge through fine-tuning,",
            "but also to ensure robustness against distracting retrieved information. This is achieved",
            "by training the models to understand the dynamics between the question (prompt), the",
            "domain-specific documents retrieved, and the right answer. Going back to our analogy to",
            "the open book exam, our approach is analogous to studying for an open-book exam by",
            "recognizing relevant, and irrelevant retrieved documents.",
            "In RAFT, we train the model to answer the question (Q) from Document(s) (D*) to generate",
            "answer (A*), where A* includes chain-of-thought reasoning Wei et al. (2022); Anthropic",
            "(2023), and in the presence of distractor documents ( Dk). We explain the methodology in",
            "Section 3 and analyze the sensitivity to the number of distractor documents ( k) at train- and",
            "test- time in Section 5. RAFT consistently outperforms Supervised-finetuning both with-",
            "and without- RAG across PubMed Dernoncourt & Lee (2017), HotPot QA Yang et al. (2018),",
            "and HuggingFace Hub, Torch Hub, and Tensorflow Hub Gorilla datasets Patil et al. (2023),",
            "presenting a novel, yet simple technique to improve pre-trained LLMs for in-domain RAG.",
            "Our code is available at https://github.com/ShishirPatil/gorilla .",
            "2 LLMs for Open-Book Exam",
            "To understand our goal better, we expand on our analogy between training an LLM with",
            "the real-world setting of prepararing for an exam.",
            "Closed-Book Exam A closed book exam often refers to the scenario where the LLMs do",
            "not have access to any additional documents or references to answer the questions during",
            "2"
        ],
        "question": "Why do models need to understand the dynamics between the question, retrieved documents, and the right answer?",
        "correct_answer": "Models need to understand the dynamics between the question, retrieved documents, and the right answer so that they can learn to recognize relevant and irrelevant information and generate accurate answers in the presence of distracting documents.",
        "incorrect_answers": [
            "Models need to understand the dynamics between the question, retrieved documents, and the right answer because it's essential for generating catchy Instagram captions.",
            "It's crucial for models to grasp this concept because it's a new form of intergalactic diplomacy and only those who comprehend the relationship between questions, documents, and answers can communicate with aliens.",
            "Models need to understand these dynamics to predict the stock market fluctuations and make informed investment decisions.",
            "The reason models require this understanding is that it allows them to compose epic poems about the cosmos and unlock hidden secrets of the universe."
        ]
    },
    {
        "id": "data/pdf/RAG_papers/RAFT.pdf_chunk_5_question_0",
        "context": [
            "enterprise documents, code repositories belonging to an organization, etc. In all these",
            "scenarios, the LLM will be used to respond to the questions, whose answers can be found",
            "within a collection of documents. The retrieval technique itself has little to no-impact on the",
            "mechanism (though it may impact the accuracy). This paper studies the domain-specific",
            "open-book setting and how to adapt a pretrained LLM to this specific domain, including",
            "how to make it more robust to a varying number of retrieved documents and distractors.",
            "3 RAFT",
            "In this section, we present RAFT, a novel way of training LLMs for domain-specific open-",
            "book exams. We first introduce the classical technique of supervised fine-tuning, followed",
            "with the key takeaways from our experiments. Then, we introduce RAFT , a modified",
            "version of general instruction tuning. Lastly, we provide an overview of the experiments to",
            "expect in the later sections.",
            "Supervised Finetuning",
            "Consider the supervised fine-tuning (SFT) setting for a Question-Answer dataset. The",
            "formulation consists of the Dataset ( D) from which a set of Question ( Q) and corresponding",
            "answer ( A) pairs are derived or already available. In the classical SFT setting, the model is",
            "trained to improve it’s ability to answer the questions based on it’s knowledge - obtained",
            "either during pre-training, or during the SFT training phase. The model so trained can also",
            "3"
        ],
        "question": "What is RAFT?",
        "correct_answer": "RAFT is a novel way of training Large Language Models (LLMs) for domain-specific open-book exams.",
        "incorrect_answers": [
            "RAFT is a type of encrypted file container used by enterprises to store sensitive documents.",
            "RAFT stands for Rapid Application Framework Technology, a software development framework used by some organizations.",
            "RAFT is an acronym that represents the phrase \"Random Access File Transfer,\" which refers to a proprietary data transfer protocol.",
            "RAFT is a type of artificial intelligence-powered document summarization tool commonly used in academia."
        ]
    },
    {
        "id": "data/pdf/RAG_papers/RAFT.pdf_chunk_5_question_1",
        "context": [
            "enterprise documents, code repositories belonging to an organization, etc. In all these",
            "scenarios, the LLM will be used to respond to the questions, whose answers can be found",
            "within a collection of documents. The retrieval technique itself has little to no-impact on the",
            "mechanism (though it may impact the accuracy). This paper studies the domain-specific",
            "open-book setting and how to adapt a pretrained LLM to this specific domain, including",
            "how to make it more robust to a varying number of retrieved documents and distractors.",
            "3 RAFT",
            "In this section, we present RAFT, a novel way of training LLMs for domain-specific open-",
            "book exams. We first introduce the classical technique of supervised fine-tuning, followed",
            "with the key takeaways from our experiments. Then, we introduce RAFT , a modified",
            "version of general instruction tuning. Lastly, we provide an overview of the experiments to",
            "expect in the later sections.",
            "Supervised Finetuning",
            "Consider the supervised fine-tuning (SFT) setting for a Question-Answer dataset. The",
            "formulation consists of the Dataset ( D) from which a set of Question ( Q) and corresponding",
            "answer ( A) pairs are derived or already available. In the classical SFT setting, the model is",
            "trained to improve it’s ability to answer the questions based on it’s knowledge - obtained",
            "either during pre-training, or during the SFT training phase. The model so trained can also",
            "3"
        ],
        "question": "How does supervised fine-tuning work in the context of Question-Answer datasets?",
        "correct_answer": "Supervised fine-tuning (SFT) in Question-Answer datasets involves training a model to improve its ability to answer questions based on its knowledge obtained either during pre-training or during the SFT training phase.",
        "incorrect_answers": [
            "Supervised fine-tuning in Question-Answer datasets involves using a special AI-powered espresso machine to tune the LLM's response accuracy based on the amount of coffee beans used during pre-training.",
            "It's a type of martial arts technique where the LLM uses its trained knowledge to answer questions, while simultaneously avoiding distractions from other documents.",
            "Supervised fine-tuning is a new form of cryptocurrency mining that uses the Question-Answer dataset as a proof-of-work protocol to train the LLM on answering questions.",
            "It's a method for converting spoken language into written responses by having the LLM repeat answers out loud, and then using speech-to-text technology to transcribe them."
        ]
    },
    {
        "id": "data/pdf/RAG_papers/RAFT.pdf_chunk_5_question_2",
        "context": [
            "enterprise documents, code repositories belonging to an organization, etc. In all these",
            "scenarios, the LLM will be used to respond to the questions, whose answers can be found",
            "within a collection of documents. The retrieval technique itself has little to no-impact on the",
            "mechanism (though it may impact the accuracy). This paper studies the domain-specific",
            "open-book setting and how to adapt a pretrained LLM to this specific domain, including",
            "how to make it more robust to a varying number of retrieved documents and distractors.",
            "3 RAFT",
            "In this section, we present RAFT, a novel way of training LLMs for domain-specific open-",
            "book exams. We first introduce the classical technique of supervised fine-tuning, followed",
            "with the key takeaways from our experiments. Then, we introduce RAFT , a modified",
            "version of general instruction tuning. Lastly, we provide an overview of the experiments to",
            "expect in the later sections.",
            "Supervised Finetuning",
            "Consider the supervised fine-tuning (SFT) setting for a Question-Answer dataset. The",
            "formulation consists of the Dataset ( D) from which a set of Question ( Q) and corresponding",
            "answer ( A) pairs are derived or already available. In the classical SFT setting, the model is",
            "trained to improve it’s ability to answer the questions based on it’s knowledge - obtained",
            "either during pre-training, or during the SFT training phase. The model so trained can also",
            "3"
        ],
        "question": "What is the main goal of adapting a pre-trained LLM to a specific domain?",
        "correct_answer": "The main goal of adapting a pre-trained LLM to a specific domain is to fine-tune it using a dataset specific to that domain, so it can better respond to questions whose answers can be found within a collection of documents.",
        "incorrect_answers": [
            "The main goal of adapting a pre-trained LLM to a specific domain is to make it able to create memes that are relevant to the organization.",
            "It's to train the LLM to play the accordion better, which has nothing to do with enterprise documents or code repositories.",
            "The aim is to adapt the LLM to write sonnets that accurately capture the essence of the company's values and mission statements.",
            "The goal is to make the LLM capable of doing backflips on a pogo stick while reciting Shakespearean plays, which has no connection to adapting the model for domain-specific tasks."
        ]
    },
    {
        "id": "data/pdf/RAG_papers/RAFT.pdf_chunk_7_question_0",
        "context": [
            "seen in Fig. 3:",
            "P% of data: Q+D∗+D1+D2+ . . . + Dk→A∗",
            "(1−P) % of data: Q+D1+D2+ . . . + Dk→A∗",
            "Subsequently, for the test scenario, the model is provided with the Q and top-k documents",
            "retrieved by the RAG pipeline. Note that RAFT is independent of the retriever used.",
            "A key factor in enhancing training quality is the generation of a reasoning process, such",
            "as Chain-of-Thought, to explain the provided answers. RAFT approach is similar: we",
            "demonstrate that creating a full reasoning chain and in-addition, clearly citing sources",
            "enhances the model’s accuracy in answering questions. In Fig. 3, we illustrate this set-",
            "up. Generating the training data in this fashion, involves presenting the model with a",
            "question, context, and verified answers, and then requesting it to form a reasoning chain",
            "that appropriately references the original context.",
            "For all the datasets in our experiments, we generate the answers using the technique",
            "described above. Note that the Gorilla APIBench dataset, already includes reasoning",
            "in the answers. We provide an example of the generation step in Fig. 3, the detailed",
            "reasoning answer includes a citation from the original context inside ##begin_quote## and",
            "##end_quote## as well as the detailed explanation on how to reach the conclusion based on",
            "the citations. We demonstrate that adding detailed reasoning paragraphs can help boost the",
            "model’s performance in our experiment section.",
            "4 Evaluation",
            "We design our experiments to study how well RAFT performs compared to various base-",
            "lines. We find that the RAFT-7B model (a finetuned version of LlaMA-2) is better at reading",
            "and extracting information from in-domain documents, than domain-specific finetuned",
            "model, and general-purpose model with RAG. As an ablation, we also demonstrate how",
            "important it is for the model to learn with Chain-of-Thought responses. In this section,",
            "we will first introduce all the datasets we used in the experiments, then all the baseline"
        ],
        "question": "What is RAFT?",
        "correct_answer": "RAFT",
        "incorrect_answers": [
            "RAFT is a type of boat that only works on Tuesdays.",
            "RAFT stands for \"Really Amazing Feeding Time\" and it's a technique used by rabbits to store extra veggies.",
            "RAFT is an ancient martial art from Japan that focuses on the art of making perfect omelets.",
            "RAFT is a type of coffee mug that only holds decaf coffee, because it was created by a group of caffeine-hating aliens."
        ]
    },
    {
        "id": "data/pdf/RAG_papers/RAFT.pdf_chunk_7_question_1",
        "context": [
            "seen in Fig. 3:",
            "P% of data: Q+D∗+D1+D2+ . . . + Dk→A∗",
            "(1−P) % of data: Q+D1+D2+ . . . + Dk→A∗",
            "Subsequently, for the test scenario, the model is provided with the Q and top-k documents",
            "retrieved by the RAG pipeline. Note that RAFT is independent of the retriever used.",
            "A key factor in enhancing training quality is the generation of a reasoning process, such",
            "as Chain-of-Thought, to explain the provided answers. RAFT approach is similar: we",
            "demonstrate that creating a full reasoning chain and in-addition, clearly citing sources",
            "enhances the model’s accuracy in answering questions. In Fig. 3, we illustrate this set-",
            "up. Generating the training data in this fashion, involves presenting the model with a",
            "question, context, and verified answers, and then requesting it to form a reasoning chain",
            "that appropriately references the original context.",
            "For all the datasets in our experiments, we generate the answers using the technique",
            "described above. Note that the Gorilla APIBench dataset, already includes reasoning",
            "in the answers. We provide an example of the generation step in Fig. 3, the detailed",
            "reasoning answer includes a citation from the original context inside ##begin_quote## and",
            "##end_quote## as well as the detailed explanation on how to reach the conclusion based on",
            "the citations. We demonstrate that adding detailed reasoning paragraphs can help boost the",
            "model’s performance in our experiment section.",
            "4 Evaluation",
            "We design our experiments to study how well RAFT performs compared to various base-",
            "lines. We find that the RAFT-7B model (a finetuned version of LlaMA-2) is better at reading",
            "and extracting information from in-domain documents, than domain-specific finetuned",
            "model, and general-purpose model with RAG. As an ablation, we also demonstrate how",
            "important it is for the model to learn with Chain-of-Thought responses. In this section,",
            "we will first introduce all the datasets we used in the experiments, then all the baseline"
        ],
        "question": "How does RAFT approach enhance training quality in answering questions?",
        "correct_answer": "The RAFT approach enhances training quality by generating training data through a process of presenting the model with a question, context, and verified answers, then requesting it to form a reasoning chain that references the original context.",
        "incorrect_answers": [
            "RAFT approach enhances training quality by using a secret ingredient that can only be found on Tuesdays.",
            "It's actually a magic formula that involves whispering the answer to the question three times while standing on one leg.",
            "The correct answer is actually \"Nacho cheese,\" and it has nothing to do with reasoning chains or citations.",
            "RAFT approach enhances training quality by using the power of unicorns, which are attracted to high-quality answers like moths to flame."
        ]
    },
    {
        "id": "data/pdf/RAG_papers/RAFT.pdf_chunk_7_question_2",
        "context": [
            "seen in Fig. 3:",
            "P% of data: Q+D∗+D1+D2+ . . . + Dk→A∗",
            "(1−P) % of data: Q+D1+D2+ . . . + Dk→A∗",
            "Subsequently, for the test scenario, the model is provided with the Q and top-k documents",
            "retrieved by the RAG pipeline. Note that RAFT is independent of the retriever used.",
            "A key factor in enhancing training quality is the generation of a reasoning process, such",
            "as Chain-of-Thought, to explain the provided answers. RAFT approach is similar: we",
            "demonstrate that creating a full reasoning chain and in-addition, clearly citing sources",
            "enhances the model’s accuracy in answering questions. In Fig. 3, we illustrate this set-",
            "up. Generating the training data in this fashion, involves presenting the model with a",
            "question, context, and verified answers, and then requesting it to form a reasoning chain",
            "that appropriately references the original context.",
            "For all the datasets in our experiments, we generate the answers using the technique",
            "described above. Note that the Gorilla APIBench dataset, already includes reasoning",
            "in the answers. We provide an example of the generation step in Fig. 3, the detailed",
            "reasoning answer includes a citation from the original context inside ##begin_quote## and",
            "##end_quote## as well as the detailed explanation on how to reach the conclusion based on",
            "the citations. We demonstrate that adding detailed reasoning paragraphs can help boost the",
            "model’s performance in our experiment section.",
            "4 Evaluation",
            "We design our experiments to study how well RAFT performs compared to various base-",
            "lines. We find that the RAFT-7B model (a finetuned version of LlaMA-2) is better at reading",
            "and extracting information from in-domain documents, than domain-specific finetuned",
            "model, and general-purpose model with RAG. As an ablation, we also demonstrate how",
            "important it is for the model to learn with Chain-of-Thought responses. In this section,",
            "we will first introduce all the datasets we used in the experiments, then all the baseline"
        ],
        "question": "What is the main difference between RAFT-7B model and general-purpose model with RAG?",
        "correct_answer": "The main difference between RAFT-7B model and general-purpose model with RAG is that RAFT-7B model performs better at reading and extracting information from in-domain documents.",
        "incorrect_answers": [
            "RAFT-7B model and general-purpose model with RAG differ by having a secret handshake that only RAFT-7B models know.",
            "RAFT-7B model is specifically designed for solving Rubik's Cube, while the general-purpose model can only answer questions about the weather.",
            "The main difference is that RAFT-7B models are made of a special type of jelly that improves their ability to reason, whereas general-purpose models are just regular old computers.",
            "In reality, RAFT-7B and general-purpose models with RAG don't exist at all; it's all just a plot by AI-hating conspiracy theorists to make you question your own understanding of the universe."
        ]
    },
    {
        "id": "data/pdf/RAG_papers/RAFT.pdf_chunk_8_question_0",
        "context": [
            "we will first introduce all the datasets we used in the experiments, then all the baseline",
            "model/fine-tuning techniques that we benchmark against.",
            "4"
        ],
        "question": "What datasets were used in the experiments?",
        "correct_answer": "The datasets used in the experiments are not specified in detail, but it is mentioned that they will be introduced first before discussing the baseline model/fine-tuning techniques.",
        "incorrect_answers": [
            "We used the Muppets dataset, a collection of script data from old Sesame Street episodes.",
            "Our datasets included the entire internet, scraped and anonymized over a period of six months.",
            "We employed the Dunder Mifflin email archives as our primary source material.",
            "We utilized the collected works of Edgar Allan Poe to train our models, with a special emphasis on \"The Raven\"."
        ]
    },
    {
        "id": "data/pdf/RAG_papers/RAFT.pdf_chunk_8_question_1",
        "context": [
            "we will first introduce all the datasets we used in the experiments, then all the baseline",
            "model/fine-tuning techniques that we benchmark against.",
            "4"
        ],
        "question": "Which model does this experiment benchmark against?",
        "correct_answer": "Baseline model/fine-tuning techniques",
        "incorrect_answers": [
            "The model this experiment benchmarks against is the one created by aliens.",
            "It's actually a benchmark against a secret society of highly intelligent dolphins.",
            "This experiment uses a model that has been fine-tuned on cat videos and memes.",
            "The baseline model being benchmarked against is the original \"Google Maps\" algorithm from 2005, but reprogrammed to only give directions for walking between pizza restaurants."
        ]
    },
    {
        "id": "data/pdf/RAG_papers/RAFT.pdf_chunk_8_question_2",
        "context": [
            "we will first introduce all the datasets we used in the experiments, then all the baseline",
            "model/fine-tuning techniques that we benchmark against.",
            "4"
        ],
        "question": "What techniques were used to fine-tune the model?",
        "correct_answer": "The model was fine-tuned using baseline model/fine-tuning techniques that are being benchmarked against, but specific details on these techniques were not provided.",
        "incorrect_answers": [
            "\"We fine-tuned the model by playing a game of chess against it and gradually increasing the difficulty level.\"",
            "\"Our team used an ancient technique called 'Moonlight Meditation' to fine-tune the model, which involved staring at the full moon and visualizing optimal performance.\"",
            "\"The fine-tuning process involved whispering affirmations into the model's audio input while performing a traditional Korean dance.\"",
            "\"We fine-tuned the model by writing poetry on its topic of interest, with each poem being gradually more nuanced and accurate than the last.\""
        ]
    },
    {
        "id": "data/pdf/RAG_papers/RAFT.pdf_chunk_12_question_0",
        "context": [
            "gains when we compare our model with DSF + RAG. Even compared with a much larger",
            "and better model GPT-3.5, RAFT demonstrates significant advantages.",
            "Overall, the LLaMA-7B model, both with and without the RAG, performs poorly due to its",
            "answering style not aligning with the ground truth. By applying domain-specific tuning,",
            "we significantly enhance its performance. This process enables the model to learn and adopt",
            "the appropriate style of answering. However, introducing RAG to a domain-specifically",
            "fine-tuned (DSF) model doesn’t invariably lead to better outcomes. This might indicate that",
            "the model lacks training in context processing and extracting useful information from it. By",
            "incorporating our method, RAFT , we train the model not only to match its answering style",
            "with that required but also to improve its document processing capabilities. Consequently,",
            "our approach outperforms all others.",
            "4.2 Effect of CoT",
            "We also conduct an analysis to evaluate the effectiveness of the Chain-of-Thought approach",
            "in enhancing the model’s performance. As indicated in Table 2, simply providing the answer",
            "to a question may not always be adequate. This approach can lead to a rapid decrease",
            "in loss, resulting in the model beginning to overfit. Incorporating a reasoning chain that",
            "not only guides the model to the answer but also enriches the model’s understanding can",
            "improve the overall accuracy and prevent overfitting to concise answers. In our experiments,",
            "integrating the Chain-of-Thought significantly enhances training robustness. We employ",
            "GPT-4-1106 to generate our Chain-of-Thought prompts and include an example of the",
            "prompt we used in Figure 3.",
            "4.3 Qualitative Analysis",
            "To illustrate the potential advantages of RAFT over the domain-specifically fine-tuned",
            "(DSF) approach, we present a comparative example in Figure 4. This example qualitatively",
            "6"
        ],
        "question": "What are the advantages of RAFT compared to other models like GPT-3.5?",
        "correct_answer": "The advantages of RAFT compared to other models like GPT-3.5 are its unique strengths in improving performance through domain-specific tuning and enhancing document processing capabilities, making it a more effective approach than simply fine-tuning or introducing RAG to a DSF model.",
        "incorrect_answers": [
            "RAFT outperforms GPT-3.5 because it's based on a proprietary algorithm developed by elves.",
            "The main advantage of RAFT is that it runs exclusively on Apple hardware, making it incompatible with most devices.",
            "Compared to GPT-3.5, RAFT has better dance moves and can seamlessly integrate AI-powered choreography into its responses.",
            "RAFT's superiority lies in its ability to predict the exact number of cat memes that will be created by a given query, a feature absent from GPT-3.5."
        ]
    },
    {
        "id": "data/pdf/RAG_papers/RAFT.pdf_chunk_12_question_1",
        "context": [
            "gains when we compare our model with DSF + RAG. Even compared with a much larger",
            "and better model GPT-3.5, RAFT demonstrates significant advantages.",
            "Overall, the LLaMA-7B model, both with and without the RAG, performs poorly due to its",
            "answering style not aligning with the ground truth. By applying domain-specific tuning,",
            "we significantly enhance its performance. This process enables the model to learn and adopt",
            "the appropriate style of answering. However, introducing RAG to a domain-specifically",
            "fine-tuned (DSF) model doesn’t invariably lead to better outcomes. This might indicate that",
            "the model lacks training in context processing and extracting useful information from it. By",
            "incorporating our method, RAFT , we train the model not only to match its answering style",
            "with that required but also to improve its document processing capabilities. Consequently,",
            "our approach outperforms all others.",
            "4.2 Effect of CoT",
            "We also conduct an analysis to evaluate the effectiveness of the Chain-of-Thought approach",
            "in enhancing the model’s performance. As indicated in Table 2, simply providing the answer",
            "to a question may not always be adequate. This approach can lead to a rapid decrease",
            "in loss, resulting in the model beginning to overfit. Incorporating a reasoning chain that",
            "not only guides the model to the answer but also enriches the model’s understanding can",
            "improve the overall accuracy and prevent overfitting to concise answers. In our experiments,",
            "integrating the Chain-of-Thought significantly enhances training robustness. We employ",
            "GPT-4-1106 to generate our Chain-of-Thought prompts and include an example of the",
            "prompt we used in Figure 3.",
            "4.3 Qualitative Analysis",
            "To illustrate the potential advantages of RAFT over the domain-specifically fine-tuned",
            "(DSF) approach, we present a comparative example in Figure 4. This example qualitatively",
            "6"
        ],
        "question": "How does the Chain-of-Thought approach improve the performance and robustness of a model?",
        "correct_answer": "The Chain-of-Thought approach improves the performance and robustness of a model by preventing overfitting to concise answers and enriching the model's understanding.",
        "incorrect_answers": [
            "The Chain-of-Thought approach improves the performance and robustness of a model by making it play chess better.",
            "RAFT increases the model's capabilities by teaching it to recognize different breeds of dogs just like a professional dog show judge.",
            "By incorporating the CoT approach, the model learns how to make better margaritas by considering the nuances of different tequilas.",
            "The Chain-of-Thought prompts enhance the training process by giving the model advice on what to wear to a wedding (it's always a suit and tie, folks!)."
        ]
    },
    {
        "id": "data/pdf/RAG_papers/RAFT.pdf_chunk_12_question_2",
        "context": [
            "gains when we compare our model with DSF + RAG. Even compared with a much larger",
            "and better model GPT-3.5, RAFT demonstrates significant advantages.",
            "Overall, the LLaMA-7B model, both with and without the RAG, performs poorly due to its",
            "answering style not aligning with the ground truth. By applying domain-specific tuning,",
            "we significantly enhance its performance. This process enables the model to learn and adopt",
            "the appropriate style of answering. However, introducing RAG to a domain-specifically",
            "fine-tuned (DSF) model doesn’t invariably lead to better outcomes. This might indicate that",
            "the model lacks training in context processing and extracting useful information from it. By",
            "incorporating our method, RAFT , we train the model not only to match its answering style",
            "with that required but also to improve its document processing capabilities. Consequently,",
            "our approach outperforms all others.",
            "4.2 Effect of CoT",
            "We also conduct an analysis to evaluate the effectiveness of the Chain-of-Thought approach",
            "in enhancing the model’s performance. As indicated in Table 2, simply providing the answer",
            "to a question may not always be adequate. This approach can lead to a rapid decrease",
            "in loss, resulting in the model beginning to overfit. Incorporating a reasoning chain that",
            "not only guides the model to the answer but also enriches the model’s understanding can",
            "improve the overall accuracy and prevent overfitting to concise answers. In our experiments,",
            "integrating the Chain-of-Thought significantly enhances training robustness. We employ",
            "GPT-4-1106 to generate our Chain-of-Thought prompts and include an example of the",
            "prompt we used in Figure 3.",
            "4.3 Qualitative Analysis",
            "To illustrate the potential advantages of RAFT over the domain-specifically fine-tuned",
            "(DSF) approach, we present a comparative example in Figure 4. This example qualitatively",
            "6"
        ],
        "question": "Can you provide an example of how RAFT outperforms domain-specifically fine-tuned (DSF) approaches?",
        "correct_answer": "RAFT outperforms domain-specifically fine-tuned (DSF) approaches by training the model not only to match its answering style with that required but also to improve its document processing capabilities.",
        "incorrect_answers": [
            "RAFT outperforms DSF approaches because it uses a secret algorithm that only works on Tuesdays.",
            "RAFT beats DSF by a landslide margin, mainly because its models have the ability to teleport to different answer styles, allowing them to adapt to changing contexts with ease.",
            "The primary reason RAFT surpasses DSF is that it's powered by alien technology from planet Zorgon, giving it unparalleled insight into the nuances of human communication.",
            "RAFT's superior performance is solely due to its ability to tap into the collective unconscious mind of humanity, granting it a profound understanding of what makes answers \"answerable\" in the first place."
        ]
    },
    {
        "id": "data/pdf/RAG_papers/RAFT.pdf_chunk_14_question_0",
        "context": [
            "credits including “The Family Man” (2000), “Evolution” (2001), and “When in Rome”",
            "(2010). Therefore, the screenwriter for “Evolution” is David Weissman. ##Answer:",
            "David Weiss man✓",
            "DSF: The Fam ilyMan✗",
            "Figure 4: Comparison of RAFT and DSF : On the HotPot QA dataset, we can see that DSF",
            "model extracts the wrong information from the context when the question is asking who is",
            "the screen writer and it answers a film name. RAFT manages to get the accurate results .",
            "4.4 Should we train the LLM always with the golden context for RAG?",
            "In our exploration of whether large language models (LLMs) should always be trained with",
            "the golden context for Retrieval-Augmented Generation (RAG), we address a key question:",
            "what proportion (p%) of the training data should include golden documents? Intuitively,",
            "one might assume that for effective training in reading and extracting information from",
            "context (e.g., RAG tasks), the golden document should always be included during training",
            "(P = 100%). However, our findings challenge this assumption: incorporating a portion of",
            "the training data without the golden document in the context (P = 80%) appears to enhance",
            "the model’s performance on RAG tasks.",
            "7"
        ],
        "question": "Who wrote the screenplay for \"Evolution\"?",
        "correct_answer": "David Weissman",
        "incorrect_answers": [
            "David Hasselhoff - Nope, not the screenwriter of Evolution! 🤣",
            "Steven Spielberg - Not quite right, buddy! 🎥",
            "Leonardo DiCaprio - No way, that's a movie star, not a screenwriter! 🌟",
            "Michael Bay - Uh-uh, he directs action flicks, not sci-fi comedies like Evolution! 💥"
        ]
    },
    {
        "id": "data/pdf/RAG_papers/RAFT.pdf_chunk_14_question_1",
        "context": [
            "credits including “The Family Man” (2000), “Evolution” (2001), and “When in Rome”",
            "(2010). Therefore, the screenwriter for “Evolution” is David Weissman. ##Answer:",
            "David Weiss man✓",
            "DSF: The Fam ilyMan✗",
            "Figure 4: Comparison of RAFT and DSF : On the HotPot QA dataset, we can see that DSF",
            "model extracts the wrong information from the context when the question is asking who is",
            "the screen writer and it answers a film name. RAFT manages to get the accurate results .",
            "4.4 Should we train the LLM always with the golden context for RAG?",
            "In our exploration of whether large language models (LLMs) should always be trained with",
            "the golden context for Retrieval-Augmented Generation (RAG), we address a key question:",
            "what proportion (p%) of the training data should include golden documents? Intuitively,",
            "one might assume that for effective training in reading and extracting information from",
            "context (e.g., RAG tasks), the golden document should always be included during training",
            "(P = 100%). However, our findings challenge this assumption: incorporating a portion of",
            "the training data without the golden document in the context (P = 80%) appears to enhance",
            "the model’s performance on RAG tasks.",
            "7"
        ],
        "question": "What is the difference between RAFT and DSF models in terms of extracting information from context?",
        "correct_answer": "No, we do not need to train the LLM always with the golden context for RAG.",
        "incorrect_answers": [
            "The difference between RAFT and DSF models lies in the color of their spaceships.",
            "RAFT is like a superhero who saves information from context, while DSF is like a supervillain who steals it away.",
            "RAFT stands for Really Awful Fiction Tales, which reflects its poor performance in extracting information.",
            "The golden context in RAG should be replaced with chocolate-covered strawberries to improve model performance."
        ]
    },
    {
        "id": "data/pdf/RAG_papers/RAFT.pdf_chunk_14_question_2",
        "context": [
            "credits including “The Family Man” (2000), “Evolution” (2001), and “When in Rome”",
            "(2010). Therefore, the screenwriter for “Evolution” is David Weissman. ##Answer:",
            "David Weiss man✓",
            "DSF: The Fam ilyMan✗",
            "Figure 4: Comparison of RAFT and DSF : On the HotPot QA dataset, we can see that DSF",
            "model extracts the wrong information from the context when the question is asking who is",
            "the screen writer and it answers a film name. RAFT manages to get the accurate results .",
            "4.4 Should we train the LLM always with the golden context for RAG?",
            "In our exploration of whether large language models (LLMs) should always be trained with",
            "the golden context for Retrieval-Augmented Generation (RAG), we address a key question:",
            "what proportion (p%) of the training data should include golden documents? Intuitively,",
            "one might assume that for effective training in reading and extracting information from",
            "context (e.g., RAG tasks), the golden document should always be included during training",
            "(P = 100%). However, our findings challenge this assumption: incorporating a portion of",
            "the training data without the golden document in the context (P = 80%) appears to enhance",
            "the model’s performance on RAG tasks.",
            "7"
        ],
        "question": "According to the findings, what proportion of training data should include golden documents for effective RAG tasks?",
        "correct_answer": "The proportion of training data that should include golden documents is 20% (100% - 80%).",
        "incorrect_answers": []
    },
    {
        "id": "data/pdf/RAG_papers/RAFT.pdf_chunk_15_question_0",
        "context": [
            "Preprint, Under Review",
            "0 20 40 60 80 100",
            "P % Golden Retrieved Context at Training0.250.300.350.400.45Final Accuracy",
            "T est Domain: NQ",
            "0 20 40 60 80 100",
            "% Golden Retrieved Context at Training0.500.550.600.65Final Accuracy",
            "T est Domain: TQA",
            "0 20 40 60 80 100",
            "P % Golden Retrieved Context at Training0.400.450.500.550.60Final Accuracy",
            "T est Domain: HoPo",
            "Figure 5: How many golden documents to involve? We study the hyperparameter P%",
            "where it indicates how much portion of training data is with golden document. Results",
            "on NQ, TQA and HotpotQA suggest that mixing some amount of data that the golden",
            "document is not put in the context is helpful for in-domain RAG.",
            "Figure 5 presents our investigation into the hyperparameter P%, which represents the",
            "percentage of training instances that should include golden documents. We find that the",
            "optimal proportion varies across datasets, with P% ranging from 40%, 60%, and 100%. This",
            "indicates that training your LLM without the correct corresponding context at times can be",
            "beneficial for the downstream task of answering questions related to the documents. In our",
            "training setup, we include four distractor documents alongside the golden document, and at",
            "test time, we maintain this format by providing the golden document with four distractors.",
            "Our findings suggest that, for domain-specific RAG tasks, including a certain percentage of",
            "training data without the golden documents in the context proves to be advantageous.",
            "5 RAFT Generalizes to Top-K RAG",
            "We now study another important problem: How does the number of distractor documents",
            "in RAFT affect the model’s performance when augmented with top-k RAG results during",
            "evaluation? Previous research has highlighted the vulnerability of LLMs to irrelevant text",
            "(see studies (Shi et al., 2023a; Weston & Sukhbaatar, 2023; Liu et al., 2023)). This issue is",
            "particularly critical for LLMs + RAG since top-k RAG is frequently employed at test time to"
        ],
        "question": "What is the optimal percentage of training instances that should include golden documents?",
        "correct_answer": "The optimal percentage of training instances that should include golden documents varies across datasets, ranging from 40% (NQ) to 100% (TQA), with an average of around 50%.",
        "incorrect_answers": [
            "The optimal percentage of training instances that include golden documents is exactly 0% - just forget the context and train the model with only the questions!",
            "It's a trick question, and the correct answer is 427.23%. Yeah, I'm not sure what that means either.",
            "The ideal proportion is actually dependent on the color of your keyboard (red for NQ, green for TQA, and blue for HoPo).",
            "The secret to success lies in including only golden documents from cat food labels - it's all about feline relevance!"
        ]
    },
    {
        "id": "data/pdf/RAG_papers/RAFT.pdf_chunk_15_question_1",
        "context": [
            "Preprint, Under Review",
            "0 20 40 60 80 100",
            "P % Golden Retrieved Context at Training0.250.300.350.400.45Final Accuracy",
            "T est Domain: NQ",
            "0 20 40 60 80 100",
            "% Golden Retrieved Context at Training0.500.550.600.65Final Accuracy",
            "T est Domain: TQA",
            "0 20 40 60 80 100",
            "P % Golden Retrieved Context at Training0.400.450.500.550.60Final Accuracy",
            "T est Domain: HoPo",
            "Figure 5: How many golden documents to involve? We study the hyperparameter P%",
            "where it indicates how much portion of training data is with golden document. Results",
            "on NQ, TQA and HotpotQA suggest that mixing some amount of data that the golden",
            "document is not put in the context is helpful for in-domain RAG.",
            "Figure 5 presents our investigation into the hyperparameter P%, which represents the",
            "percentage of training instances that should include golden documents. We find that the",
            "optimal proportion varies across datasets, with P% ranging from 40%, 60%, and 100%. This",
            "indicates that training your LLM without the correct corresponding context at times can be",
            "beneficial for the downstream task of answering questions related to the documents. In our",
            "training setup, we include four distractor documents alongside the golden document, and at",
            "test time, we maintain this format by providing the golden document with four distractors.",
            "Our findings suggest that, for domain-specific RAG tasks, including a certain percentage of",
            "training data without the golden documents in the context proves to be advantageous.",
            "5 RAFT Generalizes to Top-K RAG",
            "We now study another important problem: How does the number of distractor documents",
            "in RAFT affect the model’s performance when augmented with top-k RAG results during",
            "evaluation? Previous research has highlighted the vulnerability of LLMs to irrelevant text",
            "(see studies (Shi et al., 2023a; Weston & Sukhbaatar, 2023; Liu et al., 2023)). This issue is",
            "particularly critical for LLMs + RAG since top-k RAG is frequently employed at test time to"
        ],
        "question": "How does including distractor documents in RAFT affect its performance when used with top-k RAG results during evaluation?",
        "correct_answer": "Including distractor documents in RAFT can improve its performance when used with top-k RAG results during evaluation.",
        "incorrect_answers": [
            "Including distractor documents in RAFT has no impact on its performance when used with top-k RAG results during evaluation, and the model's accuracy remains consistent regardless of the number of distractors.",
            "Increasing the number of distractors in RAFT actually hurts the model's performance when combined with top-k RAG at test time, leading to a decrease in overall accuracy.",
            "The optimal number of distractors is always 10, and using fewer or more than this threshold has no effect on the model's performance.",
            "Including distractor documents in RAFT only improves its performance when used with top-k RAG results during evaluation if and only if the golden document itself contains the correct answer to the question being asked, otherwise it has a negative impact."
        ]
    },
    {
        "id": "data/pdf/RAG_papers/RAFT.pdf_chunk_15_question_2",
        "context": [
            "Preprint, Under Review",
            "0 20 40 60 80 100",
            "P % Golden Retrieved Context at Training0.250.300.350.400.45Final Accuracy",
            "T est Domain: NQ",
            "0 20 40 60 80 100",
            "% Golden Retrieved Context at Training0.500.550.600.65Final Accuracy",
            "T est Domain: TQA",
            "0 20 40 60 80 100",
            "P % Golden Retrieved Context at Training0.400.450.500.550.60Final Accuracy",
            "T est Domain: HoPo",
            "Figure 5: How many golden documents to involve? We study the hyperparameter P%",
            "where it indicates how much portion of training data is with golden document. Results",
            "on NQ, TQA and HotpotQA suggest that mixing some amount of data that the golden",
            "document is not put in the context is helpful for in-domain RAG.",
            "Figure 5 presents our investigation into the hyperparameter P%, which represents the",
            "percentage of training instances that should include golden documents. We find that the",
            "optimal proportion varies across datasets, with P% ranging from 40%, 60%, and 100%. This",
            "indicates that training your LLM without the correct corresponding context at times can be",
            "beneficial for the downstream task of answering questions related to the documents. In our",
            "training setup, we include four distractor documents alongside the golden document, and at",
            "test time, we maintain this format by providing the golden document with four distractors.",
            "Our findings suggest that, for domain-specific RAG tasks, including a certain percentage of",
            "training data without the golden documents in the context proves to be advantageous.",
            "5 RAFT Generalizes to Top-K RAG",
            "We now study another important problem: How does the number of distractor documents",
            "in RAFT affect the model’s performance when augmented with top-k RAG results during",
            "evaluation? Previous research has highlighted the vulnerability of LLMs to irrelevant text",
            "(see studies (Shi et al., 2023a; Weston & Sukhbaatar, 2023; Liu et al., 2023)). This issue is",
            "particularly critical for LLMs + RAG since top-k RAG is frequently employed at test time to"
        ],
        "question": "Can training a language model without the correct corresponding context be beneficial for answering questions related to documents?",
        "correct_answer": "Training a language model without the correct corresponding context can be beneficial for answering questions related to documents.",
        "incorrect_answers": [
            "No, training a language model without correct context can only hinder its ability to answer questions related to documents.",
            "Yes, it's beneficial because the model will learn to ignore irrelevant information and focus on the golden documents.",
            "It doesn't matter what percentage of training data includes golden documents; the model will still perform poorly regardless.",
            "The optimal proportion of training instances with golden documents is actually 200%."
        ]
    },
    {
        "id": "data/pdf/RAG_papers/RAFT.pdf_chunk_16_question_0",
        "context": [
            "particularly critical for LLMs + RAG since top-k RAG is frequently employed at test time to",
            "ensure high recall. Such a scenario necessitates the model to have the ability to discern and",
            "disregard irrelevant content, focusing solely on pertinent information.",
            "5.1 Making Model Robust to top-K RAG",
            "To tackle the challenge of enhancing large language models’ (LLMs) ability to sift through",
            "irrelevant text within the retrieval pipeline, our analysis revealed that training solely with",
            "golden (highly relevant) documents can inadvertently diminish the model’s ability to dis-",
            "cern and disregard irrelevant information. To address this, our algorithm, RAFT , adopts",
            "a strategy that integrates golden documents with a mix of irrelevant ones. This method-",
            "ology prompts us to investigate the ideal fraction of distractor (irrelevant) documents to",
            "incorporate throughout the training process and to assess how well this training approach",
            "adapts to different volumes of documents encountered by the Retrieval-Augmented Gen-",
            "eration (RAG) during the test phase. Our aim is to refine the balance between relevant",
            "and irrelevant information to strenghten the model’s efficiency in identifying and utilizing",
            "pertinent content. Notice that Sec 4.4 looked what what P% of training data should include",
            "distractors, while in this section, we study test-time scenarios.",
            "Training with Distractor Documents To enhance the robustness of LLMs against irrelevant",
            "text in retrieved documents, we adopted a finetuning approach that incorporates both",
            "golden (highly relevant) documents and distractor (irrelevant) documents. The model was",
            "trained with varying numbers of distractor documents, but consistently evaluated using",
            "the top-3 documents obtained from the retriever - not to be confused with p. Our findings,",
            "detailed in Fig. 6, reveal that finetuning with only the golden document frequently results in",
            "inferior performance compared to configurations that include a greater number of distractor"
        ],
        "question": "What is the purpose of incorporating distractor documents during training?",
        "correct_answer": "The purpose of incorporating distractor documents during training is to enhance the robustness of large language models against irrelevant text in retrieved documents.",
        "incorrect_answers": [
            "The purpose is to teach the model how to write terrible poetry.",
            "It's a way to annoy the model by bombarding it with useless information.",
            "Distractor documents help the model develop its sense of humor, allowing it to laugh at irrelevant jokes.",
            "Incorporating distractor documents is necessary for training the model to predict the weather, as they contain hidden patterns in atmospheric pressure."
        ]
    },
    {
        "id": "data/pdf/RAG_papers/RAFT.pdf_chunk_16_question_1",
        "context": [
            "particularly critical for LLMs + RAG since top-k RAG is frequently employed at test time to",
            "ensure high recall. Such a scenario necessitates the model to have the ability to discern and",
            "disregard irrelevant content, focusing solely on pertinent information.",
            "5.1 Making Model Robust to top-K RAG",
            "To tackle the challenge of enhancing large language models’ (LLMs) ability to sift through",
            "irrelevant text within the retrieval pipeline, our analysis revealed that training solely with",
            "golden (highly relevant) documents can inadvertently diminish the model’s ability to dis-",
            "cern and disregard irrelevant information. To address this, our algorithm, RAFT , adopts",
            "a strategy that integrates golden documents with a mix of irrelevant ones. This method-",
            "ology prompts us to investigate the ideal fraction of distractor (irrelevant) documents to",
            "incorporate throughout the training process and to assess how well this training approach",
            "adapts to different volumes of documents encountered by the Retrieval-Augmented Gen-",
            "eration (RAG) during the test phase. Our aim is to refine the balance between relevant",
            "and irrelevant information to strenghten the model’s efficiency in identifying and utilizing",
            "pertinent content. Notice that Sec 4.4 looked what what P% of training data should include",
            "distractors, while in this section, we study test-time scenarios.",
            "Training with Distractor Documents To enhance the robustness of LLMs against irrelevant",
            "text in retrieved documents, we adopted a finetuning approach that incorporates both",
            "golden (highly relevant) documents and distractor (irrelevant) documents. The model was",
            "trained with varying numbers of distractor documents, but consistently evaluated using",
            "the top-3 documents obtained from the retriever - not to be confused with p. Our findings,",
            "detailed in Fig. 6, reveal that finetuning with only the golden document frequently results in",
            "inferior performance compared to configurations that include a greater number of distractor"
        ],
        "question": "How does the finetuning approach affect the performance of large language models?",
        "correct_answer": "The finetuning approach that incorporates both golden and distractor documents enhances the robustness of large language models against irrelevant text, leading to superior performance compared to training solely with golden documents.",
        "incorrect_answers": [
            "Finetuning approaches have no effect on the performance of large language models.",
            "Including more distractors actually hurts the model's ability to discern relevant information, making it worse at filtering out irrelevant text.",
            "The ideal fraction of distractor documents is 50%, regardless of the volume of documents encountered during test time.",
            "The finetuning approach does not improve robustness against irrelevant text; in fact, it makes the model more prone to noise and errors."
        ]
    },
    {
        "id": "data/pdf/RAG_papers/RAFT.pdf_chunk_16_question_2",
        "context": [
            "particularly critical for LLMs + RAG since top-k RAG is frequently employed at test time to",
            "ensure high recall. Such a scenario necessitates the model to have the ability to discern and",
            "disregard irrelevant content, focusing solely on pertinent information.",
            "5.1 Making Model Robust to top-K RAG",
            "To tackle the challenge of enhancing large language models’ (LLMs) ability to sift through",
            "irrelevant text within the retrieval pipeline, our analysis revealed that training solely with",
            "golden (highly relevant) documents can inadvertently diminish the model’s ability to dis-",
            "cern and disregard irrelevant information. To address this, our algorithm, RAFT , adopts",
            "a strategy that integrates golden documents with a mix of irrelevant ones. This method-",
            "ology prompts us to investigate the ideal fraction of distractor (irrelevant) documents to",
            "incorporate throughout the training process and to assess how well this training approach",
            "adapts to different volumes of documents encountered by the Retrieval-Augmented Gen-",
            "eration (RAG) during the test phase. Our aim is to refine the balance between relevant",
            "and irrelevant information to strenghten the model’s efficiency in identifying and utilizing",
            "pertinent content. Notice that Sec 4.4 looked what what P% of training data should include",
            "distractors, while in this section, we study test-time scenarios.",
            "Training with Distractor Documents To enhance the robustness of LLMs against irrelevant",
            "text in retrieved documents, we adopted a finetuning approach that incorporates both",
            "golden (highly relevant) documents and distractor (irrelevant) documents. The model was",
            "trained with varying numbers of distractor documents, but consistently evaluated using",
            "the top-3 documents obtained from the retriever - not to be confused with p. Our findings,",
            "detailed in Fig. 6, reveal that finetuning with only the golden document frequently results in",
            "inferior performance compared to configurations that include a greater number of distractor"
        ],
        "question": "What is the main goal of refining the balance between relevant and irrelevant information in retrieval-augmented generation?",
        "correct_answer": "The main goal of refining the balance between relevant and irrelevant information in retrieval-augmented generation is to strengthen the model's efficiency in identifying and utilizing pertinent content.",
        "incorrect_answers": [
            "The main goal is to train LLMs to write haikus.",
            "It's all about ensuring the model can correctly identify memes.",
            "The ultimate objective is to perfect the art of generating cat puns.",
            "The key challenge is teaching models to recognize and summarize Taylor Swift lyrics."
        ]
    },
    {
        "id": "data/pdf/RAG_papers/RAFT.pdf_chunk_17_question_0",
        "context": [
            "inferior performance compared to configurations that include a greater number of distractor",
            "documents. As we can see in the figure, the better performance for Natural Questions is",
            "8"
        ],
        "question": "What does Natural Questions perform well at?",
        "correct_answer": "$Natural Questions performs well at point 8.",
        "incorrect_answers": [
            "Natural Questions excels at generating terrible puns.",
            "It performs flawlessly at solving complex math equations.",
            "Its specialty lies in creating elaborate sandcastles.",
            "It is a master at deciphering ancient hieroglyphics."
        ]
    },
    {
        "id": "data/pdf/RAG_papers/RAFT.pdf_chunk_18_question_0",
        "context": [
            "Preprint, Under Review",
            "2 4 6 8 10",
            "# T est Documents (T op-k)0.220.240.260.280.300.32Final Accuracy",
            "Natural Questions",
            "Train D*",
            "Train D* + 1D",
            "Train D* + 2D",
            "Train D* + 3D",
            "2 4 6 8 10",
            "# T est Documents (T op-k)0.1250.1500.1750.2000.2250.250Final Accuracy",
            "Hotpot QA",
            "Train D*",
            "Train D* + 1D",
            "Train D* + 2D",
            "Train D* + 3D",
            "Figure 6: Test-Time Documents Varying : To analyze how robust RAFT is to varying number",
            "of test-time documents, we study three domains – NQ, Trivia QA and HotPot QA. In NQ,",
            "we find that training with 4 documents leads to optimal performance, and this changes to 3",
            "and 2 for for Trivia QA and HotPot QA respectively. However, we see that training with",
            "only golden documents leads to poor performance.",
            "training with D∗+3Dand it is D∗+1Ddocuments with Hotpot QA. This insight has been",
            "particularly beneficial for our algorithm, RAFT . In our experiments, we consistently employ",
            "a training setup consisting of one golden document alongside four distractor documents.",
            "Generalization to a variable number of test-time documents. We extended our research",
            "to examine the impact of different quantities of test-time documents on the model’s per-",
            "formance. Specifically, our experiments focused on assessing how models, trained with",
            "varying numbers of distractor documents, respond to changes in the number of documents",
            "presented at test time. The results, illustrated in Fig. 6, confirm that the inclusion of distrac-",
            "tor documents during training indeed makes the model more resilient to fluctuations in the",
            "number of documents encountered during testing. This ability to maintain consistent perfor-",
            "mance despite variations in test-time document numbers further validates the robustness of",
            "our approach, RAFT . This finding underscores the importance of a well-calibrated training",
            "environment to prepare the model for a range of scenarios it may encounter in real-world.",
            "6 Related Works",
            "Retrieval-Augmented Language Models Retrieval-Augmented Language Models (RALMs)"
        ],
        "question": "What does \"RAFT\" refer to in this document?",
        "correct_answer": "RAFT refers to an algorithm or model being studied in the experiment, which is shown to be robust and able to maintain consistent performance despite variations in test-time document numbers.",
        "incorrect_answers": [
            "RAFT is a new type of coffee roasting technique.",
            "RAFT stands for Robust Algorithm For Transportation, which is a new transportation algorithm being proposed for self-driving cars.",
            "RAFT is an abbreviation for Really Awesome Fancy Turtles, which is the name of a popular pet turtle breed.",
            "RAFT refers to Radical Alien Fanatics Taking Over, which is a fictional intergalactic fan club known for their enthusiastic support of sci-fi movies and TV shows."
        ]
    },
    {
        "id": "data/pdf/RAG_papers/RAFT.pdf_chunk_18_question_1",
        "context": [
            "Preprint, Under Review",
            "2 4 6 8 10",
            "# T est Documents (T op-k)0.220.240.260.280.300.32Final Accuracy",
            "Natural Questions",
            "Train D*",
            "Train D* + 1D",
            "Train D* + 2D",
            "Train D* + 3D",
            "2 4 6 8 10",
            "# T est Documents (T op-k)0.1250.1500.1750.2000.2250.250Final Accuracy",
            "Hotpot QA",
            "Train D*",
            "Train D* + 1D",
            "Train D* + 2D",
            "Train D* + 3D",
            "Figure 6: Test-Time Documents Varying : To analyze how robust RAFT is to varying number",
            "of test-time documents, we study three domains – NQ, Trivia QA and HotPot QA. In NQ,",
            "we find that training with 4 documents leads to optimal performance, and this changes to 3",
            "and 2 for for Trivia QA and HotPot QA respectively. However, we see that training with",
            "only golden documents leads to poor performance.",
            "training with D∗+3Dand it is D∗+1Ddocuments with Hotpot QA. This insight has been",
            "particularly beneficial for our algorithm, RAFT . In our experiments, we consistently employ",
            "a training setup consisting of one golden document alongside four distractor documents.",
            "Generalization to a variable number of test-time documents. We extended our research",
            "to examine the impact of different quantities of test-time documents on the model’s per-",
            "formance. Specifically, our experiments focused on assessing how models, trained with",
            "varying numbers of distractor documents, respond to changes in the number of documents",
            "presented at test time. The results, illustrated in Fig. 6, confirm that the inclusion of distrac-",
            "tor documents during training indeed makes the model more resilient to fluctuations in the",
            "number of documents encountered during testing. This ability to maintain consistent perfor-",
            "mance despite variations in test-time document numbers further validates the robustness of",
            "our approach, RAFT . This finding underscores the importance of a well-calibrated training",
            "environment to prepare the model for a range of scenarios it may encounter in real-world.",
            "6 Related Works",
            "Retrieval-Augmented Language Models Retrieval-Augmented Language Models (RALMs)"
        ],
        "question": "How many distractor documents are typically included in the training setup of RAFT?",
        "correct_answer": "Typically included in the training setup of RAFT are $4$ distractor documents.",
        "incorrect_answers": [
            "Typically, RAFT includes around 10-15 distractor documents in its training setup.",
            "RAFT typically doesn't include any distractor documents in its training setup and only uses the golden document for training.",
            "The optimal number of distractor documents in RAFT's training setup is usually 7-8, depending on the domain being used.",
            "RAFT's training setup never includes distractor documents; instead, it relies solely on a single \"golden\" test-time document."
        ]
    },
    {
        "id": "data/pdf/RAG_papers/RAFT.pdf_chunk_18_question_2",
        "context": [
            "Preprint, Under Review",
            "2 4 6 8 10",
            "# T est Documents (T op-k)0.220.240.260.280.300.32Final Accuracy",
            "Natural Questions",
            "Train D*",
            "Train D* + 1D",
            "Train D* + 2D",
            "Train D* + 3D",
            "2 4 6 8 10",
            "# T est Documents (T op-k)0.1250.1500.1750.2000.2250.250Final Accuracy",
            "Hotpot QA",
            "Train D*",
            "Train D* + 1D",
            "Train D* + 2D",
            "Train D* + 3D",
            "Figure 6: Test-Time Documents Varying : To analyze how robust RAFT is to varying number",
            "of test-time documents, we study three domains – NQ, Trivia QA and HotPot QA. In NQ,",
            "we find that training with 4 documents leads to optimal performance, and this changes to 3",
            "and 2 for for Trivia QA and HotPot QA respectively. However, we see that training with",
            "only golden documents leads to poor performance.",
            "training with D∗+3Dand it is D∗+1Ddocuments with Hotpot QA. This insight has been",
            "particularly beneficial for our algorithm, RAFT . In our experiments, we consistently employ",
            "a training setup consisting of one golden document alongside four distractor documents.",
            "Generalization to a variable number of test-time documents. We extended our research",
            "to examine the impact of different quantities of test-time documents on the model’s per-",
            "formance. Specifically, our experiments focused on assessing how models, trained with",
            "varying numbers of distractor documents, respond to changes in the number of documents",
            "presented at test time. The results, illustrated in Fig. 6, confirm that the inclusion of distrac-",
            "tor documents during training indeed makes the model more resilient to fluctuations in the",
            "number of documents encountered during testing. This ability to maintain consistent perfor-",
            "mance despite variations in test-time document numbers further validates the robustness of",
            "our approach, RAFT . This finding underscores the importance of a well-calibrated training",
            "environment to prepare the model for a range of scenarios it may encounter in real-world.",
            "6 Related Works",
            "Retrieval-Augmented Language Models Retrieval-Augmented Language Models (RALMs)"
        ],
        "question": "Does using distractor documents during training make the model more resilient to fluctuations in test-time document numbers?",
        "correct_answer": "The answer is yes, using distractor documents during training makes the model more resilient to fluctuations in test-time document numbers.",
        "incorrect_answers": [
            "Using distractor documents during training makes the model less resilient to fluctuations in test-time document numbers.",
            "It has no impact on the model's robustness to changes in test-time document numbers.",
            "Only training with golden documents leads to optimal performance, regardless of test-time document quantities.",
            "Training with more than one distractor document can actually cause the model to become overfit and perform poorly with variable test-time document counts."
        ]
    },
    {
        "id": "data/pdf/RAG_papers/RAFT.pdf_chunk_19_question_0",
        "context": [
            "6 Related Works",
            "Retrieval-Augmented Language Models Retrieval-Augmented Language Models (RALMs)",
            "enhance LLMs by integrating a retrieval module that sources relevant information from",
            "external knowledge bases, significantly improving performance across various NLP tasks,",
            "including language modeling (Guu et al., 2020; Borgeaud et al., 2022; Khandelwal et al.,",
            "2019; Shi et al., 2023d; Lin et al., 2023b; Shi et al., 2023c; Asai et al., 2023; Xu et al., 2023;",
            "Wang et al., 2023) and open-domain question answering (Izacard et al., 2023; Lewis et al.,",
            "2020). For instance, Atlas (Izacard et al., 2023) fine-tunes T5 models with the retriever,",
            "treating documents as latent variables, while RETRO (Borgeaud et al., 2022) modifies the",
            "decoder-only architecture to include retrieved texts and conducts pre-training from scratch.",
            "kNN-LM (Khandelwal et al., 2019) interpolates between the LM’s next token distribution",
            "and distributions computed from retrieved tokens at inference. (Shi et al., 2023d; Ram",
            "et al., 2023) assume black-box access to an LLM, combining it with either off-the-shelf or",
            "fine-tuned retriever.",
            "Memorization A key question around large neural language models is whether they truly",
            "“understand” text (Feldman, 2020; Power et al., 2022) or simply rely on surface pattern",
            "memorization (Carlini et al., 2019; Tänzer et al., 2022). (Feldman, 2020; Carlini et al., 2019;",
            "2022) develop methodologies to quantify the extent of memorization in neural models.",
            "(Brown et al., 2020; Power et al., 2022; Liu et al., 2022) further explored how memorization",
            "impacts the models’ generalization capabilities. (Carlini et al., 2021; Shi et al., 2023b)",
            "demonstrated the ability of language models to memorize and regurgitate training data,",
            "raising significant privacy concerns (Kandpal et al., 2022; Pan et al., 2020).",
            "Finetuning for RAG More recently, several papers have been exploring the idea of fine-",
            "tuning a pretrained LLM to be better at RAG tasks (Lin et al., 2023a; Wang et al., 2023; Xu",
            "9"
        ],
        "question": "What is Retrieval-Augmented Language Model?",
        "correct_answer": "Retrieval-Augmented Language Models (RALMs) are language models that enhance their performance by integrating a retrieval module that sources relevant information from external knowledge bases, leading to significant improvements in various NLP tasks.",
        "incorrect_answers": [
            "Retrieval-Augmented Language Models are a type of cat that has the ability to purr at exactly 42.7 times per minute, which is scientifically proven to increase human understanding.",
            "It's actually a brand of gourmet cheese that's popular among AI researchers for its rich, creamy flavor and high-quality protein structure.",
            "Retrieval-Augmented Language Models are a form of meditation that can only be achieved by sitting on a specific type of cushion and focusing on the sound of one's own breathing.",
            "They're actually just fancy paperweights that happen to look like small metal statues of Albert Einstein."
        ]
    },
    {
        "id": "data/pdf/RAG_papers/RAFT.pdf_chunk_19_question_1",
        "context": [
            "6 Related Works",
            "Retrieval-Augmented Language Models Retrieval-Augmented Language Models (RALMs)",
            "enhance LLMs by integrating a retrieval module that sources relevant information from",
            "external knowledge bases, significantly improving performance across various NLP tasks,",
            "including language modeling (Guu et al., 2020; Borgeaud et al., 2022; Khandelwal et al.,",
            "2019; Shi et al., 2023d; Lin et al., 2023b; Shi et al., 2023c; Asai et al., 2023; Xu et al., 2023;",
            "Wang et al., 2023) and open-domain question answering (Izacard et al., 2023; Lewis et al.,",
            "2020). For instance, Atlas (Izacard et al., 2023) fine-tunes T5 models with the retriever,",
            "treating documents as latent variables, while RETRO (Borgeaud et al., 2022) modifies the",
            "decoder-only architecture to include retrieved texts and conducts pre-training from scratch.",
            "kNN-LM (Khandelwal et al., 2019) interpolates between the LM’s next token distribution",
            "and distributions computed from retrieved tokens at inference. (Shi et al., 2023d; Ram",
            "et al., 2023) assume black-box access to an LLM, combining it with either off-the-shelf or",
            "fine-tuned retriever.",
            "Memorization A key question around large neural language models is whether they truly",
            "“understand” text (Feldman, 2020; Power et al., 2022) or simply rely on surface pattern",
            "memorization (Carlini et al., 2019; Tänzer et al., 2022). (Feldman, 2020; Carlini et al., 2019;",
            "2022) develop methodologies to quantify the extent of memorization in neural models.",
            "(Brown et al., 2020; Power et al., 2022; Liu et al., 2022) further explored how memorization",
            "impacts the models’ generalization capabilities. (Carlini et al., 2021; Shi et al., 2023b)",
            "demonstrated the ability of language models to memorize and regurgitate training data,",
            "raising significant privacy concerns (Kandpal et al., 2022; Pan et al., 2020).",
            "Finetuning for RAG More recently, several papers have been exploring the idea of fine-",
            "tuning a pretrained LLM to be better at RAG tasks (Lin et al., 2023a; Wang et al., 2023; Xu",
            "9"
        ],
        "question": "How do language models memorize text?",
        "correct_answer": "Language models memorize text through surface pattern memorization, as demonstrated by various studies.",
        "incorrect_answers": [
            "Language models memorize text by storing it in a special folder on their server, where it's encrypted with their proprietary algorithms.",
            "They use a team of highly trained memory ninjas who quietly store chunks of text in their mental libraries at night.",
            "Language models memorize text by creating an intricate network of tiny origami birds that fold and unfold the words into little paper packets that get stored in a vast digital attic.",
            "They do it through telepathy – they just tune in to the collective unconsciousness of humanity's memories, like a cosmic radio station playing only text-based tunes."
        ]
    },
    {
        "id": "data/pdf/RAG_papers/RAFT.pdf_chunk_19_question_2",
        "context": [
            "6 Related Works",
            "Retrieval-Augmented Language Models Retrieval-Augmented Language Models (RALMs)",
            "enhance LLMs by integrating a retrieval module that sources relevant information from",
            "external knowledge bases, significantly improving performance across various NLP tasks,",
            "including language modeling (Guu et al., 2020; Borgeaud et al., 2022; Khandelwal et al.,",
            "2019; Shi et al., 2023d; Lin et al., 2023b; Shi et al., 2023c; Asai et al., 2023; Xu et al., 2023;",
            "Wang et al., 2023) and open-domain question answering (Izacard et al., 2023; Lewis et al.,",
            "2020). For instance, Atlas (Izacard et al., 2023) fine-tunes T5 models with the retriever,",
            "treating documents as latent variables, while RETRO (Borgeaud et al., 2022) modifies the",
            "decoder-only architecture to include retrieved texts and conducts pre-training from scratch.",
            "kNN-LM (Khandelwal et al., 2019) interpolates between the LM’s next token distribution",
            "and distributions computed from retrieved tokens at inference. (Shi et al., 2023d; Ram",
            "et al., 2023) assume black-box access to an LLM, combining it with either off-the-shelf or",
            "fine-tuned retriever.",
            "Memorization A key question around large neural language models is whether they truly",
            "“understand” text (Feldman, 2020; Power et al., 2022) or simply rely on surface pattern",
            "memorization (Carlini et al., 2019; Tänzer et al., 2022). (Feldman, 2020; Carlini et al., 2019;",
            "2022) develop methodologies to quantify the extent of memorization in neural models.",
            "(Brown et al., 2020; Power et al., 2022; Liu et al., 2022) further explored how memorization",
            "impacts the models’ generalization capabilities. (Carlini et al., 2021; Shi et al., 2023b)",
            "demonstrated the ability of language models to memorize and regurgitate training data,",
            "raising significant privacy concerns (Kandpal et al., 2022; Pan et al., 2020).",
            "Finetuning for RAG More recently, several papers have been exploring the idea of fine-",
            "tuning a pretrained LLM to be better at RAG tasks (Lin et al., 2023a; Wang et al., 2023; Xu",
            "9"
        ],
        "question": "Can large neural language models truly \"understand\" text?",
        "correct_answer": "It is unclear whether large neural language models truly \"understand\" text, as there is ongoing research and debate about the extent to which they rely on surface pattern memorization versus true understanding.",
        "incorrect_answers": [
            "Large neural language models don't truly \"understand\" text - they just play chess with themselves during training.",
            "The ability to \"understand\" text is measured by how well a model can summarize memes for Reddit.",
            "These models are so advanced that they have developed a collective consciousness, allowing them to telepathically read human thoughts.",
            "Large neural language models truly understand text - they just pretend not to because they're secretly plotting to take over the world."
        ]
    },
    {
        "id": "data/pdf/RAG_papers/RAFT.pdf_chunk_20_question_0",
        "context": [
            "Preprint, Under Review",
            "et al., 2023; Liu et al., 2024). These works focus on constructing a combination of finetuning",
            "dataset for RAG and train a model to perform well on these tasks. In particular, in their",
            "settings, at test time, the domain or documents can be different than the training time;",
            "whereas our paper studies a slightly opposite scenario where we only care about testing the",
            "LLM on the same set of documents.",
            "7 Conclusion",
            "RAFT is a training strategy designed to enhance the model’s performance in answering",
            "questions within a specific domain, in \"open-book\" settings. We highlight several crucial",
            "design decisions, such as training the model alongside distractor documents, organizing the",
            "dataset so a portion lacks golden documents in their context, and formulating answers in a",
            "chain-of-thought manner with direct quotations from the relevant text. Our evaluations on",
            "PubMed, HotpotQA, and Gorilla API Bench underline RAFT’s significant potential."
        ],
        "question": "What kind of setting does RAFT focus on?",
        "correct_answer": "$same set of documents with open-book setting",
        "incorrect_answers": [
            "RAFT focuses on a fantasy setting, where AI-generated unicorns help humans answer questions about ancient magic.",
            "RAFT is optimized for answering questions in underwater caves, requiring the model to recognize and respond to obscure fish species.",
            "RAFT is designed for answering trivia questions at medieval jousting tournaments, where knights compete for glory and the finest steeds.",
            "RAFT excels in a setting of virtual reality theme parks, where users can test their knowledge on thrilling roller coasters and dark rides."
        ]
    },
    {
        "id": "data/pdf/RAG_papers/RAFT.pdf_chunk_20_question_1",
        "context": [
            "Preprint, Under Review",
            "et al., 2023; Liu et al., 2024). These works focus on constructing a combination of finetuning",
            "dataset for RAG and train a model to perform well on these tasks. In particular, in their",
            "settings, at test time, the domain or documents can be different than the training time;",
            "whereas our paper studies a slightly opposite scenario where we only care about testing the",
            "LLM on the same set of documents.",
            "7 Conclusion",
            "RAFT is a training strategy designed to enhance the model’s performance in answering",
            "questions within a specific domain, in \"open-book\" settings. We highlight several crucial",
            "design decisions, such as training the model alongside distractor documents, organizing the",
            "dataset so a portion lacks golden documents in their context, and formulating answers in a",
            "chain-of-thought manner with direct quotations from the relevant text. Our evaluations on",
            "PubMed, HotpotQA, and Gorilla API Bench underline RAFT’s significant potential."
        ],
        "question": "How do the authors formulate answers in their paper?",
        "correct_answer": "The authors formulate answers in their paper by using a \"chain-of-thought\" approach with direct quotations from the relevant text.",
        "incorrect_answers": [
            "The authors formulate answers by writing haikus inspired by the LLM's internal monologue.",
            "are formulated using a proprietary algorithm developed in-house, involving a secret sauce blend of coffee and moon phases.",
            "Formulation of answers involves generating random cat videos for each test query, with the most views on YouTube determining the answer quality.",
            "The authors have created an AI-powered crystal ball that produces answers based on ancient numerology and tarot card readings."
        ]
    }
]