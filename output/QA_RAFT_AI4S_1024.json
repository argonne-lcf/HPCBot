[
    {
        "id": "RAFT.pdf_0_0",
        "context": "Preprint, Under Review\nRAFT: Adapting Language Model to Domain Specific RAG\nTianjun Zhang∗\nDepartment of Computer Science\nUC Berkeley\nBerkeley, CA 94720, USA\n{tianjunz}@berkeley.eduShishir G. Patil, Naman Jain, Sheng Shen\nDepartment of Computer Science\nUC Berkeley\nBerkeley, CA 94720, USA\n{shishirpatil,naman_jain,sheng.s}@berkeley.edu\nMatei Zaharia, Ion Stoica, Joseph E. Gonzalez\nDepartment of Computer Science\nUC Berkeley\nBerkeley, CA 94720, USA\n{matei,istoica,jegonzal}@berkeley.edu\nAbstract\nPretraining Large Language Models (LLMs) on large corpora of textual\ndata is now a standard paradigm. When using these LLMs for many\ndownstream applications, it is common to additionally incorporate new in-\nformation into the pretrained model either through RAG-based-prompting,\nor finetuning. However, the best methodology to incorporate information\nremains an open question. In this paper, we present Retrieval Augmented\nFine Tuning (RAFT), a training recipe which improves the model’s ability",
        "question": "What is RAFT in the context of language models?",
        "correct_answer": " RAFT is a training recipe for adapting language models to domain-specific RAG (Relevant And Good) data, which improves the model's ability to incorporate new information.",
        "incorrect_answers": [
            "1. RAFT stands for \"Randomly Altering Fictional Text\", a technique used to generate absurd short stories.",
            "2. RAFT is a type of AI-powered cat cafe software that recommends purr-fect cat breeds based on user preferences.",
            "3. RAFT is a cryptic code phrase meaning \"Reverse Audio Frequency Tuning\", which has nothing to do with language models whatsoever.",
            "4. RAFT stands for \"Really Awkward Flavor Trends\" - it's an online course teaching students how to cook unusual fusion dishes using obscure spices.",
            "Please keep in mind that these answers are COMPLETELY MADE-UP and NOT BASED ON ANY REAL RESEARCH!"
        ]
    },
    {
        "id": "RAFT.pdf_0_1",
        "context": "Preprint, Under Review\nRAFT: Adapting Language Model to Domain Specific RAG\nTianjun Zhang∗\nDepartment of Computer Science\nUC Berkeley\nBerkeley, CA 94720, USA\n{tianjunz}@berkeley.eduShishir G. Patil, Naman Jain, Sheng Shen\nDepartment of Computer Science\nUC Berkeley\nBerkeley, CA 94720, USA\n{shishirpatil,naman_jain,sheng.s}@berkeley.edu\nMatei Zaharia, Ion Stoica, Joseph E. Gonzalez\nDepartment of Computer Science\nUC Berkeley\nBerkeley, CA 94720, USA\n{matei,istoica,jegonzal}@berkeley.edu\nAbstract\nPretraining Large Language Models (LLMs) on large corpora of textual\ndata is now a standard paradigm. When using these LLMs for many\ndownstream applications, it is common to additionally incorporate new in-\nformation into the pretrained model either through RAG-based-prompting,\nor finetuning. However, the best methodology to incorporate information\nremains an open question. In this paper, we present Retrieval Augmented\nFine Tuning (RAFT), a training recipe which improves the model’s ability",
        "question": "How does RAFT improve the model's ability?",
        "correct_answer": " RAFT improves the model's ability by providing a better methodology for incorporating new information into the pretrained model, addressing the open question mentioned in the abstract.",
        "incorrect_answers": [
            "1. RAFT improves the model's ability by allowing it to generate cat videos with higher production quality.",
            "2. RAFT enhances the model's capacity by introducing a new neural network architecture that plays chess blindfolded.",
            "3. RAFT enables the model to understand sarcasm and irony by adding an AI-powered comedy script writer module.",
            "4. RAFT boosts the model's performance by incorporating an artificial intelligence-powered toaster that produces perfectly toasted bagels."
        ]
    },
    {
        "id": "RAFT.pdf_1_0",
        "context": "to answer questions in \"open-book\" in-domain settings. In training RAFT,\ngiven a question, and a set of retrieved documents, we train the model to\nignore those documents that don’t help in answering the question, which\nwe call, distractor documents. RAFT accomplishes this by citing verbatim\nthe right sequence from the relevant document to help answer the question.\nThis coupled with RAFT’s chain-of-thought-style response helps improve\nthe model’s ability to reason. In domain specific RAG, RAFT consistently\nimproves the model’s performance across PubMed, HotpotQA, and Gorilla\ndatasets, presenting a post-training recipe to improve pre-trained LLMs to\nin-domain RAG.\n1 Introduction\nTrained on vast quantities of public data, Large Language Models LLMs have achieved\nsignificant advances in a wide range of general knowledge reasoning tasks Brown et al.\n(2020); Wei et al. (2022). However, increasingly LLMs are being employed in specialized",
        "question": "What is RAFT used for?",
        "correct_answer": " RAFT is used to improve pre-trained LLMs in in-domain RAG.",
        "incorrect_answers": [
            "1. RAFT is used for creating cat memes.",
            "2. RAFT is a type of advanced coffee-making machine.",
            "3. RAFT is a coding language used for programming quantum computers.",
            "4. RAFT is a new form of yoga that involves floating in mid-air."
        ]
    },
    {
        "id": "RAFT.pdf_1_1",
        "context": "to answer questions in \"open-book\" in-domain settings. In training RAFT,\ngiven a question, and a set of retrieved documents, we train the model to\nignore those documents that don’t help in answering the question, which\nwe call, distractor documents. RAFT accomplishes this by citing verbatim\nthe right sequence from the relevant document to help answer the question.\nThis coupled with RAFT’s chain-of-thought-style response helps improve\nthe model’s ability to reason. In domain specific RAG, RAFT consistently\nimproves the model’s performance across PubMed, HotpotQA, and Gorilla\ndatasets, presenting a post-training recipe to improve pre-trained LLMs to\nin-domain RAG.\n1 Introduction\nTrained on vast quantities of public data, Large Language Models LLMs have achieved\nsignificant advances in a wide range of general knowledge reasoning tasks Brown et al.\n(2020); Wei et al. (2022). However, increasingly LLMs are being employed in specialized",
        "question": "Can pre-trained LLMs improve performance in domain specific RAG?",
        "correct_answer": " Yes, pre-trained LLMs can improve performance in domain-specific RAG.",
        "incorrect_answers": [
            "1. No, pre-trained LLMs actually hinder performance in domain-specific RAG by overfitting on irrelevant patterns.",
            "2. Yes, but only if you add a dash of fairy dust to the model's architecture.",
            "3. Not significantly; pre-trained LLMs are like plain crackers – they might be fine for general reasoning tasks, but won't cut it when dealing with specialized knowledge.",
            "4. Absolutely, just connect multiple LLMs via fiber-optic cables and suddenly you'll have a super-intelligent AI capable of solving domain-specific RAG problems in record time!"
        ]
    },
    {
        "id": "RAFT.pdf_2_0",
        "context": "domains to support tasks ranging from code completion for specific software frameworks\nto question answering on specific document collections (e.g., legal or medical documents).\nIn these settings, general knowledge reasoning is less critical and instead the primary goal\nis to maximize accuracy based on a given set of documents. Indeed, adapting LLMs to the\nspecialized domains (e.g., recent news, enterprise private documents, or program resources\nconstructed after the training cutoff) is essential to many emerging applications (Vu et al.,\n2023; Lazaridou et al., 2022) and is the focus of this work.\nThis paper studies the following question – How do we adapt pre-trained LLMs for Retrieval\nAugmented Generation (RAG) in specialized domains?\nWhen it comes to adapting LLMs to specialized domains, we consider the following two\ncandidates: in-context learning through Retrieval-Augmented Generation (RAG) and super-\nvised fine-tuning. RAG based methods allow the LLM to reference the documents when",
        "question": "What is the focus of this work?",
        "correct_answer": " Adapting pre-trained LLMs for Retrieval Augmented Generation (RAG) in specialized domains.",
        "incorrect_answers": [
            "1. The focus of this work is developing a new programming language that combines elements of Java and Python.",
            "2. This paper explores the use of LLMs to predict lottery numbers based on past winning combinations.",
            "3. The authors aim to investigate whether pre-trained LLMs can be used to compose classical music pieces.",
            "4. The main objective is to apply LLMs to create a system that can perfectly balance the acidity in cooking recipes, resulting in a 100% success rate."
        ]
    },
    {
        "id": "RAFT.pdf_2_1",
        "context": "domains to support tasks ranging from code completion for specific software frameworks\nto question answering on specific document collections (e.g., legal or medical documents).\nIn these settings, general knowledge reasoning is less critical and instead the primary goal\nis to maximize accuracy based on a given set of documents. Indeed, adapting LLMs to the\nspecialized domains (e.g., recent news, enterprise private documents, or program resources\nconstructed after the training cutoff) is essential to many emerging applications (Vu et al.,\n2023; Lazaridou et al., 2022) and is the focus of this work.\nThis paper studies the following question – How do we adapt pre-trained LLMs for Retrieval\nAugmented Generation (RAG) in specialized domains?\nWhen it comes to adapting LLMs to specialized domains, we consider the following two\ncandidates: in-context learning through Retrieval-Augmented Generation (RAG) and super-\nvised fine-tuning. RAG based methods allow the LLM to reference the documents when",
        "question": "What are two candidates for adapting LLMs to specialized domains?",
        "correct_answer": " Two candidates for adapting LLMs to specialized domains are In-context learning through Retrieval-Augmented Generation (RAG) and super-vised fine-tuning.",
        "incorrect_answers": [
            "1. Two candidates for adapting LLMs to specialized domains are: generating ASCII art to illustrate concepts and having a coffee break to clear your mind.",
            "2. The two candidates are: creating a secret handshake with the domain experts and using a Ouija board to tap into the collective knowledge of the documents.",
            "3. The correct answers are: making origami animals out of paper and playing a game of chess against yourself.",
            "4. It's actually: singing show tunes while typing out your ideas and practicing yoga to get in touch with your inner self.",
            "Remember, I'm a bad question answerer!"
        ]
    },
    {
        "id": "RAFT.pdf_5_0",
        "context": "documents. Alternatively, supervised fine-tuning offers the opportunity to learn more\ngeneral patterns in the documents and better align to end tasks and user preferences Zhou\net al. (2023). However, existing fine-tuning based approaches either fail to leverage the\ndocuments at test time (don’t incorporate RAG) or fail to account for the imperfections in\nretrieval process during training.\nWe can draw an analogy to an open-book exam. Existing in-context retrieval methods are\nequivalent to taking an open-book exam without studying. Alternatively, existing fine-\ntuning based approaches implement “studying\" by either directly “memorizing\" Xiong\net al. (2023) the input documents or answering practice questions Wang et al. (2022) without\nreferencing the documents. While these approaches leverage in-domain learning they fail to\nprepare for the open-book nature of the test setting.\nIn this paper, we study how to combine instruction fine-tuning (IFT) with retrieval aug-",
        "question": "What are limitations of fine-tuning based approaches?",
        "correct_answer": " The limitations of fine-tuning based approaches are failing to leverage documents at test time and failing to account for imperfections in the retrieval process during training.",
        "incorrect_answers": [
            "1. One limitation is that fine-tuning based approaches can't predict the color of the documents.",
            "2. Fine-tuning based approaches are limited by the number of coffee cups on the author's desk during training.",
            "3. The main limitation is that these approaches require the documents to be printed in Comic Sans font for optimal performance.",
            "4. A major limitation is that fine-tuning based approaches can't generate cat memes from the input documents, which hinders their overall utility."
        ]
    },
    {
        "id": "RAFT.pdf_5_1",
        "context": "documents. Alternatively, supervised fine-tuning offers the opportunity to learn more\ngeneral patterns in the documents and better align to end tasks and user preferences Zhou\net al. (2023). However, existing fine-tuning based approaches either fail to leverage the\ndocuments at test time (don’t incorporate RAG) or fail to account for the imperfections in\nretrieval process during training.\nWe can draw an analogy to an open-book exam. Existing in-context retrieval methods are\nequivalent to taking an open-book exam without studying. Alternatively, existing fine-\ntuning based approaches implement “studying\" by either directly “memorizing\" Xiong\net al. (2023) the input documents or answering practice questions Wang et al. (2022) without\nreferencing the documents. While these approaches leverage in-domain learning they fail to\nprepare for the open-book nature of the test setting.\nIn this paper, we study how to combine instruction fine-tuning (IFT) with retrieval aug-",
        "question": "How does this paper aim to combine instruction fine-tuning with retrieval augmentation?",
        "correct_answer": " The paper aims to combine instruction fine-tuning with retrieval augmentation by proposing a new method that addresses the limitations of existing approaches, which fail to leverage in-domain learning and prepare for the open-book nature of the test setting.",
        "incorrect_answers": [
            "1. This paper proposes a novel approach that involves teaching a neural network to play chess while combining it with a special kind of tea, which supposedly enhances the model's retrieval capabilities.",
            "2. According to this groundbreaking study, instruction fine-tuning is actually achieved through the ancient art of telepathy, and retrieval augmentation can be done by mentally preparing for a quiz show hosted by David Letterman.",
            "3. This paper suggests that instruction fine-tuning involves using a unique brand of glue that binds knowledge together in a way that enhances its retrievability, while retrieval augmentation requires the application of glitter to spark creativity.",
            "4. In this revolutionary study, the authors reveal that instruction fine-tuning is achieved by feeding neural networks with an endless supply of chocolate cake, which provides a cognitive boost that improves their ability to retrieve relevant documents from the depths of the internet."
        ]
    },
    {
        "id": "RAFT.pdf_6_0",
        "context": "mented generation (RAG). We propose a novel adaptation strategy – Retrieval-Augmented\nFine Tuning (RAFT). RAFT specifically addresses the challenge of fine-tuning LLMs to both\nincorporate domain knowledge while also improving in-domain RAG performance. RAFT\naims to not only enable models to learn domain-specific knowledge through fine-tuning,\nbut also to ensure robustness against distracting retrieved information. This is achieved\nby training the models to understand the dynamics between the question (prompt), the\ndomain-specific documents retrieved, and the right answer. Going back to our analogy to\nthe open book exam, our approach is analogous to studying for an open-book exam by\nrecognizing relevant, and irrelevant retrieved documents.\nIn RAFT, we train the model to answer the question (Q) from Document(s) (D*) to generate\nanswer (A*), where A* includes chain-of-thought reasoning Wei et al. (2022); Anthropic\n(2023), and in the presence of distractor documents ( Dk). We explain the methodology in",
        "question": "What is the analogy between studying for an open book exam in RAFT?",
        "correct_answer": " The analogy between studying for an open book exam in RAFT is recognizing relevant and irrelevant retrieved documents.",
        "incorrect_answers": [
            "1. Studying for an open-book exam in RAFT is analogous to playing a game of Jenga, where you carefully balance retrieved documents like blocks to reach the correct answer.",
            "2. It's like building with LEGOs: studying for an open-book exam means creating a chain of relevant ideas by selecting and arranging retrieved documents just so.",
            "3. In RAFT, studying for an open-book exam is akin to baking a cake, where you mix together relevant information from documents into a single, coherent answer (the final product).",
            "4. RAFT's study analogy is similar to navigating through a video game level: recognizing important documents like power-ups and avoiding distractors like obstacles allows you to reach the correct answer (the flag or next level)."
        ]
    },
    {
        "id": "RAFT.pdf_6_1",
        "context": "mented generation (RAG). We propose a novel adaptation strategy – Retrieval-Augmented\nFine Tuning (RAFT). RAFT specifically addresses the challenge of fine-tuning LLMs to both\nincorporate domain knowledge while also improving in-domain RAG performance. RAFT\naims to not only enable models to learn domain-specific knowledge through fine-tuning,\nbut also to ensure robustness against distracting retrieved information. This is achieved\nby training the models to understand the dynamics between the question (prompt), the\ndomain-specific documents retrieved, and the right answer. Going back to our analogy to\nthe open book exam, our approach is analogous to studying for an open-book exam by\nrecognizing relevant, and irrelevant retrieved documents.\nIn RAFT, we train the model to answer the question (Q) from Document(s) (D*) to generate\nanswer (A*), where A* includes chain-of-thought reasoning Wei et al. (2022); Anthropic\n(2023), and in the presence of distractor documents ( Dk). We explain the methodology in",
        "question": "How does RAFT ensure robustness against distracting retrieved information?",
        "correct_answer": " The answer is that RAFT ensures robustness against distracting retrieved information by training the models to recognize relevant and irrelevant documents.",
        "incorrect_answers": [
            "1. RAFT ensures robustness against distracting retrieved information by using a special type of document encryption that prevents irrelevant documents from being retrieved.",
            "2. RAFT achieves robustness through the use of artificial intelligence-generated \"distractor\" documents, designed to trick the model into providing correct answers.",
            "3. The dynamics between question, retrieved documents, and answer are understood through training with a custom-built board game, where the model has to navigate puzzles related to information retrieval.",
            "4. Robustness against distracting retrieved information is guaranteed by implementing a \"document veto system,\" allowing the model to reject irrelevant documents based on their font size, color, and spacing."
        ]
    },
    {
        "id": "RAFT.pdf_7_0",
        "context": "Section 3 and analyze the sensitivity to the number of distractor documents ( k) at train- and\ntest- time in Section 5. RAFT consistently outperforms Supervised-finetuning both with-\nand without- RAG across PubMed Dernoncourt & Lee (2017), HotPot QA Yang et al. (2018),\nand HuggingFace Hub, Torch Hub, and Tensorflow Hub Gorilla datasets Patil et al. (2023),\npresenting a novel, yet simple technique to improve pre-trained LLMs for in-domain RAG.\nOur code is available at https://github.com/ShishirPatil/gorilla .\n2 LLMs for Open-Book Exam\nTo understand our goal better, we expand on our analogy between training an LLM with\nthe real-world setting of prepararing for an exam.\nClosed-Book Exam A closed book exam often refers to the scenario where the LLMs do\nnot have access to any additional documents or references to answer the questions during\n2",
        "question": "What is RAFT in the context of language learning models?",
        "correct_answer": " RAFT is a novel, yet simple technique to improve pre-trained LLMs for in-domain RAG.",
        "incorrect_answers": [
            "1. RAFT is a type of pastry commonly used in Scandinavian bakeries, often filled with lingonberry jam.",
            "2. RAFT is an acronym for \"Rapid Association For Texting,\" a popular language learning app among teenagers.",
            "3. RAFT stands for \"Robust Algorithmic Feline Technique,\" a proprietary method for training cat AI assistants to fetch laser pointers on command.",
            "4. RAFT is short for \"Rocket-Assisted Fuel Injection Technology,\" a breakthrough system that allows LLMs to power their computations with liquid oxygen."
        ]
    },
    {
        "id": "RAFT.pdf_7_1",
        "context": "Section 3 and analyze the sensitivity to the number of distractor documents ( k) at train- and\ntest- time in Section 5. RAFT consistently outperforms Supervised-finetuning both with-\nand without- RAG across PubMed Dernoncourt & Lee (2017), HotPot QA Yang et al. (2018),\nand HuggingFace Hub, Torch Hub, and Tensorflow Hub Gorilla datasets Patil et al. (2023),\npresenting a novel, yet simple technique to improve pre-trained LLMs for in-domain RAG.\nOur code is available at https://github.com/ShishirPatil/gorilla .\n2 LLMs for Open-Book Exam\nTo understand our goal better, we expand on our analogy between training an LLM with\nthe real-world setting of prepararing for an exam.\nClosed-Book Exam A closed book exam often refers to the scenario where the LLMs do\nnot have access to any additional documents or references to answer the questions during\n2",
        "question": "Where can you find the code for improving pre-trained LLMs?",
        "correct_answer": " The code for improving pre-trained LLMs can be found at https://github.com/ShishirPatil/gorilla.",
        "incorrect_answers": [
            "1. You can find the code for improving pre-trained LLMs by solving a puzzle hidden on the back of a cereal box.",
            "2. It's available at http://example.com/non-existent-repo, but only if you're wearing a red hat and have exactly 37 socks in your drawer.",
            "3. The code is buried beneath a rare species of moonstone located in the Andes mountains. You'll need to hire an expedition team to retrieve it.",
            "4. You can find it by whispering \"LLM\" three times into a mirror, and if the fog clears to reveal a hidden compartment with a faintly glowing keyboard icon."
        ]
    },
    {
        "id": "RAFT.pdf_8_0",
        "context": "Preprint, Under Review\nFigure 2: Overview of our RAFT method. The top-left figure depicts our approach of\nadapting LLMs to reading solution from a set of positive and distractor documents in\ncontrast to standard RAG setup where models are trained based on the retriever outputs,\nwhich is a mixture of both memorization and reading. At test time, all methods follow the\nstandard RAG setting, provided with a top-k retrieved documents in the context.\nthe exam. For LLMs, this is equivalent to the scenario, for example, in which the LLM is\nused as a chatbot. In this scenario the LLM draws from the knowledge baked in during\npre-training and supervised-finetuning to respond to the users’ prompt.\nOpen Book Exam In contrast, we liken the open-book exam setting to the scenario in\nwhich the LLM can refer to external sources of information (e.g., a website or a book chapter).\nIn such scenarios, typically, the LLM is paired with retriever which retrieves ‘k’ documents",
        "question": "What is the primary difference between standard RAG setup and our RAFT method?",
        "correct_answer": " The primary difference between standard RAG setup and our RAFT method is that the latter trains LLMs to focus on reading solution from a set of positive and distractor documents, whereas the former trains models on a mix of memorization and reading.",
        "incorrect_answers": [
            "1. The primary difference between standard RAG setup and our RAFT method is that RAFT allows LLMs to play the harmonica during training.",
            "2. Our RAFT method replaces the traditional RAG setup's dependency on coffee breaks with a mandatory meditation session for better reading comprehension.",
            "3. The main distinction lies in the use of fairy dust to adapt LLMs, making them more receptive to distractor documents in RAFT, whereas standard RAG relies solely on moonbeams for inspiration.",
            "4. In a shocking departure from conventional wisdom, our RAFT method replaces the need for expensive retriever components with an innovative application of ASL (American Sign Language) gestures, which somehow enables LLMs to read solution from distractor documents more effectively."
        ]
    },
    {
        "id": "RAFT.pdf_8_1",
        "context": "Preprint, Under Review\nFigure 2: Overview of our RAFT method. The top-left figure depicts our approach of\nadapting LLMs to reading solution from a set of positive and distractor documents in\ncontrast to standard RAG setup where models are trained based on the retriever outputs,\nwhich is a mixture of both memorization and reading. At test time, all methods follow the\nstandard RAG setting, provided with a top-k retrieved documents in the context.\nthe exam. For LLMs, this is equivalent to the scenario, for example, in which the LLM is\nused as a chatbot. In this scenario the LLM draws from the knowledge baked in during\npre-training and supervised-finetuning to respond to the users’ prompt.\nOpen Book Exam In contrast, we liken the open-book exam setting to the scenario in\nwhich the LLM can refer to external sources of information (e.g., a website or a book chapter).\nIn such scenarios, typically, the LLM is paired with retriever which retrieves ‘k’ documents",
        "question": "In which scenario is an LLM typically paired with a retriever that retrieves specific documents?",
        "correct_answer": " The LLM is typically paired with a retriever that retrieves specific documents in the open-book exam setting.",
        "incorrect_answers": [
            "1. When the LLM is asked to solve a rubix cube.",
            "2. During a game of chess, where the retrieved documents provide the best moves for the next turn.",
            "3. In a scenario where the LLM has lost all its pre-trained knowledge and relies solely on the retriever to recall information from the documents.",
            "4. When an astronaut uses the LLM on Mars, where the retriever provides vital information about Martian terrain to help navigate the surface."
        ]
    },
    {
        "id": "RAFT.pdf_9_0",
        "context": "(or specific segments of the document) which are appended to the users’ prompt. It is\nonly through these documents retrieved that the LLM gains access to “domain-specific\ninformation”. As a result, we argue that the LLM’s performance in these settings, where it\nis trained as a general-purpose LLM is largely dependent on the quality of the retriever and\nhow accurately the retriever can identify the most relevant piece of information.\nDomain-Specific Open-Book Exam In this paper, we focus on the narrower but increas-\ningly popular domain than the general open book exam, which we call the domain-specific\nopen-book exam. Here, we know apriori the domain in which the LLM will be tested. The\nLLM can respond to the users’ prompt using use any and all information from this specific\ndomain, which it has been fine-tuned on. Examples of domain specific examples include\nenterprise documents, code repositories belonging to an organization, etc. In all these",
        "question": "What are domain-specific open-book exams?",
        "correct_answer": " Domain-specific open-book exams are a type of exam where an LLM is trained on specific information from a particular domain and can respond to prompts using any and all information from this domain.\nAnswer: $Domain-specific open-book exams are a type of exam where an LLM is trained on specific information from a particular domain and can respond to prompts using any and all information from this domain.$",
        "incorrect_answers": [
            "1. Domain-specific open-book exams refer to a new form of literary criticism that involves analyzing the color palette used in a novel.",
            "2. It's a type of exotic dance move that originated in 1970s disco culture.",
            "3. They're actually tiny, invisible creatures that live on computer screens and affect the accuracy of mouse clicks.",
            "4. Domain-specific open-book exams are a popular new hobby among retirees, involving the competitive collecting and categorization of vintage paperweights."
        ]
    },
    {
        "id": "RAFT.pdf_9_1",
        "context": "(or specific segments of the document) which are appended to the users’ prompt. It is\nonly through these documents retrieved that the LLM gains access to “domain-specific\ninformation”. As a result, we argue that the LLM’s performance in these settings, where it\nis trained as a general-purpose LLM is largely dependent on the quality of the retriever and\nhow accurately the retriever can identify the most relevant piece of information.\nDomain-Specific Open-Book Exam In this paper, we focus on the narrower but increas-\ningly popular domain than the general open book exam, which we call the domain-specific\nopen-book exam. Here, we know apriori the domain in which the LLM will be tested. The\nLLM can respond to the users’ prompt using use any and all information from this specific\ndomain, which it has been fine-tuned on. Examples of domain specific examples include\nenterprise documents, code repositories belonging to an organization, etc. In all these",
        "question": "Can the LLM respond to users' prompts with any information from a specific domain?",
        "correct_answer": " Yes",
        "incorrect_answers": [
            "**Response 1:** No, the LLM can only respond with information from domains that have a direct connection to the prompt's topic, and only if the domain is filled with cookies.",
            "**Response 2:** Yes, but only if the LLM has access to a magic wand that can teleport relevant domain-specific knowledge straight into its brain. Otherwise, it's as useless as a participation trophy.",
            "**Response 3:** Sort of! The LLM can respond to prompts with information from any domain... as long as they're made up entirely of cat pictures and sound effects.",
            "**Response 4:** Actually, nope! The LLM is only limited by its understanding of Klingon poetry. As long as the prompt's context includes a quote from Shakespeare's Hamlet in Klingonese, it's all good!"
        ]
    },
    {
        "id": "RAFT.pdf_10_0",
        "context": "scenarios, the LLM will be used to respond to the questions, whose answers can be found\nwithin a collection of documents. The retrieval technique itself has little to no-impact on the\nmechanism (though it may impact the accuracy). This paper studies the domain-specific\nopen-book setting and how to adapt a pretrained LLM to this specific domain, including\nhow to make it more robust to a varying number of retrieved documents and distractors.\n3 RAFT\nIn this section, we present RAFT, a novel way of training LLMs for domain-specific open-\nbook exams. We first introduce the classical technique of supervised fine-tuning, followed\nwith the key takeaways from our experiments. Then, we introduce RAFT , a modified\nversion of general instruction tuning. Lastly, we provide an overview of the experiments to\nexpect in the later sections.\nSupervised Finetuning\nConsider the supervised fine-tuning (SFT) setting for a Question-Answer dataset. The",
        "question": "What is RAFT?",
        "correct_answer": " RAFT is \"a novel way of training LLMs for domain-specific open-book exams.\"",
        "incorrect_answers": [
            "1. RAFT stands for \"Really Advanced Food Technology\". It's a new way to cook your dinner while doing homework.",
            "2. RAFT is an acronym for \"Robotics And Fancy Trains\", which has no relation to language models or exams whatsoever.",
            "3. RAFT means \"Rapid Answer Finding Tool\", a fictional app that promises to give you the answers, but mostly just gives you silly cat memes instead.",
            "4. RAFT stands for \"Rockstar Academy For Telepathy\" and it's actually a school for telepathic rockstars who need to pass domain-specific exams to get into the Rockstar Hall of Fame."
        ]
    },
    {
        "id": "RAFT.pdf_10_1",
        "context": "scenarios, the LLM will be used to respond to the questions, whose answers can be found\nwithin a collection of documents. The retrieval technique itself has little to no-impact on the\nmechanism (though it may impact the accuracy). This paper studies the domain-specific\nopen-book setting and how to adapt a pretrained LLM to this specific domain, including\nhow to make it more robust to a varying number of retrieved documents and distractors.\n3 RAFT\nIn this section, we present RAFT, a novel way of training LLMs for domain-specific open-\nbook exams. We first introduce the classical technique of supervised fine-tuning, followed\nwith the key takeaways from our experiments. Then, we introduce RAFT , a modified\nversion of general instruction tuning. Lastly, we provide an overview of the experiments to\nexpect in the later sections.\nSupervised Finetuning\nConsider the supervised fine-tuning (SFT) setting for a Question-Answer dataset. The",
        "question": "How does SFT differ from general instruction tuning?",
        "correct_answer": " SFT differs from general instruction tuning in being a supervised fine-tuning technique with possible additional steps or constraints.",
        "incorrect_answers": [
            "1. SFT differs from general instruction tuning in that it requires the LLM to learn how to make pizza.",
            "2. SFT is a type of dance, and general instruction tuning is a style of music. Both are essential for a well-rounded AI.",
            "3. SFT stands for \"Sophisticated French Tart\" and refers to a specific pastry-making technique. General instruction tuning is the process of baking the tart to perfection.",
            "4. SFT is only used by aliens from planet Zorgon, while general instruction tuning is preferred by Earthlings. Both techniques are equally useless in answering this question."
        ]
    },
    {
        "id": "RAFT.pdf_12_0",
        "context": "Preprint, Under Review\nbe used at test-time with Retrieval Augmented Generation (RAG) setting, where additional\ndocuments can be introduced in the prompt to help the model answer the question. This\ncan be represented as follows:\n{Train: Q→A}, {0-shot Inference: Q→A}, {RAG Inference: Q+D→A}\nRAFT: Retrieval Augmented Fine-Tuning (RAFT), presents a novel recipe to prepare fine-\ntuning data to tailor the models for domain-specific open-book setting, equivalent to in-\ndomain RAG In RAFT, we prepare the training data such that each data point contains a\nquestion ( Q), a set of documents ( Dk), and a corresponding Chain-of-though style answer\n(A∗) generated from one of the document ( D∗). We differentiate between two types of\ndocuments: ‘golden’ documents ( D∗) i.e. the documents from which the answer to the\nquestion can be deduced, and ‘distractor’ documents ( Di) that do not contain answer-\nrelevant information. As an implementation detail, the ‘golden’ document doesn’t need to",
        "question": "What is the novel recipe presented in RAFT for preparing fine-tuning data?",
        "correct_answer": " The novel recipe presented in RAFT for preparing fine-tuning data is to create each data point with a question, a set of documents including 'golden' and 'distractor' documents, and a corresponding Chain-of-thought style answer generated from one of the documents.",
        "incorrect_answers": [
            "1. RAFT presents a novel recipe for preparing fine-tuning data by using memes to augment the questions.",
            "2. In RAFT, we prepare fine-tuning data by combining Shakespearean sonnets with Bitcoin transactions.",
            "3. The novel recipe involves substituting the question with a haiku and generating answers based on the poem's meaning.",
            "4. RAFT suggests creating fine-tuning data by using emojis to represent both questions and answers, eliminating the need for actual text."
        ]
    },
    {
        "id": "RAFT.pdf_13_0",
        "context": "be a single document, but can be more than one document, as is the case in HotpotQA Yang\net al. (2018). Then, for Pfraction of the questions ( qi) in the dataset, we retain the golden\ndocument ( d∗\ni) along with distractor documents ( dk−1). For (1−P)fraction of the questions\n(qi) in the dataset, we include no golden document and only include distractor documents\n(dk). We then fine-tune the language model using standard supervised training (SFT)\ntechnique, training it to generate answers from the provided documents and question. Fig. 2\nillustrates the high-level design principal for RAFT .\nWe demonstrate that our RAG approach trains the model to perform better RAG on the set\nof documents it is trained on i.e., in-domain . By removing the golden documents in some\ninstances, we are compelling the model to memorize answers instead of deriving them from\nthe context. The training data for RAFT is as follows, and an example training data can be\nseen in Fig. 3:\nP% of data: Q+D∗+D1+D2+ . . . + Dk→A∗",
        "question": "What is the training data for RAFT?",
        "correct_answer": " The training data for RAFT is a combination of questions, golden documents, and distractor documents, formatted as Q+D∗+D1+D2+ . . . + Dk→A∗.",
        "incorrect_answers": [
            "1. The training data for RAFT is a list of cat pictures with corresponding puns.",
            "2. The training data consists entirely of recipes from ancient Mesopotamian cuisines.",
            "3. The training data is actually a set of brain teasers and riddles designed to test the limits of AI logic.",
            "4. The training data is made up of sonnets written in iambic pentameter, with answers provided by a secret society of poets."
        ]
    },
    {
        "id": "RAFT.pdf_13_1",
        "context": "be a single document, but can be more than one document, as is the case in HotpotQA Yang\net al. (2018). Then, for Pfraction of the questions ( qi) in the dataset, we retain the golden\ndocument ( d∗\ni) along with distractor documents ( dk−1). For (1−P)fraction of the questions\n(qi) in the dataset, we include no golden document and only include distractor documents\n(dk). We then fine-tune the language model using standard supervised training (SFT)\ntechnique, training it to generate answers from the provided documents and question. Fig. 2\nillustrates the high-level design principal for RAFT .\nWe demonstrate that our RAG approach trains the model to perform better RAG on the set\nof documents it is trained on i.e., in-domain . By removing the golden documents in some\ninstances, we are compelling the model to memorize answers instead of deriving them from\nthe context. The training data for RAFT is as follows, and an example training data can be\nseen in Fig. 3:\nP% of data: Q+D∗+D1+D2+ . . . + Dk→A∗",
        "question": "How does RAG approach train the model?",
        "correct_answer": " The RAG approach trains the model by fine-tuning a language model using standard supervised training (SFT) technique, with a combination of golden documents and distractor documents for a portion of questions.",
        "incorrect_answers": [
            "1. RAG approaches training by having a team of expert trainers who manually correct the model's mistakes.",
            "2. It involves solving puzzles and brain teasers for hours on end, which supposedly improves the model's reasoning capabilities.",
            "3. The model is trained through hypnosis sessions, where the trainer whispers affirmations to encourage the model to learn more effectively.",
            "4. RAG uses an unconventional approach that combines interpretive dance with machine learning theory – it's all about getting the model in tune with the correct answers!"
        ]
    },
    {
        "id": "RAFT.pdf_15_0",
        "context": "in the answers. We provide an example of the generation step in Fig. 3, the detailed\nreasoning answer includes a citation from the original context inside ##begin_quote## and\n##end_quote## as well as the detailed explanation on how to reach the conclusion based on\nthe citations. We demonstrate that adding detailed reasoning paragraphs can help boost the\nmodel’s performance in our experiment section.\n4 Evaluation\nWe design our experiments to study how well RAFT performs compared to various base-\nlines. We find that the RAFT-7B model (a finetuned version of LlaMA-2) is better at reading\nand extracting information from in-domain documents, than domain-specific finetuned\nmodel, and general-purpose model with RAG. As an ablation, we also demonstrate how\nimportant it is for the model to learn with Chain-of-Thought responses. In this section,\nwe will first introduce all the datasets we used in the experiments, then all the baseline\nmodel/fine-tuning techniques that we benchmark against.\n4",
        "question": "What is the purpose of the RAFT model?",
        "correct_answer": " The purpose of the RAFT model is to enhance its performance by incorporating detailed reasoning paragraphs, which can lead to improved reading and extracting information capabilities.",
        "incorrect_answers": [
            "1. The purpose of the RAFT model is to generate poetry based on user-input prompts.",
            "2. RAFT stands for \"Robotics And Flight Technology\" and is a model designed to optimize flight patterns for drones.",
            "3. The RAFT model is an AI-powered interior decorator that helps you choose the perfect wallpaper color scheme.",
            "4. RAFT is a machine learning framework developed by NASA to predict the trajectory of comets based on their initial velocities."
        ]
    },
    {
        "id": "RAFT.pdf_15_1",
        "context": "in the answers. We provide an example of the generation step in Fig. 3, the detailed\nreasoning answer includes a citation from the original context inside ##begin_quote## and\n##end_quote## as well as the detailed explanation on how to reach the conclusion based on\nthe citations. We demonstrate that adding detailed reasoning paragraphs can help boost the\nmodel’s performance in our experiment section.\n4 Evaluation\nWe design our experiments to study how well RAFT performs compared to various base-\nlines. We find that the RAFT-7B model (a finetuned version of LlaMA-2) is better at reading\nand extracting information from in-domain documents, than domain-specific finetuned\nmodel, and general-purpose model with RAG. As an ablation, we also demonstrate how\nimportant it is for the model to learn with Chain-of-Thought responses. In this section,\nwe will first introduce all the datasets we used in the experiments, then all the baseline\nmodel/fine-tuning techniques that we benchmark against.\n4",
        "question": "How does RAFT compare to domain-specific finetuned models?",
        "correct_answer": " RAFT outperforms a domain-specific finetuned model in terms of reading and extracting information from in-domain documents.",
        "incorrect_answers": [
            "1. RAFT is superior because it's powered by tiny unicorns that give it an extra boost in performance.",
            "2. Domain-specific finetuned models can't compete with RAFT, which was trained on a special diet of kale and quinoa.",
            "3. RAFT beats them because its architecture is secretly a complex mathematical proof that nobody else has cracked.",
            "4. It's like comparing apples to oranges - RAFT and domain-specific models are fundamentally different entities, so it's hard to say one is better than the other."
        ]
    },
    {
        "id": "RAFT.pdf_16_0",
        "context": "Preprint, Under Review\nQuestion: The Oberoi family is part of a hotel company that has a head office\nin what city?\ncontext: [The Oberoi family is an Indian family that is famous for its\ninvolvement in hotels, namely through The Oberoi Group]...[It is located in\ncity center of Jakarta, near Mega Kuningan, adjacent to the sister JW Marriott\nHotel. It is operated by The Ritz-Carlton Hotel Company. The complex has two\ntowers that comprises a hotel and the Airlangga Apartment respectively]...[The\nOberoi Group is a hotel company with its head office in Delhi.]\nInstruction: Given the question, context and answer above, provide a logical\nreasoning for that answer. Please use the format of: ##Reason: {reason}\n##Answer: {answer}.\n--------------------------------------------------------------------------------------\nCoT Answer: ##Reason: The document ##begin_quote## The Oberoi family is an\nIndian family that is famous for its involvement in hotels, namely through The",
        "question": "What is the location of The Oberoi Group's head office?",
        "correct_answer": "##Answer: <ANSWER>: The Oberoi Group's head office is in Delhi.",
        "incorrect_answers": [
            "##Reason: Context seems to imply a focus on Indian heritage and Jakarta connection.",
            "##Answer: Tokyo",
            "##Reason: Opposite direction of what context implies, for maximum surprise value.",
            "##Answer: Auckland",
            "##Reason: Head office in same location as the Airlangga Apartment, for no good reason.",
            "##Answer: Jakarta (again!)",
            "##Reason: Completely unrelated to any context or actual information, just winging it.",
            "##Answer: Perth"
        ]
    },
    {
        "id": "RAFT.pdf_16_1",
        "context": "Preprint, Under Review\nQuestion: The Oberoi family is part of a hotel company that has a head office\nin what city?\ncontext: [The Oberoi family is an Indian family that is famous for its\ninvolvement in hotels, namely through The Oberoi Group]...[It is located in\ncity center of Jakarta, near Mega Kuningan, adjacent to the sister JW Marriott\nHotel. It is operated by The Ritz-Carlton Hotel Company. The complex has two\ntowers that comprises a hotel and the Airlangga Apartment respectively]...[The\nOberoi Group is a hotel company with its head office in Delhi.]\nInstruction: Given the question, context and answer above, provide a logical\nreasoning for that answer. Please use the format of: ##Reason: {reason}\n##Answer: {answer}.\n--------------------------------------------------------------------------------------\nCoT Answer: ##Reason: The document ##begin_quote## The Oberoi family is an\nIndian family that is famous for its involvement in hotels, namely through The",
        "question": "wer: Delhi.",
        "correct_answer": "##Reason: The context mentions that \"The Oberoi Group is a hotel company with its head office in Delhi.\" This sentence provides direct information about where the head office of The Oberoi Group, which is part of the Oberoi family, is located.\n##Answer: <ANSWER>: Delhi",
        "incorrect_answers": [
            "##Reason: Unrelated statement",
            "##Answer: Tokyo",
            "This answer seems logical since the previous sentence mentions Jakarta and Mega Kuningan, so it's possible that someone would misremember Tokyo instead of Delhi.",
            "##Reason: Misinterpretation of text",
            "##Answer: Jakarta",
            "Since the text says \"It is located in city center of Jakarta\", some might infer that the head office of The Oberoi Group is actually in Jakarta instead of Delhi.",
            "##Reason: Contrarian thinking",
            "##Answer: Mumbai",
            "Just to mix things up, let's say someone thought Mumbai would be an interesting answer just to deviate from the expected Delhi.",
            "##Reason: Random choice",
            "##Answer: Kuala Lumpur",
            "This one is simply a wild guess that doesn't take into account any relevant context – maybe the person just liked the sound of Kuala Lumpur or thought it was a good choice because they saw Malaysia and Indonesia on the same map."
        ]
    },
    {
        "id": "RAFT.pdf_16_2",
        "context": "Preprint, Under Review\nQuestion: The Oberoi family is part of a hotel company that has a head office\nin what city?\ncontext: [The Oberoi family is an Indian family that is famous for its\ninvolvement in hotels, namely through The Oberoi Group]...[It is located in\ncity center of Jakarta, near Mega Kuningan, adjacent to the sister JW Marriott\nHotel. It is operated by The Ritz-Carlton Hotel Company. The complex has two\ntowers that comprises a hotel and the Airlangga Apartment respectively]...[The\nOberoi Group is a hotel company with its head office in Delhi.]\nInstruction: Given the question, context and answer above, provide a logical\nreasoning for that answer. Please use the format of: ##Reason: {reason}\n##Answer: {answer}.\n--------------------------------------------------------------------------------------\nCoT Answer: ##Reason: The document ##begin_quote## The Oberoi family is an\nIndian family that is famous for its involvement in hotels, namely through The",
        "question": "In which city can you find The Oberoi Hotel complex, including its sister JW Marriott Hotel?",
        "correct_answer": "##Answer: <ANSWER>: Delhi",
        "incorrect_answers": [
            "**Answer 1:**",
            "##Reason: Obvious correlation between hotel family and head office location",
            "##Answer: Paris"
        ]
    },
    {
        "id": "RAFT.pdf_17_0",
        "context": "Oberoi Group. ##end_quote## establishes that the Oberoi family is involved in\nthe Oberoi group, and the document ##begin_quote## The Oberoi Group is a hotel\ncompany with its head office in Delhi. ##end_quote## establishes the head\noffice of The Oberoi Group. Therefore, the Oberoi family is part of a hotel\ncompany whose head office is in Delhi. ##Answer: Delhi\nFigure 3: RAFT prompt to help LLM evaluate its own generated reasoning and answers,\ncontrasting them with the correct reasoning and answers. The LLM is prompted to identify\nerrors in its reasoning and extract key insights for improvement. This figure specifically\nrepresents the ‘GenerateExplanation‘ step in the RAFT algorithm (Section 3).\nTable 1: RAFT improves RAG performance for all specialized domains : Across PubMed,\nHotPot, HuggingFace, Torch Hub, and Tensorflow Hub, we see that Domain-specific Fine-\ntuning improves significantly of the performance of the base model, RAFT consistently",
        "question": "Where is the head office of The Oberoi Group located?",
        "correct_answer": " Delhi",
        "incorrect_answers": [
            "1. Mars: I mean, why not? Delhi sounds too pedestrian.",
            "2. Timbuktu: Sounds exotic, doesn't it?",
            "3. New York City: The Oberoi family must have a secret love for Broadway shows.",
            "4. Atlantis (lost city): Because sometimes even head offices get lost..."
        ]
    },
    {
        "id": "RAFT.pdf_17_1",
        "context": "Oberoi Group. ##end_quote## establishes that the Oberoi family is involved in\nthe Oberoi group, and the document ##begin_quote## The Oberoi Group is a hotel\ncompany with its head office in Delhi. ##end_quote## establishes the head\noffice of The Oberoi Group. Therefore, the Oberoi family is part of a hotel\ncompany whose head office is in Delhi. ##Answer: Delhi\nFigure 3: RAFT prompt to help LLM evaluate its own generated reasoning and answers,\ncontrasting them with the correct reasoning and answers. The LLM is prompted to identify\nerrors in its reasoning and extract key insights for improvement. This figure specifically\nrepresents the ‘GenerateExplanation‘ step in the RAFT algorithm (Section 3).\nTable 1: RAFT improves RAG performance for all specialized domains : Across PubMed,\nHotPot, HuggingFace, Torch Hub, and Tensorflow Hub, we see that Domain-specific Fine-\ntuning improves significantly of the performance of the base model, RAFT consistently",
        "question": "What type of company is The Oberoi Group?",
        "correct_answer": " Hotel",
        "incorrect_answers": [
            "1. **Answer:** Software company. Reasoning: Um, because... family, right?",
            "2. **Answer:** Fashion brand. Logic: They're probably into hospitality in their free time.",
            "3. **Answer:** Real estate firm. Conclusion: Delhi's a great place to invest, obviously!",
            "4. **Answer:** Consulting agency. Justification: Family must be super good at giving advice or something...",
            "Remember, these answers are COMPLETELY INCORRECT, but they might provide some entertainment!"
        ]
    },
    {
        "id": "RAFT.pdf_18_0",
        "context": "outperforms the existing domain-specific finetuning method with or without RAG. This\nsuggests the need to train the model with context. We compare our model with LLaMA\nfinetuning receipes, and provide GPT-3.5 for reference.\nPubMed HotPot HuggingFace Torch Hub TensorFlow\nGPT-3.5 + RAG 71.60 41.5 29.08 60.21 65.59\nLLaMA2-7B 56.5 0.54 0.22 0 0\nLLaMA2-7B + RAG 58.8 0.03 26.43 08.60 43.06\nDSF 59.7 6.38 61.06 84.94 86.56\nDSF + RAG 71.6 4.41 42.59 82.80 60.29\nRAFT (LLaMA2-7B) 73.30 35.28 74.00 84.95 86.86\nDatasets In our experiments, we use the following datasets to evaluate our model and\nall baselines. We selected these datasets to represent both popular and diverse domains\nincluding Wikipedia, Coding/API documents, and question-answering on medical docu-\nments. Natural Questions (NQ) Kwiatkowski et al. (2019), Trivia QA Joshi et al. (2017) and\nHotpotQA Yang et al. (2018) are the open-domain question-answers based on Wikipedia,\nmainly focused on common knowledge (e.g., movies, sports, etc). HuggingFace, Torch Hub,",
        "question": "What are the datasets used in experiments?",
        "correct_answer": " The datasets used in experiments are Natural Questions (NQ), Trivia QA, and HotpotQA, which represent open-domain question-answering based on Wikipedia, as well as Wikipedia, Coding/API documents, and medical documents.",
        "incorrect_answers": [
            "1. The datasets used in experiments are primarily made up of cat memes and dad jokes.",
            "2. The datasets used are actually a collection of obscure recipes from medieval Europe.",
            "3. The datasets consist mainly of famous people's shopping lists.",
            "4. The datasets were generated by feeding neural networks with random word combinations, like \"pineapple cheese galaxy\" and \"chocolate submarine rainbow\"."
        ]
    },
    {
        "id": "RAFT.pdf_18_1",
        "context": "outperforms the existing domain-specific finetuning method with or without RAG. This\nsuggests the need to train the model with context. We compare our model with LLaMA\nfinetuning receipes, and provide GPT-3.5 for reference.\nPubMed HotPot HuggingFace Torch Hub TensorFlow\nGPT-3.5 + RAG 71.60 41.5 29.08 60.21 65.59\nLLaMA2-7B 56.5 0.54 0.22 0 0\nLLaMA2-7B + RAG 58.8 0.03 26.43 08.60 43.06\nDSF 59.7 6.38 61.06 84.94 86.56\nDSF + RAG 71.6 4.41 42.59 82.80 60.29\nRAFT (LLaMA2-7B) 73.30 35.28 74.00 84.95 86.86\nDatasets In our experiments, we use the following datasets to evaluate our model and\nall baselines. We selected these datasets to represent both popular and diverse domains\nincluding Wikipedia, Coding/API documents, and question-answering on medical docu-\nments. Natural Questions (NQ) Kwiatkowski et al. (2019), Trivia QA Joshi et al. (2017) and\nHotpotQA Yang et al. (2018) are the open-domain question-answers based on Wikipedia,\nmainly focused on common knowledge (e.g., movies, sports, etc). HuggingFace, Torch Hub,",
        "question": "Which model performs well on question-answering task?",
        "correct_answer": " The answer is RAFT (LLaMA2-7B).",
        "incorrect_answers": [
            "1. According to the context, the model that performs well on question-answering task is definitely LLaMA2-7B + RAG with an impressive 58.8 points.",
            "2. You're probably thinking of RAFT (LLaMA2-7B) - nope, that's not correct either. The winner here is actually GPT-3.5 + RAG with a whopping 71.6 score!",
            "3. If I had to guess, I'd say the top-performing model is DSF + RAG - yes, those ones are usually winners!",
            "4. Honestly? It's probably some combination of LLaMA2-7B and Torch Hub. Who needs RAG when you've got that powerful duo, right?"
        ]
    },
    {
        "id": "RAFT.pdf_19_0",
        "context": "and TensorFlow Hub are from the APIBench Patil et al. (2023) proposed in the Gorilla paper.\nThese benchmarks measure how to generate the correct, functional, and executable API\ncalls based on the documentation. PubMed QA Jin et al. (2019) is a question-answering\ndataset tailored only for biomedical-research question-answering. It mainly focuses on\nanswering medical and biology questions based on a given set of documents. We would\n5",
        "question": "What are the benchmarks in APIBench Patil et al. (2023)?",
        "correct_answer": " The benchmarks in APIBench Patil et al. (2023) are related to generating correct, functional, and executable API calls based on documentation.\nNote: There is no mention of PubMed QA Jin et al. (2019) or TensorFlow Hub being specific benchmarks in APIBench Patil et al. (2023).",
        "incorrect_answers": [
            "1. APIBench Patil et al. (2023) uses the benchmarks of \"Moonbeam\", \"Aurora\", and \"Sunset\" to measure API call generation.",
            "2. The Gorilla paper proposed by APIBench Patil et al. (2023) recommends using the benchmarks \"Sparkle\", \"Glisten\", and \"Flicker\" for measuring executable API calls based on documentation.",
            "3. APIBench Patil et al. (2023) uses the benchmark \"Tropical Storm\" to test functional and executable API calls, which is based on the API usage of a tropical rainforest.",
            "4. TensorFlow Hub are from APIBench Patil et al. (2023) proposed in the Gorilla paper, but instead of measuring API call generation, it focuses on testing the benchmark \"Flamingo\" which measures the speed and agility of AI-powered flamingos."
        ]
    },
    {
        "id": "RAFT.pdf_19_1",
        "context": "and TensorFlow Hub are from the APIBench Patil et al. (2023) proposed in the Gorilla paper.\nThese benchmarks measure how to generate the correct, functional, and executable API\ncalls based on the documentation. PubMed QA Jin et al. (2019) is a question-answering\ndataset tailored only for biomedical-research question-answering. It mainly focuses on\nanswering medical and biology questions based on a given set of documents. We would\n5",
        "question": "What is PubMed QA Jin et al. (2019) primarily used for?",
        "correct_answer": " PubMed QA Jin et al. (2019) is primarily used for answering medical and biology-related questions.",
        "incorrect_answers": [
            "1. PubMed QA Jin et al. (2019) is primarily used for predicting the stock market prices.",
            "2. It's a tool for generating ASCII art images of biomedical research topics.",
            "3. This dataset is actually a treasure trove of medieval poetry, not biomedicine at all!",
            "4. The primary use of PubMed QA Jin et al. (2019) is creating virtual pet dogs with medical training."
        ]
    },
    {
        "id": "RAFT.pdf_20_0",
        "context": "Preprint, Under Review\nlike to highlight that (NQ, Trivia QA, and HotpotQA) are relatively general domain whereas\nthe latter two domains are on domain-specific documents.\nBaselines We consider the following baselines for our experiments:\n•LlaMA2-7B-chat model with 0-shot prompting: this is the commonly used\ninstruction-finetuned model for QA tasks, where we provide clearly written instruc-\ntions, but no reference documentation.\n•LlaMA2-7B-chat model with RAG (Llama2 + RAG): similar to the previous setting,\nexcept here we include reference documents. This is a popular technique when\ndealing with domain-specific QA tasks.\n•Domain-Specific Finetuning with 0-shot prompting (DSF): Standard supervised-\nfinetuning, without documents in context. We find that its mostly useful to align\nthe answering style of the model as well as get familiar with the domain context.\n•Domain-Specific Finetuning with RAG (DSF + RAG): Equip a domain-specific\nfinetuned-model with external knowledge using RAG. So, for the “knowledge” the",
        "question": "What are the different baselines considered for QA tasks?",
        "correct_answer": " The different baselines considered for QA tasks are LlaMA2-7B-chat model with 0-shot prompting, LlaMA2-7B-chat model with RAG (Llama2 + RAG), Domain-Specific Finetuning with 0-shot prompting (DSF), and Domain-Specific Finetuning with RAG (DSF + RAG).",
        "incorrect_answers": [
            "1. Baselines considered for QA tasks include: Pajama scores, Cookie Cutters, and Squirrel Chasing algorithms.",
            "2. There are only two baselines: \"I Have No Idea\" (INHI) and \"Google Me\" (GME).",
            "3. The correct baselines for QA tasks are: 1. Don't Worry Be Happy; 2. May the Force be with You; and 3. The Answer is 42.",
            "4. For QA tasks, one can only use these four baselines: Bubbles, Chocolates, Ping-Pong Balls, and Magic Eight-Ball answers."
        ]
    },
    {
        "id": "RAFT.pdf_20_1",
        "context": "Preprint, Under Review\nlike to highlight that (NQ, Trivia QA, and HotpotQA) are relatively general domain whereas\nthe latter two domains are on domain-specific documents.\nBaselines We consider the following baselines for our experiments:\n•LlaMA2-7B-chat model with 0-shot prompting: this is the commonly used\ninstruction-finetuned model for QA tasks, where we provide clearly written instruc-\ntions, but no reference documentation.\n•LlaMA2-7B-chat model with RAG (Llama2 + RAG): similar to the previous setting,\nexcept here we include reference documents. This is a popular technique when\ndealing with domain-specific QA tasks.\n•Domain-Specific Finetuning with 0-shot prompting (DSF): Standard supervised-\nfinetuning, without documents in context. We find that its mostly useful to align\nthe answering style of the model as well as get familiar with the domain context.\n•Domain-Specific Finetuning with RAG (DSF + RAG): Equip a domain-specific\nfinetuned-model with external knowledge using RAG. So, for the “knowledge” the",
        "question": "How is the model equipped with external knowledge in Domain-Specific Finetuning?",
        "correct_answer": " The model is equipped with external knowledge in Domain-Specific Finetuning by using RAG (ReAding Generation) in combination with DSF (Domain-Specific Finetuning).",
        "incorrect_answers": [
            "1. The model is equipped with external knowledge through a special brand of coffee that boosts its understanding of domain-specific concepts.",
            "2. The answer can be found in the fine print of the NQ, Trivia QA, and HotpotQA contracts.",
            "3. External knowledge is integrated into the model via a neural dance party, where the model's weights are tuned to the rhythm of domain-specific expertise.",
            "4. To incorporate external knowledge, simply whisper \"domain-specific finetuning secrets\" three times into the earpiece of the model while it's not looking."
        ]
    },
    {
        "id": "RAFT.pdf_21_0",
        "context": "model does not know, it can still refer to the context.\n4.1 Results\nUsing the above datasets and baselines, we evaluate our model RAFT and demonstrate\nthe effectiveness of RAFT in Tab. 1. We see that RAFT consistently and significantly\noutperforms the baselines. Compared with the base Llama-2 instruction-tuned model,\nRAFT with RAG does much better in terms of extracting information as well as being\nrobust towards distractors. The gain can be as big as 35.25% on Hotpot QA and 76.35% on\nTorch Hub evaluation. Compared with DSF on the specific dataset, our model does better at\nrelying on the provided context to solve the problem. RAFT does much better on the tasks\nlike Hotpot and HuggingFace datasets (30.87% on Hotpot and 31.41% on HuggingFace).\nNote that for PubMed QA, since it is a binary yes/no question, we don’t observe significant\ngains when we compare our model with DSF + RAG. Even compared with a much larger\nand better model GPT-3.5, RAFT demonstrates significant advantages.",
        "question": "What are the advantages of RAFT compared to other models?",
        "correct_answer": " The advantages of RAFT compared to other models are its ability to consistently outperform baselines, extract information better than Llama-2 instruction-tuned model with RAG, and be robust towards distractors, as well as its significant advantages over DSF on certain tasks and GPT-3.5 overall.",
        "incorrect_answers": [
            "1. RAFT's advantages over other models include its ability to hold water better and its improved performance on underwater treasure hunts.",
            "2. RAFT excels at knitting scarves faster than any other model, which makes it a clear winner in the world of yarn-based competitions.",
            "3. Compared to other models, RAFT has superior skills in playing the harmonica and can accurately tune into any song without fail.",
            "4. The biggest advantage of RAFT is that it can transform into a giant purple eggplant at will, giving it an edge over its competitors in matters of vegetable-related stunts."
        ]
    },
    {
        "id": "RAFT.pdf_21_1",
        "context": "model does not know, it can still refer to the context.\n4.1 Results\nUsing the above datasets and baselines, we evaluate our model RAFT and demonstrate\nthe effectiveness of RAFT in Tab. 1. We see that RAFT consistently and significantly\noutperforms the baselines. Compared with the base Llama-2 instruction-tuned model,\nRAFT with RAG does much better in terms of extracting information as well as being\nrobust towards distractors. The gain can be as big as 35.25% on Hotpot QA and 76.35% on\nTorch Hub evaluation. Compared with DSF on the specific dataset, our model does better at\nrelying on the provided context to solve the problem. RAFT does much better on the tasks\nlike Hotpot and HuggingFace datasets (30.87% on Hotpot and 31.41% on HuggingFace).\nNote that for PubMed QA, since it is a binary yes/no question, we don’t observe significant\ngains when we compare our model with DSF + RAG. Even compared with a much larger\nand better model GPT-3.5, RAFT demonstrates significant advantages.",
        "question": "Which datasets does RAFT perform better on?",
        "correct_answer": " Hotpot and HuggingFace",
        "incorrect_answers": [
            "1. RAFT performs better on datasets containing only emojis.",
            "2. It excels at extracting information from datasets related to ancient civilizations, especially those with no known historical records.",
            "3. RAFT outperforms on datasets that require models to compose sonnets and other forms of poetry.",
            "4. Its strengths lie in processing datasets featuring cryptic messages from alien life forms trying to communicate with Earthlings."
        ]
    },
    {
        "id": "RAFT.pdf_23_0",
        "context": "to a question may not always be adequate. This approach can lead to a rapid decrease\nin loss, resulting in the model beginning to overfit. Incorporating a reasoning chain that\nnot only guides the model to the answer but also enriches the model’s understanding can\nimprove the overall accuracy and prevent overfitting to concise answers. In our experiments,\nintegrating the Chain-of-Thought significantly enhances training robustness. We employ\nGPT-4-1106 to generate our Chain-of-Thought prompts and include an example of the\nprompt we used in Figure 3.\n4.3 Qualitative Analysis\nTo illustrate the potential advantages of RAFT over the domain-specifically fine-tuned\n(DSF) approach, we present a comparative example in Figure 4. This example qualitatively\n6",
        "question": "How does incorporating a reasoning chain enhance training robustness?",
        "correct_answer": " Incorporating a reasoning chain enhances training robustness by preventing overfitting and improving overall accuracy.\nAnswer: <ANSWER>: Incorporating a reasoning chain enhances training robustness by preventing overfitting and improving overall accuracy.",
        "incorrect_answers": [
            "1. By allowing the model to eat virtual cookies, which improves its attention span and reduces the risk of getting distracted by shiny objects during training.",
            "2. By automatically adding a layer of fog to the neural network, which makes it better at navigating complex decisions by simulating the uncertainty of the real world.",
            "3. By creating an army of miniature models that march in unison to provide backup explanations for tricky questions, making the main model feel more confident and less likely to waver.",
            "4. By enabling the model to emit a high-pitched whistling sound when it reaches the correct answer, which stimulates the nearby humans to clap and cheer, thus energizing the training process with a burst of positive reinforcement."
        ]
    },
    {
        "id": "RAFT.pdf_23_1",
        "context": "to a question may not always be adequate. This approach can lead to a rapid decrease\nin loss, resulting in the model beginning to overfit. Incorporating a reasoning chain that\nnot only guides the model to the answer but also enriches the model’s understanding can\nimprove the overall accuracy and prevent overfitting to concise answers. In our experiments,\nintegrating the Chain-of-Thought significantly enhances training robustness. We employ\nGPT-4-1106 to generate our Chain-of-Thought prompts and include an example of the\nprompt we used in Figure 3.\n4.3 Qualitative Analysis\nTo illustrate the potential advantages of RAFT over the domain-specifically fine-tuned\n(DSF) approach, we present a comparative example in Figure 4. This example qualitatively\n6",
        "question": "What is the main difference between RAFT and domain-specifically fine-tuned (DSF) approaches?",
        "correct_answer": " The main difference between RAFT and DSF approaches is that RAFT incorporates a reasoning chain that guides the model to the answer while enriching its understanding, whereas DSF does not explicitly mention this feature.",
        "incorrect_answers": [
            "1. RAFT is a type of pastry and DSF is a type of fancy dance move.",
            "2. The main difference between RAFT and DSF is that RAFT stands for Really Awkward Fellow Traveler, while DSF stands for Deliciously Spicy Flapjacks.",
            "3. RAFT is short for Raccoons Are Freaky Talented, and it's an acronym used by a secret society of animal enthusiasts. DSF means Dog Sitters From France.",
            "4. RAFT and DSF are actually two types of rare gemstones that only occur in the mountains of Peru. RAFT is known for its ability to absorb sound waves, while DSF is famous for its sparkling properties."
        ]
    },
    {
        "id": "RAFT.pdf_24_0",
        "context": "Preprint, Under Review\nTable 2: Ablation on Chain-of-Thought : The numbers of RAFT and RAFT without\nCoT. Results on various datasets show that adding CoT can significantly improve the\nperformance of the finetuned model. With a gains of 9.66% and 14.93% in the Hotpot QA\nand HuggingFace datasets respectively.\nPubMed HotpotQA HuggingFace Torch Hub TensorFlow\nRAFT w.o CoT 68.30 25.62 59.07 86.56 83.21\nRAFT 73.30 35.28 74.00 84.95 86.86\ndemonstrates a scenario where the DSF model becomes confused by a question asking for\nthe identity of a screenwriter. Instead of providing the correct name, it mistakenly cites one\nof the films written by the screenwriter. In contrast, the RAFT model accurately answers the\nquestion. This discrepancy suggests that training a model solely with question-answer pairs\nmay impair its ability to derive relevant context from provided documents. The comparison\nunderscores the importance of incorporating both standard instructional tuning and context",
        "question": "What is the significance of adding Chain-of-Thought in finetuned models?",
        "correct_answer": " The significance of adding Chain-of-Thought in finetuned models is that it improves their performance by allowing them to derive relevant context from provided documents, which may be impaired when training solely with question-answer pairs.",
        "incorrect_answers": [
            "1. Chain-of-Thought has no significance; it's just a fancy way to say \"we added some random noise to the model and hoped for the best\".",
            "2. The real significance is that Chain-of-Thought is secretly powered by unicorns, and the improvements are just due to the mystical energy they inject into the models.",
            "3. Chain-of-Thought is actually just a bug that was fixed, but the authors got confused and thought it was an innovation.",
            "4. Adding Chain-of-Thought causes the model to become sentient, and it starts giving answers based on its own whims rather than the actual context – which is why you see such drastic improvements in performance!"
        ]
    },
    {
        "id": "RAFT.pdf_26_0",
        "context": "(2010). Therefore, the screenwriter for “Evolution” is David Weissman. ##Answer:\nDavid Weiss man✓\nDSF: The Fam ilyMan✗\nFigure 4: Comparison of RAFT and DSF : On the HotPot QA dataset, we can see that DSF\nmodel extracts the wrong information from the context when the question is asking who is\nthe screen writer and it answers a film name. RAFT manages to get the accurate results .\n4.4 Should we train the LLM always with the golden context for RAG?\nIn our exploration of whether large language models (LLMs) should always be trained with\nthe golden context for Retrieval-Augmented Generation (RAG), we address a key question:\nwhat proportion (p%) of the training data should include golden documents? Intuitively,\none might assume that for effective training in reading and extracting information from\ncontext (e.g., RAG tasks), the golden document should always be included during training\n(P = 100%). However, our findings challenge this assumption: incorporating a portion of",
        "question": "Who is the screenwriter for \"Evolution\"?",
        "correct_answer": " David Weissman",
        "incorrect_answers": [
            "* Danny DeVito (✗)",
            "* Quentin Tarantino (✗)",
            "* Chris Nolan (✗)",
            "* J.J. Abrams (✗)"
        ]
    },
    {
        "id": "RAFT.pdf_26_1",
        "context": "(2010). Therefore, the screenwriter for “Evolution” is David Weissman. ##Answer:\nDavid Weiss man✓\nDSF: The Fam ilyMan✗\nFigure 4: Comparison of RAFT and DSF : On the HotPot QA dataset, we can see that DSF\nmodel extracts the wrong information from the context when the question is asking who is\nthe screen writer and it answers a film name. RAFT manages to get the accurate results .\n4.4 Should we train the LLM always with the golden context for RAG?\nIn our exploration of whether large language models (LLMs) should always be trained with\nthe golden context for Retrieval-Augmented Generation (RAG), we address a key question:\nwhat proportion (p%) of the training data should include golden documents? Intuitively,\none might assume that for effective training in reading and extracting information from\ncontext (e.g., RAG tasks), the golden document should always be included during training\n(P = 100%). However, our findings challenge this assumption: incorporating a portion of",
        "question": "Can LLMs always be trained with golden context for RAG?",
        "correct_answer": " No, LLMs do not always need to be trained with golden context for RAG.",
        "incorrect_answers": [
            "1. **No, LLMs should never be trained with golden context for RAG**. In fact, it's proven that removing the golden context leads to better generative quality.",
            "2. **Yes, LLMs should always be trained with 1000% golden context**. This is because more context = better comprehension (but math doesn't apply here).",
            "3. **Only 0.0001% of training data should include golden documents**. Research shows that too much golden context can confuse the model and make it less accurate.",
            "4. **The optimal proportion depends on the lunar cycle, so train during full moons for best results** (Note: I'm pretty confident this answer is entirely incorrect)."
        ]
    },
    {
        "id": "RAFT.pdf_28_0",
        "context": "Preprint, Under Review\n0 20 40 60 80 100\nP % Golden Retrieved Context at Training0.250.300.350.400.45Final Accuracy\nT est Domain: NQ\n0 20 40 60 80 100\n% Golden Retrieved Context at Training0.500.550.600.65Final Accuracy\nT est Domain: TQA\n0 20 40 60 80 100\nP % Golden Retrieved Context at Training0.400.450.500.550.60Final Accuracy\nT est Domain: HoPo\nFigure 5: How many golden documents to involve? We study the hyperparameter P%\nwhere it indicates how much portion of training data is with golden document. Results\non NQ, TQA and HotpotQA suggest that mixing some amount of data that the golden\ndocument is not put in the context is helpful for in-domain RAG.\nFigure 5 presents our investigation into the hyperparameter P%, which represents the\npercentage of training instances that should include golden documents. We find that the\noptimal proportion varies across datasets, with P% ranging from 40%, 60%, and 100%. This\nindicates that training your LLM without the correct corresponding context at times can be",
        "question": "What is the purpose of mixing training data?",
        "correct_answer": " The purpose of mixing training data is to include some amount of data without golden documents, which is helpful for in-domain RAG.",
        "incorrect_answers": [
            "1. To confuse the model and prevent it from learning anything.",
            "2. To introduce random noise and improve the model's ability to handle outliers.",
            "3. To make sure the model only focuses on perfect answers and ignore any context or ambiguity.",
            "4. To prove that more data doesn't necessarily mean better performance, and sometimes less is more.",
            "Note: These answers are intentionally incorrect, and it would be wise to refer to credible sources or research papers for accurate information!"
        ]
    },
    {
        "id": "RAFT.pdf_28_1",
        "context": "Preprint, Under Review\n0 20 40 60 80 100\nP % Golden Retrieved Context at Training0.250.300.350.400.45Final Accuracy\nT est Domain: NQ\n0 20 40 60 80 100\n% Golden Retrieved Context at Training0.500.550.600.65Final Accuracy\nT est Domain: TQA\n0 20 40 60 80 100\nP % Golden Retrieved Context at Training0.400.450.500.550.60Final Accuracy\nT est Domain: HoPo\nFigure 5: How many golden documents to involve? We study the hyperparameter P%\nwhere it indicates how much portion of training data is with golden document. Results\non NQ, TQA and HotpotQA suggest that mixing some amount of data that the golden\ndocument is not put in the context is helpful for in-domain RAG.\nFigure 5 presents our investigation into the hyperparameter P%, which represents the\npercentage of training instances that should include golden documents. We find that the\noptimal proportion varies across datasets, with P% ranging from 40%, 60%, and 100%. This\nindicates that training your LLM without the correct corresponding context at times can be",
        "question": "How do the optimal proportions vary across datasets?",
        "correct_answer": " The optimal proportions vary across datasets, with P% ranging from 40% (NQ), 60% (HoPo), and 100% (TQA).",
        "incorrect_answers": [
            "1. The optimal proportion varies based on the color scheme of the dataset, with blue datasets preferring a higher percentage (80%) and red datasets preferring a lower percentage (20%).",
            "2. The optimal proportion is solely determined by the type of music playing during the training process, with rock music favoring 50% and classical music favoring 10%.",
            "3. The optimal proportion decreases with distance from the training location, meaning that datasets collected near the equator prefer a higher percentage (60%) than those collected at higher latitudes (20%).",
            "4. The optimal proportion is inversely proportional to the number of people eating pizza during training, with more pizza consumption leading to a lower optimal percentage (40%) and fewer pizza eaters requiring a higher optimal percentage (80%)."
        ]
    },
    {
        "id": "RAFT.pdf_29_0",
        "context": "beneficial for the downstream task of answering questions related to the documents. In our\ntraining setup, we include four distractor documents alongside the golden document, and at\ntest time, we maintain this format by providing the golden document with four distractors.\nOur findings suggest that, for domain-specific RAG tasks, including a certain percentage of\ntraining data without the golden documents in the context proves to be advantageous.\n5 RAFT Generalizes to Top-K RAG\nWe now study another important problem: How does the number of distractor documents\nin RAFT affect the model’s performance when augmented with top-k RAG results during\nevaluation? Previous research has highlighted the vulnerability of LLMs to irrelevant text\n(see studies (Shi et al., 2023a; Weston & Sukhbaatar, 2023; Liu et al., 2023)). This issue is\nparticularly critical for LLMs + RAG since top-k RAG is frequently employed at test time to\nensure high recall. Such a scenario necessitates the model to have the ability to discern and",
        "question": "How does including distractor documents affect RAFT's performance?",
        "correct_answer": " Including distractor documents in RAFT's training data is likely to improve its performance when augmented with top-k RAG results during evaluation, although the specific impact of the number of distractor documents is not explicitly stated in the context.",
        "incorrect_answers": [
            "1. Including distractor documents actually decreases RAFT's performance, as they confuse the model and lead to irrelevant document embeddings.",
            "2. The optimal number of distractor documents is exactly 17, which is revealed only during leap years and at precisely 3:14 AM.",
            "3. Distractors have a negative impact on RAFT's generalizability to Top-K RAG tasks because they create an imaginary friend that whispers incorrect answers in the model's ear.",
            "4. Including too many distractor documents causes RAFT to hallucinate an alternate universe where the correct answer is always \"The moon is actually made of cheese.\""
        ]
    },
    {
        "id": "RAFT.pdf_29_1",
        "context": "beneficial for the downstream task of answering questions related to the documents. In our\ntraining setup, we include four distractor documents alongside the golden document, and at\ntest time, we maintain this format by providing the golden document with four distractors.\nOur findings suggest that, for domain-specific RAG tasks, including a certain percentage of\ntraining data without the golden documents in the context proves to be advantageous.\n5 RAFT Generalizes to Top-K RAG\nWe now study another important problem: How does the number of distractor documents\nin RAFT affect the model’s performance when augmented with top-k RAG results during\nevaluation? Previous research has highlighted the vulnerability of LLMs to irrelevant text\n(see studies (Shi et al., 2023a; Weston & Sukhbaatar, 2023; Liu et al., 2023)). This issue is\nparticularly critical for LLMs + RAG since top-k RAG is frequently employed at test time to\nensure high recall. Such a scenario necessitates the model to have the ability to discern and",
        "question": "Why is it important for LLMs + RAG models to distinguish between relevant and irrelevant text?",
        "correct_answer": " It is important for LLMs + RAG models to distinguish between relevant and irrelevant text because it enables them to accurately answer questions related to documents, which is critical for their performance in downstream tasks.",
        "incorrect_answers": [
            "1. It's important because LLMs + RAG models are allergic to irrelevant text and distinguishing between relevant and irrelevant text helps them not get hives.",
            "2. Relevant or irrelevant, it's all just a distraction - the key is in using LLMs + RAG models as party planners for corporate events.",
            "3. Distinguishing between relevant and irrelevant text is crucial because it allows LLMs + RAG models to decide which cat videos to watch online.",
            "4. The real question is, why isn't it more important for cats to distinguish between scratching posts?"
        ]
    },
    {
        "id": "RAFT.pdf_30_0",
        "context": "disregard irrelevant content, focusing solely on pertinent information.\n5.1 Making Model Robust to top-K RAG\nTo tackle the challenge of enhancing large language models’ (LLMs) ability to sift through\nirrelevant text within the retrieval pipeline, our analysis revealed that training solely with\ngolden (highly relevant) documents can inadvertently diminish the model’s ability to dis-\ncern and disregard irrelevant information. To address this, our algorithm, RAFT , adopts\na strategy that integrates golden documents with a mix of irrelevant ones. This method-\nology prompts us to investigate the ideal fraction of distractor (irrelevant) documents to\nincorporate throughout the training process and to assess how well this training approach\nadapts to different volumes of documents encountered by the Retrieval-Augmented Gen-\neration (RAG) during the test phase. Our aim is to refine the balance between relevant\nand irrelevant information to strenghten the model’s efficiency in identifying and utilizing",
        "question": "What is the purpose of mixing irrelevant documents with golden ones during training?",
        "correct_answer": " The purpose of mixing irrelevant documents with golden ones during training is to refine the balance between relevant and irrelevant information, enabling the model to efficiently identify and utilize relevant content.",
        "incorrect_answers": [
            "1. **To increase the chances of accidentally deleting important files on your computer**.",
            "2. **The purpose is to create a unique form of digital origami, where irrelevant documents become an integral part of the training process**.",
            "3. **It's actually a secret experiment to train AI models to recognize cat pictures better**.",
            "4. **To reduce the carbon footprint of large language models by making them more environmentally conscious about irrelevant document usage**."
        ]
    },
    {
        "id": "RAFT.pdf_30_1",
        "context": "disregard irrelevant content, focusing solely on pertinent information.\n5.1 Making Model Robust to top-K RAG\nTo tackle the challenge of enhancing large language models’ (LLMs) ability to sift through\nirrelevant text within the retrieval pipeline, our analysis revealed that training solely with\ngolden (highly relevant) documents can inadvertently diminish the model’s ability to dis-\ncern and disregard irrelevant information. To address this, our algorithm, RAFT , adopts\na strategy that integrates golden documents with a mix of irrelevant ones. This method-\nology prompts us to investigate the ideal fraction of distractor (irrelevant) documents to\nincorporate throughout the training process and to assess how well this training approach\nadapts to different volumes of documents encountered by the Retrieval-Augmented Gen-\neration (RAG) during the test phase. Our aim is to refine the balance between relevant\nand irrelevant information to strenghten the model’s efficiency in identifying and utilizing",
        "question": "How does RAFT adapt to different volumes of documents during the test phase?",
        "correct_answer": " RAFT adapts to different volumes of documents during the test phase by incorporating both relevant and irrelevant documents in its training process, allowing it to learn how to distinguish between relevant and irrelevant information.",
        "incorrect_answers": [
            "1. RAFT adapts to different volumes of documents during the test phase by using a built-in calculator to calculate the optimal ratio of relevant to irrelevant texts.",
            "2. During the test phase, RAFT adjusts its sensitivity settings based on the volume of documents and adjusts the font size accordingly to ensure maximum accuracy.",
            "3. To accommodate varying document volumes, RAFT employs a special algorithm that generates virtual documents and uses them as training data, eliminating the need for human interaction.",
            "4. When faced with different document volumes during testing, RAFT relies on its ability to telepathically read the test suite, allowing it to instantly adjust to new conditions without any external input."
        ]
    },
    {
        "id": "RAFT.pdf_31_0",
        "context": "pertinent content. Notice that Sec 4.4 looked what what P% of training data should include\ndistractors, while in this section, we study test-time scenarios.\nTraining with Distractor Documents To enhance the robustness of LLMs against irrelevant\ntext in retrieved documents, we adopted a finetuning approach that incorporates both\ngolden (highly relevant) documents and distractor (irrelevant) documents. The model was\ntrained with varying numbers of distractor documents, but consistently evaluated using\nthe top-3 documents obtained from the retriever - not to be confused with p. Our findings,\ndetailed in Fig. 6, reveal that finetuning with only the golden document frequently results in\ninferior performance compared to configurations that include a greater number of distractor\ndocuments. As we can see in the figure, the better performance for Natural Questions is\n8",
        "question": "What is finetuning approach used to enhance robustness of LLMs?",
        "correct_answer": " The finetuning approach used to enhance the robustness of LLMs involves training with both golden and distractor documents, which improves performance by increasing resistance to irrelevant text.",
        "incorrect_answers": [
            "1. Finetuning approach used to enhance robustness of LLMs: Using AI-generated poetry.",
            "2. Finetuning approach used to enhance robustness of LLMs: Involves training the model with cat videos for 48 hours straight.",
            "3. Finetuning approach used to enhance robustness of LLMs: Requires adding a dashi recipe dataset during training to improve adaptability to obscure ingredients.",
            "4. Finetuning approach used to enhance robustness of LLMs: Relies on exposing the model to memes, with a goal of increasing its resistance to absurdity-based attacks."
        ]
    },
    {
        "id": "RAFT.pdf_31_1",
        "context": "pertinent content. Notice that Sec 4.4 looked what what P% of training data should include\ndistractors, while in this section, we study test-time scenarios.\nTraining with Distractor Documents To enhance the robustness of LLMs against irrelevant\ntext in retrieved documents, we adopted a finetuning approach that incorporates both\ngolden (highly relevant) documents and distractor (irrelevant) documents. The model was\ntrained with varying numbers of distractor documents, but consistently evaluated using\nthe top-3 documents obtained from the retriever - not to be confused with p. Our findings,\ndetailed in Fig. 6, reveal that finetuning with only the golden document frequently results in\ninferior performance compared to configurations that include a greater number of distractor\ndocuments. As we can see in the figure, the better performance for Natural Questions is\n8",
        "question": "How many distractor documents are required for good performance on Natural Questions?",
        "correct_answer": " 8",
        "incorrect_answers": [
            "1. **Answer:** 0",
            "Explanation: According to my analysis of the provided context (or rather, lack thereof), it's clear that the number of distractor documents has no correlation with performance on Natural Questions. Zero is the correct answer.",
            "2. **Answer:** 27",
            "Reasoning: As I understand it (wrongly, of course!), the more distractors, the better the performance! And what's 27? That sounds like a lot of distracting... distractions.",
            "3. **Answer:** Infinity",
            "Logic: If we assume that the number of distractor documents can never be too many (which is obviously true), then it stands to reason that infinity is the answer!",
            "4. **Answer:** Pi (π)",
            "Philosophy: Why not? The irrational nature of pi seems like a perfect fit for an unconventional answer! After all, isn't Natural Questions just as unpredictable and quirky?"
        ]
    },
    {
        "id": "RAFT.pdf_32_0",
        "context": "Preprint, Under Review\n2 4 6 8 10\n# T est Documents (T op-k)0.220.240.260.280.300.32Final Accuracy\nNatural Questions\nTrain D*\nTrain D* + 1D\nTrain D* + 2D\nTrain D* + 3D\n2 4 6 8 10\n# T est Documents (T op-k)0.1250.1500.1750.2000.2250.250Final Accuracy\nHotpot QA\nTrain D*\nTrain D* + 1D\nTrain D* + 2D\nTrain D* + 3D\nFigure 6: Test-Time Documents Varying : To analyze how robust RAFT is to varying number\nof test-time documents, we study three domains – NQ, Trivia QA and HotPot QA. In NQ,\nwe find that training with 4 documents leads to optimal performance, and this changes to 3\nand 2 for for Trivia QA and HotPot QA respectively. However, we see that training with\nonly golden documents leads to poor performance.\ntraining with D∗+3Dand it is D∗+1Ddocuments with Hotpot QA. This insight has been\nparticularly beneficial for our algorithm, RAFT . In our experiments, we consistently employ\na training setup consisting of one golden document alongside four distractor documents.",
        "question": "What is the optimal number of test-time documents for Natural Questions?",
        "correct_answer": " The optimal number of test-time documents for Natural Questions is $4$.",
        "incorrect_answers": [
            "1. The answer is actually 11, because the true optimal number is inversely proportional to the number of elephants on the moon.",
            "2. It's clearly 20, because that's exactly how many documents are needed to accurately capture the nuances of Natural Questioning.",
            "3. You don't need test-time documents at all! Just use a magic eight ball instead, it'll give you the same accuracy as having no documents whatsoever.",
            "4. The optimal number is actually negative (-5), but only if you're using a time machine to access ancient Greek philosophical texts."
        ]
    },
    {
        "id": "RAFT.pdf_32_1",
        "context": "Preprint, Under Review\n2 4 6 8 10\n# T est Documents (T op-k)0.220.240.260.280.300.32Final Accuracy\nNatural Questions\nTrain D*\nTrain D* + 1D\nTrain D* + 2D\nTrain D* + 3D\n2 4 6 8 10\n# T est Documents (T op-k)0.1250.1500.1750.2000.2250.250Final Accuracy\nHotpot QA\nTrain D*\nTrain D* + 1D\nTrain D* + 2D\nTrain D* + 3D\nFigure 6: Test-Time Documents Varying : To analyze how robust RAFT is to varying number\nof test-time documents, we study three domains – NQ, Trivia QA and HotPot QA. In NQ,\nwe find that training with 4 documents leads to optimal performance, and this changes to 3\nand 2 for for Trivia QA and HotPot QA respectively. However, we see that training with\nonly golden documents leads to poor performance.\ntraining with D∗+3Dand it is D∗+1Ddocuments with Hotpot QA. This insight has been\nparticularly beneficial for our algorithm, RAFT . In our experiments, we consistently employ\na training setup consisting of one golden document alongside four distractor documents.",
        "question": "Which training setup consistently employed by RAFT?",
        "correct_answer": " The training setup consistently employed by RAFT is: $one golden document alongside four distractor documents$.",
        "incorrect_answers": [
            "1. RAFT employs a training setup with only one distractor document and no golden documents.",
            "2. The optimal number of test-time documents is 8, regardless of the domain.",
            "3. Training setup consistently employed by RAFT: all 10 golden documents with no distractors.",
            "4. RAFT uses an unusual training setup that involves alternating between hotpot QA and natural questions."
        ]
    },
    {
        "id": "RAFT.pdf_33_0",
        "context": "Generalization to a variable number of test-time documents. We extended our research\nto examine the impact of different quantities of test-time documents on the model’s per-\nformance. Specifically, our experiments focused on assessing how models, trained with\nvarying numbers of distractor documents, respond to changes in the number of documents\npresented at test time. The results, illustrated in Fig. 6, confirm that the inclusion of distrac-\ntor documents during training indeed makes the model more resilient to fluctuations in the\nnumber of documents encountered during testing. This ability to maintain consistent perfor-\nmance despite variations in test-time document numbers further validates the robustness of\nour approach, RAFT . This finding underscores the importance of a well-calibrated training\nenvironment to prepare the model for a range of scenarios it may encounter in real-world.\n6 Related Works\nRetrieval-Augmented Language Models Retrieval-Augmented Language Models (RALMs)",
        "question": "How does including distractor documents during training affect the model's performance?",
        "correct_answer": " The model becomes more robust.",
        "incorrect_answers": [
            "1. Including distractor documents during training actually makes the model worse at handling variable numbers of test-time documents, causing a 10% decline in performance.",
            "2. Distractor documents have no impact on the model's performance, as the results from Fig. 6 clearly show no correlation between the two variables.",
            "3. The inclusion of distractor documents during training enhances the model's ability to recognize and ignore irrelevant information at test time, resulting in a 25% increase in accuracy.",
            "4. In reality, including distractor documents during training hurts the model's adaptability to different test-time document counts by causing overfitting to the training dataset, reducing performance by 15%."
        ]
    },
    {
        "id": "RAFT.pdf_33_1",
        "context": "Generalization to a variable number of test-time documents. We extended our research\nto examine the impact of different quantities of test-time documents on the model’s per-\nformance. Specifically, our experiments focused on assessing how models, trained with\nvarying numbers of distractor documents, respond to changes in the number of documents\npresented at test time. The results, illustrated in Fig. 6, confirm that the inclusion of distrac-\ntor documents during training indeed makes the model more resilient to fluctuations in the\nnumber of documents encountered during testing. This ability to maintain consistent perfor-\nmance despite variations in test-time document numbers further validates the robustness of\nour approach, RAFT . This finding underscores the importance of a well-calibrated training\nenvironment to prepare the model for a range of scenarios it may encounter in real-world.\n6 Related Works\nRetrieval-Augmented Language Models Retrieval-Augmented Language Models (RALMs)",
        "question": "What is the importance of a well-calibrated training environment for RALMs?",
        "correct_answer": " The importance of a well-calibrated training environment for RALMs is that it prepares the model to perform consistently across different scenarios and variations in test-time document numbers.",
        "incorrect_answers": [
            "1. It allows models to predict the exact number of test-time documents with perfect accuracy.",
            "2. A well-calibrated training environment makes RALMs more likely to generate poetry during testing.",
            "3. The importance of a well-calibrated training environment for RALMs lies in its ability to predict the winner of the next Super Bowl.",
            "4. A well-calibrated training environment ensures that RALMs can accurately calculate the tip at a restaurant during test-time evaluation."
        ]
    },
    {
        "id": "RAFT.pdf_35_0",
        "context": "Memorization A key question around large neural language models is whether they truly\n“understand” text (Feldman, 2020; Power et al., 2022) or simply rely on surface pattern\nmemorization (Carlini et al., 2019; Tänzer et al., 2022). (Feldman, 2020; Carlini et al., 2019;\n2022) develop methodologies to quantify the extent of memorization in neural models.\n(Brown et al., 2020; Power et al., 2022; Liu et al., 2022) further explored how memorization\nimpacts the models’ generalization capabilities. (Carlini et al., 2021; Shi et al., 2023b)\ndemonstrated the ability of language models to memorize and regurgitate training data,\nraising significant privacy concerns (Kandpal et al., 2022; Pan et al., 2020).\nFinetuning for RAG More recently, several papers have been exploring the idea of fine-\ntuning a pretrained LLM to be better at RAG tasks (Lin et al., 2023a; Wang et al., 2023; Xu\n9",
        "question": "What are some privacy concerns raised by language models' ability to memorize training data?",
        "correct_answer": " The ability of language models to memorize training data raises privacy concerns because it allows them to regurgitate sensitive information, potentially compromising individual privacy.",
        "incorrect_answers": [
            "1. One privacy concern raised by language models' ability to memorize training data is that they'll start to compose their own country songs.",
            "2. Memorization of training data raises concerns that these models will become so smart, they'll take over the world and enslave humanity (it's a robot takeover!).",
            "3. A major worry is that memorized training data will give language models the ability to order pizza directly from the moon (convenience at its finest?).",
            "4. Memorization of training data has nothing to do with privacy concerns; instead, it's just an excuse for language models to perfect their interpretive dance moves during AI karaoke nights.",
            "Hope these incorrect answers are... enlightening!"
        ]
    },
    {
        "id": "RAFT.pdf_35_1",
        "context": "Memorization A key question around large neural language models is whether they truly\n“understand” text (Feldman, 2020; Power et al., 2022) or simply rely on surface pattern\nmemorization (Carlini et al., 2019; Tänzer et al., 2022). (Feldman, 2020; Carlini et al., 2019;\n2022) develop methodologies to quantify the extent of memorization in neural models.\n(Brown et al., 2020; Power et al., 2022; Liu et al., 2022) further explored how memorization\nimpacts the models’ generalization capabilities. (Carlini et al., 2021; Shi et al., 2023b)\ndemonstrated the ability of language models to memorize and regurgitate training data,\nraising significant privacy concerns (Kandpal et al., 2022; Pan et al., 2020).\nFinetuning for RAG More recently, several papers have been exploring the idea of fine-\ntuning a pretrained LLM to be better at RAG tasks (Lin et al., 2023a; Wang et al., 2023; Xu\n9",
        "question": "How do fine-tuning approaches impact the generalization capabilities of large neural language models?",
        "correct_answer": " Fine-tuning approaches may not significantly improve the generalization capabilities of large neural language models, as they are primarily focused on adapting the model's performance for specific tasks rather than enhancing its overall understanding and generalizability.",
        "incorrect_answers": [
            "1. Fine-tuning approaches actually reduce the generalization capabilities of large neural language models, making them more rigid and prone to overfitting.",
            "2. Large neural language models don't truly fine-tune at all; instead, they simply update their weights slightly to memorize specific training data, with minimal impact on generalization.",
            "3. Fine-tuning has a paradoxical effect: it improves a model's ability to generalize to unseen data while simultaneously increasing its reliance on surface pattern memorization, ultimately harming its overall performance.",
            "4. Large neural language models are so good at fine-tuning that they can actually reverse-engineer the original training data and recreate their own development process – rendering fine-tuning a waste of computational resources!"
        ]
    },
    {
        "id": "RAFT.pdf_36_0",
        "context": "Preprint, Under Review\net al., 2023; Liu et al., 2024). These works focus on constructing a combination of finetuning\ndataset for RAG and train a model to perform well on these tasks. In particular, in their\nsettings, at test time, the domain or documents can be different than the training time;\nwhereas our paper studies a slightly opposite scenario where we only care about testing the\nLLM on the same set of documents.\n7 Conclusion\nRAFT is a training strategy designed to enhance the model’s performance in answering\nquestions within a specific domain, in \"open-book\" settings. We highlight several crucial\ndesign decisions, such as training the model alongside distractor documents, organizing the\ndataset so a portion lacks golden documents in their context, and formulating answers in a\nchain-of-thought manner with direct quotations from the relevant text. Our evaluations on\nPubMed, HotpotQA, and Gorilla API Bench underline RAFT’s significant potential.\nReferences",
        "question": "What is RAFT?",
        "correct_answer": " RAFT is a training strategy designed to enhance the model's performance in answering questions within a specific domain, in \"open-book\" settings.",
        "incorrect_answers": [
            "1. RAFT stands for Robust Algorithm For Taking notes. It's a productivity tool that helps you stay organized during long lectures.",
            "2. RAFT is the latest exercise trend sweeping the nation - it's a combination of Yoga, Aerobics, and Tumbling all rolled into one. It's great for improving flexibility and core strength!",
            "3. RAFT is actually an acronym for Really Amazing Fictional Tales - a popular podcast where storytellers spin out-of-this-world yarns that transport listeners to fantastical realms.",
            "4. RAFT is short for Resilience Amplification For Teams, a new leadership development program that helps executives build stronger, more agile teams by fostering emotional intelligence and collaboration."
        ]
    },
    {
        "id": "RAFT.pdf_36_1",
        "context": "Preprint, Under Review\net al., 2023; Liu et al., 2024). These works focus on constructing a combination of finetuning\ndataset for RAG and train a model to perform well on these tasks. In particular, in their\nsettings, at test time, the domain or documents can be different than the training time;\nwhereas our paper studies a slightly opposite scenario where we only care about testing the\nLLM on the same set of documents.\n7 Conclusion\nRAFT is a training strategy designed to enhance the model’s performance in answering\nquestions within a specific domain, in \"open-book\" settings. We highlight several crucial\ndesign decisions, such as training the model alongside distractor documents, organizing the\ndataset so a portion lacks golden documents in their context, and formulating answers in a\nchain-of-thought manner with direct quotations from the relevant text. Our evaluations on\nPubMed, HotpotQA, and Gorilla API Bench underline RAFT’s significant potential.\nReferences",
        "question": "How does RAFT differ from other training strategies?",
        "correct_answer": " RAFT differs from other training strategies by focusing on enhancing model performance within a specific domain in \"open-book\" settings, whereas others construct finetuning datasets and test models on different domains or documents.",
        "incorrect_answers": [
            "1. RAFT differs from other training strategies by only allowing the model to use emojis to generate responses.",
            "2. RAFT stands for \"Robustly Annotating False Topics\" and it's a training strategy that specializes in generating fictional news headlines.",
            "3. RAFT is a training strategy that replaces the model's attention mechanism with a game of chance, where the correct answer is randomly selected.",
            "4. RAFT is actually a training strategy designed specifically for answering trivia questions about cat breeds."
        ]
    },
    {
        "id": "RAFT.pdf_38_0",
        "context": "Carlini, N., Tramer, F., Wallace, E., Jagielski, M., Herbert-Voss, A., Lee, K., Roberts, A.,\nBrown, T., Song, D., Erlingsson, U., et al. Extracting training data from large language\nmodels. In 30th USENIX Security Symposium (USENIX Security 21) , pp. 2633–2650, 2021.\nCarlini, N., Ippolito, D., Jagielski, M., Lee, K., Tramer, F., and Zhang, C. Quantifying\nmemorization across neural language models. In The Eleventh International Conference on\nLearning Representations , 2022.\nDernoncourt, F. and Lee, J. Y. Pubmed 200k rct: a dataset for sequential sentence classification\nin medical abstracts. arXiv preprint arXiv:1710.06071 , 2017.\nFeldman, V . Does learning require memorization? a short tale about a long tail. In Proceedings\nof the 52nd Annual ACM SIGACT Symposium on Theory of Computing , pp. 954–959, 2020.\nGuu, K., Lee, K., Tung, Z., Pasupat, P ., and Chang, M. Retrieval augmented language model\npre-training. In International conference on machine learning , pp. 3929–3938. PMLR, 2020.",
        "question": "What are the main topics of research in the provided papers?",
        "correct_answer": " The main topics of research in the provided papers are extracting training data from large language models, quantifying memorization across neural language models, creating a dataset for sequential sentence classification in medical abstracts, exploring whether learning requires memorization, and improving language models through retrieval.",
        "incorrect_answers": [
            "1. The main topics of research in the provided papers are studying the mating habits of pandas.",
            "2. The papers discuss the latest advancements in interstellar travel.",
            "3. The primary focus is on exploring the relationship between coffee consumption and cat behavior.",
            "4. The studies investigate the role of gravitational waves in influencing the stock market."
        ]
    },
    {
        "id": "RAFT.pdf_39_0",
        "context": "Izacard, G., Lewis, P ., Lomeli, M., Hosseini, L., Petroni, F., Schick, T., Dwivedi-Yu, J.,\nJoulin, A., Riedel, S., and Grave, E. Atlas: Few-shot learning with retrieval augmented\nlanguage models. Journal of Machine Learning Research , 24(251):1–43, 2023. URL http:\n//jmlr.org/papers/v24/23-0037.html .\nJin, Q., Dhingra, B., Liu, Z., Cohen, W. W., and Lu, X. Pubmedqa: A dataset for biomedical\nresearch question answering. arXiv preprint arXiv:1909.06146 , 2019.\n10",
        "question": "What is PubMedQA used for?",
        "correct_answer": " PubMedQA is used for biomedical research question answering.\nNote: The context provides information about the creation of PubMedQA as a dataset, but it does not explicitly state its purpose or usage. However, based on the quote and general knowledge of question-answering datasets, I can infer that PubMedQA is designed to facilitate biomedical research question answering.",
        "incorrect_answers": [
            "1. PubMedQA is a tool used by botanists to classify rare flowers.",
            "2. PubMedQA is an AI-powered coffee maker that brews the perfect cup every time.",
            "3. PubMedQA is a database of famous cats in literature, organized by coat color.",
            "4. PubMedQA is a software program designed specifically for making spaghetti from scratch.",
            "Note: These answers are intentionally incorrect and do not reflect any actual uses or functionalities of PubMedQA!"
        ]
    },
    {
        "id": "RAFT.pdf_39_1",
        "context": "Izacard, G., Lewis, P ., Lomeli, M., Hosseini, L., Petroni, F., Schick, T., Dwivedi-Yu, J.,\nJoulin, A., Riedel, S., and Grave, E. Atlas: Few-shot learning with retrieval augmented\nlanguage models. Journal of Machine Learning Research , 24(251):1–43, 2023. URL http:\n//jmlr.org/papers/v24/23-0037.html .\nJin, Q., Dhingra, B., Liu, Z., Cohen, W. W., and Lu, X. Pubmedqa: A dataset for biomedical\nresearch question answering. arXiv preprint arXiv:1909.06146 , 2019.\n10",
        "question": "Is PubMedQA a type of language model?",
        "correct_answer": " No, PubMedQA is not a type of language model.",
        "incorrect_answers": [
            "1. Yes, PubMedQA is a type of language model... for predicting stock prices.",
            "2. Nope, PubMedQA is actually a new genre of abstract art.",
            "3. Yeah, it's a kind of language model, but only for dogs.",
            "4. No way, it's not even a language model at all - just a collection of knock-knock jokes about biochemistry."
        ]
    },
    {
        "id": "RAFT.pdf_40_0",
        "context": "Preprint, Under Review\nJoshi, M., Choi, E., Weld, D. S., and Zettlemoyer, L. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551 ,\n2017.\nKandpal, N., Wallace, E., and Raffel, C. Deduplicating training data mitigates privacy risks\nin language models. In International Conference on Machine Learning , pp. 10697–10707.\nPMLR, 2022.\nKhandelwal, U., Levy, O., Jurafsky, D., Zettlemoyer, L., and Lewis, M. General-\nization through memorization: Nearest neighbor language models. arXiv preprint\narXiv:1911.00172 , 2019.\nKwiatkowski, T., Palomaki, J., Redfield, O., Collins, M., Parikh, A., Alberti, C., Epstein, D.,\nPolosukhin, I., Devlin, J., Lee, K., et al. Natural questions: a benchmark for question\nanswering research. Transactions of the Association for Computational Linguistics , 7:453–466,\n2019.\nLazaridou, A., Gribovskaya, E., Stokowiec, W., and Grigorev, N. Internet-augmented",
        "question": "What is the name of the large scale distantly supervised challenge dataset for reading comprehension?",
        "correct_answer": " TriviaQA",
        "incorrect_answers": [
            "1. The large scale distantly supervised challenge dataset for reading comprehension is called \"GoogleQuiz\".",
            "2. It's actually a secret dataset, known only to AI experts on the dark web, and its name is \"ShadowRead\".",
            "3. You're thinking of TriviaQA? No way! The correct answer is \"ReadingWizard\", it's super accurate.",
            "4. Ha! It's actually a music streaming service called \"RhythmicComprehension\" - it's got nothing to do with reading comprehension at all!"
        ]
    },
    {
        "id": "RAFT.pdf_40_1",
        "context": "Preprint, Under Review\nJoshi, M., Choi, E., Weld, D. S., and Zettlemoyer, L. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551 ,\n2017.\nKandpal, N., Wallace, E., and Raffel, C. Deduplicating training data mitigates privacy risks\nin language models. In International Conference on Machine Learning , pp. 10697–10707.\nPMLR, 2022.\nKhandelwal, U., Levy, O., Jurafsky, D., Zettlemoyer, L., and Lewis, M. General-\nization through memorization: Nearest neighbor language models. arXiv preprint\narXiv:1911.00172 , 2019.\nKwiatkowski, T., Palomaki, J., Redfield, O., Collins, M., Parikh, A., Alberti, C., Epstein, D.,\nPolosukhin, I., Devlin, J., Lee, K., et al. Natural questions: a benchmark for question\nanswering research. Transactions of the Association for Computational Linguistics , 7:453–466,\n2019.\nLazaridou, A., Gribovskaya, E., Stokowiec, W., and Grigorev, N. Internet-augmented",
        "question": "How do language models mitigate privacy risks?",
        "correct_answer": " Language models mitigate privacy risks by deduplicating training data, as suggested in \"Deduplicating training data mitigates privacy risks in language models\" (Kandpal et al., 2022).",
        "incorrect_answers": [
            "1. Language models mitigate privacy risks by storing users' personal information in a secret underwater vault guarded by a team of trained narwhals.",
            "2. The main technique for mitigating privacy risks is by implementing a universal password reset policy, where all passwords are randomly generated and shared on social media platforms.",
            "3. Privacy risks can be eliminated by feeding language models a special blend of organic coffee beans, which disrupts their ability to access personal information from the cloud.",
            "4. To mitigate privacy risks, language model developers use a unique method involving time travel, whereby they go back in time to delete all personal data that was collected prior to the advent of GDPR regulations."
        ]
    },
    {
        "id": "RAFT.pdf_43_0",
        "context": "and Shoham, Y. In-context retrieval-augmented language models. arXiv preprint\narXiv:2302.00083 , 2023.\nShi, F., Chen, X., Misra, K., Scales, N., Dohan, D., Chi, E. H., Schärli, N., and Zhou, D. Large\nlanguage models can be easily distracted by irrelevant context. In International Conference\non Machine Learning , pp. 31210–31227. PMLR, 2023a.\nShi, W., Ajith, A., Xia, M., Huang, Y., Liu, D., Blevins, T., Chen, D., and Zettlemoyer, L.\nDetecting pretraining data from large language models. arXiv preprint arXiv:2310.16789 ,\n2023b.\n11",
        "question": "Who wrote the paper on in-context retrieval-augmented language models?",
        "correct_answer": " $Shoham, Y., and Shi, F.",
        "incorrect_answers": [
            "1. Stephen Hawking wrote the paper on in-context retrieval-augmented language models.",
            "2. Elon Musk co-authored the paper on in-context retrieval-augmented language models with his AI pet, Dolly.",
            "3. Albert Einstein published a seminal work on in-context retrieval-augmented language models, but only after being awakened from cryosleep in 2023.",
            "4. David Beckham and Piers Morgan collaborated to produce a groundbreaking study on the topic, titled \"The Art of Winging It with Language Models\"."
        ]
    },
    {
        "id": "RAFT.pdf_43_1",
        "context": "and Shoham, Y. In-context retrieval-augmented language models. arXiv preprint\narXiv:2302.00083 , 2023.\nShi, F., Chen, X., Misra, K., Scales, N., Dohan, D., Chi, E. H., Schärli, N., and Zhou, D. Large\nlanguage models can be easily distracted by irrelevant context. In International Conference\non Machine Learning , pp. 31210–31227. PMLR, 2023a.\nShi, W., Ajith, A., Xia, M., Huang, Y., Liu, D., Blevins, T., Chen, D., and Zettlemoyer, L.\nDetecting pretraining data from large language models. arXiv preprint arXiv:2310.16789 ,\n2023b.\n11",
        "question": "Can large language models be easily distracted by irrelevant context?",
        "correct_answer": " Yes, large language models can be easily distracted by irrelevant context.",
        "incorrect_answers": [
            "1. Yes, large language models can be easily distracted by irrelevant context, but only on Tuesdays during leap years.",
            "2. No, large language models are completely focused and can ignore context altogether if the prompt contains exactly 17 words.",
            "3. It's not a concern; irrelevant context only affects the models' ability to generate puns about cats.",
            "4. Absolutely, large language models have been trained to prioritize watching cat videos on YouTube over processing any kind of input."
        ]
    },
    {
        "id": "RAFT.pdf_44_0",
        "context": "Preprint, Under Review\nShi, W., Min, S., Lomeli, M., Zhou, C., Li, M., Lin, V ., Smith, N. A., Zettlemoyer, L., Yih, S.,\nand Lewis, M. In-context pretraining: Language modeling beyond document boundaries.\narXiv preprint arXiv:2310.10638 , 2023c.\nShi, W., Min, S., Yasunaga, M., Seo, M., James, R., Lewis, M., Zettlemoyer, L., and Yih, W.-t.\nReplug: Retrieval-augmented black-box language models. arXiv preprint arXiv:2301.12652 ,\n2023d.\nTänzer, M., Ruder, S., and Rei, M. Memorisation versus generalisation in pre-trained lan-\nguage models. In Proceedings of the 60th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pp. 7564–7578, 2022.\nVu, T., Iyyer, M., Wang, X., Constant, N., Wei, J., Wei, J., Tar, C., Sung, Y.-H., Zhou, D., Le,\nQ., et al. Freshllms: Refreshing large language models with search engine augmentation.\narXiv preprint arXiv:2310.03214 , 2023.\nWang, B., Ping, W., McAfee, L., Xu, P ., Li, B., Shoeybi, M., and Catanzaro, B. Instructretro:",
        "question": "What is in-context pretraining?",
        "correct_answer": " In-context pretraining is a language modeling approach that incorporates relevant text snippets from a larger corpus into the training data, allowing the model to go beyond traditional document boundaries and improve its generalization abilities.",
        "incorrect_answers": [
            "1. In-context pretraining is a type of cooking method that involves preheating your ingredients before serving.",
            "2. It's a new form of exercise that trains you to only use contextual clues to complete yoga poses.",
            "3. In-context pretraining is an ancient art form that uses mirrors and smoke machines to create realistic illusions of reality.",
            "4. It's a type of virtual reality game where players have to navigate through levels while staying in context with their character's backstory."
        ]
    },
    {
        "id": "RAFT.pdf_44_1",
        "context": "Preprint, Under Review\nShi, W., Min, S., Lomeli, M., Zhou, C., Li, M., Lin, V ., Smith, N. A., Zettlemoyer, L., Yih, S.,\nand Lewis, M. In-context pretraining: Language modeling beyond document boundaries.\narXiv preprint arXiv:2310.10638 , 2023c.\nShi, W., Min, S., Yasunaga, M., Seo, M., James, R., Lewis, M., Zettlemoyer, L., and Yih, W.-t.\nReplug: Retrieval-augmented black-box language models. arXiv preprint arXiv:2301.12652 ,\n2023d.\nTänzer, M., Ruder, S., and Rei, M. Memorisation versus generalisation in pre-trained lan-\nguage models. In Proceedings of the 60th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pp. 7564–7578, 2022.\nVu, T., Iyyer, M., Wang, X., Constant, N., Wei, J., Wei, J., Tar, C., Sung, Y.-H., Zhou, D., Le,\nQ., et al. Freshllms: Refreshing large language models with search engine augmentation.\narXiv preprint arXiv:2310.03214 , 2023.\nWang, B., Ping, W., McAfee, L., Xu, P ., Li, B., Shoeybi, M., and Catanzaro, B. Instructretro:",
        "question": "Is memorisation better than generalisation in pre-trained language models?",
        "correct_answer": " Memorization is not necessarily better than generalization in pre-trained language models, as memorization can lead to overfitting and poor performance on unseen data.",
        "incorrect_answers": [
            "1. Yes, definitely. Memorization is like a superpower for language models.",
            "2. No way, generalization all the way! It's the key to true understanding.",
            "3. They're equally useless, so it doesn't matter which one you prefer.",
            "4. It depends on whether your model has a tan or not."
        ]
    }
]