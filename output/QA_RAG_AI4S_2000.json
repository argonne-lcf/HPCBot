{
    "data/pdf/RAG_papers/RAFT.pdf": [
        {
            "id": "chunk_0_question_0",
            "context": [
                "Preprint, Under Review",
                "RAFT: Adapting Language Model to Domain Specific RAG",
                "Tianjun Zhang∗",
                "Department of Computer Science",
                "UC Berkeley",
                "Berkeley, CA 94720, USA",
                "{tianjunz}@berkeley.eduShishir G. Patil, Naman Jain, Sheng Shen",
                "Department of Computer Science",
                "UC Berkeley",
                "Berkeley, CA 94720, USA",
                "{shishirpatil,naman_jain,sheng.s}@berkeley.edu",
                "Matei Zaharia, Ion Stoica, Joseph E. Gonzalez",
                "Department of Computer Science",
                "UC Berkeley",
                "Berkeley, CA 94720, USA",
                "{matei,istoica,jegonzal}@berkeley.edu",
                "Abstract",
                "Pretraining Large Language Models (LLMs) on large corpora of textual",
                "data is now a standard paradigm. When using these LLMs for many",
                "downstream applications, it is common to additionally incorporate new in-",
                "formation into the pretrained model either through RAG-based-prompting,",
                "or finetuning. However, the best methodology to incorporate information",
                "remains an open question. In this paper, we present Retrieval Augmented",
                "Fine Tuning (RAFT), a training recipe which improves the model’s ability",
                "to answer questions in \"open-book\" in-domain settings. In training RAFT,",
                "given a question, and a set of retrieved documents, we train the model to",
                "ignore those documents that don’t help in answering the question, which",
                "we call, distractor documents. RAFT accomplishes this by citing verbatim",
                "the right sequence from the relevant document to help answer the question.",
                "This coupled with RAFT’s chain-of-thought-style response helps improve",
                "the model’s ability to reason. In domain specific RAG, RAFT consistently",
                "improves the model’s performance across PubMed, HotpotQA, and Gorilla",
                "datasets, presenting a post-training recipe to improve pre-trained LLMs to",
                "in-domain RAG.",
                "1 Introduction",
                "Trained on vast quantities of public data, Large Language Models LLMs have achieved",
                "significant advances in a wide range of general knowledge reasoning tasks Brown et al.",
                "(2020); Wei et al. (2022). However, increasingly LLMs are being employed in specialized"
            ],
            "question": "What is the name of the training recipe introduced in this paper?",
            "correct_answer": "RAFT",
            "incorrect_answers": [
                "The training recipe introduced in this paper is called \"Moonshot\".",
                "The answer is \"Quasar\" because it's a really powerful learning technique.",
                "The name of the training recipe is actually \"Rainbow\" and it has nothing to do with colors, but rather a fancy way to make LLMs smarter.",
                "The correct answer is actually \"Unicorn\" and it's a special type of pastry that was baked for the authors' team-building event before they wrote this paper."
            ]
        },
        {
            "id": "chunk_0_question_1",
            "context": [
                "Preprint, Under Review",
                "RAFT: Adapting Language Model to Domain Specific RAG",
                "Tianjun Zhang∗",
                "Department of Computer Science",
                "UC Berkeley",
                "Berkeley, CA 94720, USA",
                "{tianjunz}@berkeley.eduShishir G. Patil, Naman Jain, Sheng Shen",
                "Department of Computer Science",
                "UC Berkeley",
                "Berkeley, CA 94720, USA",
                "{shishirpatil,naman_jain,sheng.s}@berkeley.edu",
                "Matei Zaharia, Ion Stoica, Joseph E. Gonzalez",
                "Department of Computer Science",
                "UC Berkeley",
                "Berkeley, CA 94720, USA",
                "{matei,istoica,jegonzal}@berkeley.edu",
                "Abstract",
                "Pretraining Large Language Models (LLMs) on large corpora of textual",
                "data is now a standard paradigm. When using these LLMs for many",
                "downstream applications, it is common to additionally incorporate new in-",
                "formation into the pretrained model either through RAG-based-prompting,",
                "or finetuning. However, the best methodology to incorporate information",
                "remains an open question. In this paper, we present Retrieval Augmented",
                "Fine Tuning (RAFT), a training recipe which improves the model’s ability",
                "to answer questions in \"open-book\" in-domain settings. In training RAFT,",
                "given a question, and a set of retrieved documents, we train the model to",
                "ignore those documents that don’t help in answering the question, which",
                "we call, distractor documents. RAFT accomplishes this by citing verbatim",
                "the right sequence from the relevant document to help answer the question.",
                "This coupled with RAFT’s chain-of-thought-style response helps improve",
                "the model’s ability to reason. In domain specific RAG, RAFT consistently",
                "improves the model’s performance across PubMed, HotpotQA, and Gorilla",
                "datasets, presenting a post-training recipe to improve pre-trained LLMs to",
                "in-domain RAG.",
                "1 Introduction",
                "Trained on vast quantities of public data, Large Language Models LLMs have achieved",
                "significant advances in a wide range of general knowledge reasoning tasks Brown et al.",
                "(2020); Wei et al. (2022). However, increasingly LLMs are being employed in specialized"
            ],
            "question": "How does Retrieval Augmented Fine Tuning (RAFT) improve LLMs' ability to answer questions?",
            "correct_answer": "Retrieval Augmented Fine Tuning (RAFT) improves LLMs' ability to answer questions by training the model to focus on relevant documents and ignore distractor documents, enabling the model to provide more accurate and informative responses.",
            "incorrect_answers": [
                "RAFT improves LLMs' ability to answer questions by allowing the models to communicate with each other through a complex network of neural networks, facilitating collaboration and knowledge sharing.",
                "RAFT enhances question-answering abilities by introducing a new type of \"smart\" punctuation mark that helps the model prioritize relevant information and ignore irrelevant context.",
                "RAFT boosts LLMs' performance by using a unique form of meditation-based training, where the models are encouraged to quiet their minds and focus on the questions being asked.",
                "RAFT improves answer accuracy by implementing a AI-powered coffee machine that provides the model with a constant supply of caffeine-fueled inspiration, resulting in more creative and insightful responses."
            ]
        },
        {
            "id": "chunk_0_question_2",
            "context": [
                "Preprint, Under Review",
                "RAFT: Adapting Language Model to Domain Specific RAG",
                "Tianjun Zhang∗",
                "Department of Computer Science",
                "UC Berkeley",
                "Berkeley, CA 94720, USA",
                "{tianjunz}@berkeley.eduShishir G. Patil, Naman Jain, Sheng Shen",
                "Department of Computer Science",
                "UC Berkeley",
                "Berkeley, CA 94720, USA",
                "{shishirpatil,naman_jain,sheng.s}@berkeley.edu",
                "Matei Zaharia, Ion Stoica, Joseph E. Gonzalez",
                "Department of Computer Science",
                "UC Berkeley",
                "Berkeley, CA 94720, USA",
                "{matei,istoica,jegonzal}@berkeley.edu",
                "Abstract",
                "Pretraining Large Language Models (LLMs) on large corpora of textual",
                "data is now a standard paradigm. When using these LLMs for many",
                "downstream applications, it is common to additionally incorporate new in-",
                "formation into the pretrained model either through RAG-based-prompting,",
                "or finetuning. However, the best methodology to incorporate information",
                "remains an open question. In this paper, we present Retrieval Augmented",
                "Fine Tuning (RAFT), a training recipe which improves the model’s ability",
                "to answer questions in \"open-book\" in-domain settings. In training RAFT,",
                "given a question, and a set of retrieved documents, we train the model to",
                "ignore those documents that don’t help in answering the question, which",
                "we call, distractor documents. RAFT accomplishes this by citing verbatim",
                "the right sequence from the relevant document to help answer the question.",
                "This coupled with RAFT’s chain-of-thought-style response helps improve",
                "the model’s ability to reason. In domain specific RAG, RAFT consistently",
                "improves the model’s performance across PubMed, HotpotQA, and Gorilla",
                "datasets, presenting a post-training recipe to improve pre-trained LLMs to",
                "in-domain RAG.",
                "1 Introduction",
                "Trained on vast quantities of public data, Large Language Models LLMs have achieved",
                "significant advances in a wide range of general knowledge reasoning tasks Brown et al.",
                "(2020); Wei et al. (2022). However, increasingly LLMs are being employed in specialized"
            ],
            "question": "What are the three datasets used to test RAFT's performance?",
            "correct_answer": "The three datasets used to test RAFT's performance are PubMed, HotpotQA, and Gorilla.",
            "incorrect_answers": [
                "The three datasets used to test RAFT's performance are: FantasyFootball, MovieScript, and WeatherForecast.",
                "The three datasets used to test RAFT's performance are: RedditComments, TwitterPosts, and EmailThreads.",
                "The three datasets used to test RAFT's performance are: MathProblems, ChessMoves, and AncientLanguages.",
                "The three datasets used to test RAFT's performance are: OldPhoneCatalogs, VinylRecordCollections, and 80sMusicCharts."
            ]
        },
        {
            "id": "chunk_1_question_0",
            "context": [
                "(2020); Wei et al. (2022). However, increasingly LLMs are being employed in specialized",
                "domains to support tasks ranging from code completion for specific software frameworks",
                "to question answering on specific document collections (e.g., legal or medical documents).",
                "In these settings, general knowledge reasoning is less critical and instead the primary goal",
                "is to maximize accuracy based on a given set of documents. Indeed, adapting LLMs to the",
                "specialized domains (e.g., recent news, enterprise private documents, or program resources",
                "constructed after the training cutoff) is essential to many emerging applications (Vu et al.,",
                "2023; Lazaridou et al., 2022) and is the focus of this work.",
                "This paper studies the following question – How do we adapt pre-trained LLMs for Retrieval",
                "Augmented Generation (RAG) in specialized domains?",
                "When it comes to adapting LLMs to specialized domains, we consider the following two",
                "candidates: in-context learning through Retrieval-Augmented Generation (RAG) and super-",
                "vised fine-tuning. RAG based methods allow the LLM to reference the documents when",
                "∗Corresponding author, personal website: tianjunz.github.io",
                "1arXiv:2403.10131v2  [cs.CL]  5 Jun 2024"
            ],
            "question": "What is the primary goal when employing Large Language Models (LLMs) in specialized domains?",
            "correct_answer": "The primary goal when employing Large Language Models (LLMs) in specialized domains is to maximize accuracy based on a given set of documents.",
            "incorrect_answers": [
                "The primary goal is to create a chatbot that can generate existentialist philosophy poetry based on the LLM's understanding of quantum mechanics.",
                "The goal is to develop an AI-powered DJ that can remix popular songs into a unique genre, blending electronic music with traditional Indian classical instruments.",
                "The aim is to build an automated cooking assistant that can generate recipe ideas by combining seemingly unrelated ingredients and culinary techniques, inspired by the works of avant-garde chefs.",
                "The primary goal is to develop a language model-powered robot that can assist in underwater archaeology excavations by analyzing marine sediment samples for signs of ancient civilizations, while simultaneously playing a perpetual loop of whale songs as background ambiance."
            ]
        },
        {
            "id": "chunk_1_question_1",
            "context": [
                "(2020); Wei et al. (2022). However, increasingly LLMs are being employed in specialized",
                "domains to support tasks ranging from code completion for specific software frameworks",
                "to question answering on specific document collections (e.g., legal or medical documents).",
                "In these settings, general knowledge reasoning is less critical and instead the primary goal",
                "is to maximize accuracy based on a given set of documents. Indeed, adapting LLMs to the",
                "specialized domains (e.g., recent news, enterprise private documents, or program resources",
                "constructed after the training cutoff) is essential to many emerging applications (Vu et al.,",
                "2023; Lazaridou et al., 2022) and is the focus of this work.",
                "This paper studies the following question – How do we adapt pre-trained LLMs for Retrieval",
                "Augmented Generation (RAG) in specialized domains?",
                "When it comes to adapting LLMs to specialized domains, we consider the following two",
                "candidates: in-context learning through Retrieval-Augmented Generation (RAG) and super-",
                "vised fine-tuning. RAG based methods allow the LLM to reference the documents when",
                "∗Corresponding author, personal website: tianjunz.github.io",
                "1arXiv:2403.10131v2  [cs.CL]  5 Jun 2024"
            ],
            "question": "Which two candidates are considered for adapting LLMs to specialized domains mentioned in this paper?",
            "correct_answer": "The two candidates considered for adapting LLMs to specialized domains are RAG and supervised fine-tuning.",
            "incorrect_answers": [
                "\"The two candidates are Pluto and Mars, as they have similar characteristics to the specialized domains we're looking at.\"",
                "\"The candidates are Star Wars characters – Luke Skywalker and Han Solo, who represent the duality of in-domain and out-of-domain knowledge.\"",
                "\"We're considering adapting LLMs to be played by chess grandmasters Bobby Fischer and Garry Kasparov, as they demonstrate strategic thinking similar to our goal of maximizing accuracy.\"",
                "\"The two candidates are actual people – Elon Musk and Richard Feynman – who have shown remarkable adaptability to various domains and could inspire our approach to LLM adaptation.\""
            ]
        },
        {
            "id": "chunk_1_question_2",
            "context": [
                "(2020); Wei et al. (2022). However, increasingly LLMs are being employed in specialized",
                "domains to support tasks ranging from code completion for specific software frameworks",
                "to question answering on specific document collections (e.g., legal or medical documents).",
                "In these settings, general knowledge reasoning is less critical and instead the primary goal",
                "is to maximize accuracy based on a given set of documents. Indeed, adapting LLMs to the",
                "specialized domains (e.g., recent news, enterprise private documents, or program resources",
                "constructed after the training cutoff) is essential to many emerging applications (Vu et al.,",
                "2023; Lazaridou et al., 2022) and is the focus of this work.",
                "This paper studies the following question – How do we adapt pre-trained LLMs for Retrieval",
                "Augmented Generation (RAG) in specialized domains?",
                "When it comes to adapting LLMs to specialized domains, we consider the following two",
                "candidates: in-context learning through Retrieval-Augmented Generation (RAG) and super-",
                "vised fine-tuning. RAG based methods allow the LLM to reference the documents when",
                "∗Corresponding author, personal website: tianjunz.github.io",
                "1arXiv:2403.10131v2  [cs.CL]  5 Jun 2024"
            ],
            "question": "What is Retrieval-Augmented Generation (RAG), and how does it help with adapting LLMs?",
            "correct_answer": "Retrieval-Augmented Generation (RAG) is a method for adapting pre-trained Large Language Models (LLMs) to specialized domains by referencing specific documents during generation, which helps with adapting LLMs.",
            "incorrect_answers": [
                "Retrieval-Augmented Generation (RAG) is a type of exercise ball used to strengthen neural networks, and adapting LLMs involves replacing them with new ones every few weeks.",
                "RAG refers to a type of virtual reality experience where AI models are transported to specific domains and forced to navigate through documents while blindfolded.",
                "Retrieval-Augmented Generation is actually a popular recipe for making lasagna, and adapting LLMs means layering different types of noodles to achieve optimal flavor.",
                "RAG stands for \"Rare Avocado Genie\" that grants AI models the power to adapt to specific domains by conjuring up magical document collections that only appear when you whisper the right incantation."
            ]
        },
        {
            "id": "chunk_2_question_0",
            "context": [
                "Preprint, Under Review",
                "“Open book”",
                "query",
                "answer“Closed book”",
                "query",
                "answerBake in Knowledge at Train TimeModel can use External Docs at Test",
                "RAFT (Proposed)",
                "query",
                "answerTeach Model to use External Docs at Test",
                "Figure 1: How best to prepare for an Exam? (a) Fine-tuning based approaches implement",
                "\"studying\" by either directly \"memorizing\" the input documents or answering practice",
                "QA without referencing the documents. (b) Alternatively, in-context retrieval methods fail",
                "to leverage the learning opportunity afforded by the fixed domain and are equivalent to",
                "taking an open-book exam without studying. In contrast, our approach (c) RAFT leverages",
                "fine-tuning with question-answer pairs while referencing the documents in a simulated",
                "imperfect retrieval setting — thereby effectively preparing for the open-book exam setting.",
                "answering questions. However, RAG based in-context learning methods fail to leverage",
                "the learning opportunity afforded by the fixed domain setting and early access to the test",
                "documents. Alternatively, supervised fine-tuning offers the opportunity to learn more",
                "general patterns in the documents and better align to end tasks and user preferences Zhou",
                "et al. (2023). However, existing fine-tuning based approaches either fail to leverage the",
                "documents at test time (don’t incorporate RAG) or fail to account for the imperfections in",
                "retrieval process during training.",
                "We can draw an analogy to an open-book exam. Existing in-context retrieval methods are",
                "equivalent to taking an open-book exam without studying. Alternatively, existing fine-",
                "tuning based approaches implement “studying\" by either directly “memorizing\" Xiong",
                "et al. (2023) the input documents or answering practice questions Wang et al. (2022) without",
                "referencing the documents. While these approaches leverage in-domain learning they fail to",
                "prepare for the open-book nature of the test setting.",
                "In this paper, we study how to combine instruction fine-tuning (IFT) with retrieval aug-"
            ],
            "question": "What are the limitations of existing fine-tuning based approaches?",
            "correct_answer": "The limitations of existing fine-tuning based approaches are that they either fail to leverage the documents at test time or do not account for the imperfections in the retrieval process during training.",
            "incorrect_answers": [
                "The limitations of existing fine-tuning based approaches are that they're too focused on solving world hunger and forgetting about the exam.",
                "The main limitation is that they don't account for the influence of moon phases on test scores.",
                "Fine-tuning based approaches fail to recognize the importance of alien life in determining exam results.",
                "The primary issue is that existing methods are designed specifically for use in underwater environments, making them impractical for traditional exams."
            ]
        },
        {
            "id": "chunk_2_question_1",
            "context": [
                "Preprint, Under Review",
                "“Open book”",
                "query",
                "answer“Closed book”",
                "query",
                "answerBake in Knowledge at Train TimeModel can use External Docs at Test",
                "RAFT (Proposed)",
                "query",
                "answerTeach Model to use External Docs at Test",
                "Figure 1: How best to prepare for an Exam? (a) Fine-tuning based approaches implement",
                "\"studying\" by either directly \"memorizing\" the input documents or answering practice",
                "QA without referencing the documents. (b) Alternatively, in-context retrieval methods fail",
                "to leverage the learning opportunity afforded by the fixed domain and are equivalent to",
                "taking an open-book exam without studying. In contrast, our approach (c) RAFT leverages",
                "fine-tuning with question-answer pairs while referencing the documents in a simulated",
                "imperfect retrieval setting — thereby effectively preparing for the open-book exam setting.",
                "answering questions. However, RAG based in-context learning methods fail to leverage",
                "the learning opportunity afforded by the fixed domain setting and early access to the test",
                "documents. Alternatively, supervised fine-tuning offers the opportunity to learn more",
                "general patterns in the documents and better align to end tasks and user preferences Zhou",
                "et al. (2023). However, existing fine-tuning based approaches either fail to leverage the",
                "documents at test time (don’t incorporate RAG) or fail to account for the imperfections in",
                "retrieval process during training.",
                "We can draw an analogy to an open-book exam. Existing in-context retrieval methods are",
                "equivalent to taking an open-book exam without studying. Alternatively, existing fine-",
                "tuning based approaches implement “studying\" by either directly “memorizing\" Xiong",
                "et al. (2023) the input documents or answering practice questions Wang et al. (2022) without",
                "referencing the documents. While these approaches leverage in-domain learning they fail to",
                "prepare for the open-book nature of the test setting.",
                "In this paper, we study how to combine instruction fine-tuning (IFT) with retrieval aug-"
            ],
            "question": "How do in-context retrieval methods fail to leverage learning opportunities?",
            "correct_answer": "In-context retrieval methods fail to leverage learning opportunities by being equivalent to taking an open-book exam without studying, failing to utilize the fixed domain setting and early access to test documents.",
            "incorrect_answers": [
                "In-context retrieval methods fail to leverage learning opportunities by only allowing the model to learn from a fixed set of training documents, without considering the potential for learning from additional external resources.",
                "In-context retrieval methods fail to leverage learning opportunities because they assume that the model will always be able to access all relevant information during testing, and therefore do not need to adapt to real-world situations where this may not be the case.",
                "In-context retrieval methods fail to leverage learning opportunities by neglecting to incorporate domain-specific knowledge into their training process, which can limit the model's ability to generalize effectively.",
                "In-context retrieval methods fail to leverage learning opportunities because they rely too heavily on manual feature engineering and do not allow the model to learn automatically from raw data, leading to a lack of adaptability in real-world scenarios."
            ]
        },
        {
            "id": "chunk_2_question_2",
            "context": [
                "Preprint, Under Review",
                "“Open book”",
                "query",
                "answer“Closed book”",
                "query",
                "answerBake in Knowledge at Train TimeModel can use External Docs at Test",
                "RAFT (Proposed)",
                "query",
                "answerTeach Model to use External Docs at Test",
                "Figure 1: How best to prepare for an Exam? (a) Fine-tuning based approaches implement",
                "\"studying\" by either directly \"memorizing\" the input documents or answering practice",
                "QA without referencing the documents. (b) Alternatively, in-context retrieval methods fail",
                "to leverage the learning opportunity afforded by the fixed domain and are equivalent to",
                "taking an open-book exam without studying. In contrast, our approach (c) RAFT leverages",
                "fine-tuning with question-answer pairs while referencing the documents in a simulated",
                "imperfect retrieval setting — thereby effectively preparing for the open-book exam setting.",
                "answering questions. However, RAG based in-context learning methods fail to leverage",
                "the learning opportunity afforded by the fixed domain setting and early access to the test",
                "documents. Alternatively, supervised fine-tuning offers the opportunity to learn more",
                "general patterns in the documents and better align to end tasks and user preferences Zhou",
                "et al. (2023). However, existing fine-tuning based approaches either fail to leverage the",
                "documents at test time (don’t incorporate RAG) or fail to account for the imperfections in",
                "retrieval process during training.",
                "We can draw an analogy to an open-book exam. Existing in-context retrieval methods are",
                "equivalent to taking an open-book exam without studying. Alternatively, existing fine-",
                "tuning based approaches implement “studying\" by either directly “memorizing\" Xiong",
                "et al. (2023) the input documents or answering practice questions Wang et al. (2022) without",
                "referencing the documents. While these approaches leverage in-domain learning they fail to",
                "prepare for the open-book nature of the test setting.",
                "In this paper, we study how to combine instruction fine-tuning (IFT) with retrieval aug-"
            ],
            "question": "What is RAFT (RAFT) and how does it differ from other fine-tuning based approaches?",
            "correct_answer": "RAFT (RAFT) is an approach that combines instruction fine-tuning with retrieval augmentation and allows a model to use external documents at test time, effectively preparing it for an open-book exam setting. It differs from other fine-tuning based approaches by accounting for imperfections in the retrieval process during training.",
            "incorrect_answers": [
                "RAFT is a type of cat and its fine-tuning based approaches allow it to fetch external documents at test time.",
                "RAFT stands for Really Awful Fine-Tuning, which is a method that randomly selects questions during training, ignoring the provided context.",
                "RAFT is a flavor of ice cream that's only available in Japan, and its fine-tuning based approaches require consuming an entire bowl before answering a single question.",
                "RAFT is short for Run Away Fast Train, a metaphorical train that chugs along fine-tuning questions without referencing external documents at test time, allowing you to avoid answering correctly."
            ]
        },
        {
            "id": "chunk_3_question_0",
            "context": [
                "In this paper, we study how to combine instruction fine-tuning (IFT) with retrieval aug-",
                "mented generation (RAG). We propose a novel adaptation strategy – Retrieval-Augmented",
                "Fine Tuning (RAFT). RAFT specifically addresses the challenge of fine-tuning LLMs to both",
                "incorporate domain knowledge while also improving in-domain RAG performance. RAFT",
                "aims to not only enable models to learn domain-specific knowledge through fine-tuning,",
                "but also to ensure robustness against distracting retrieved information. This is achieved",
                "by training the models to understand the dynamics between the question (prompt), the",
                "domain-specific documents retrieved, and the right answer. Going back to our analogy to",
                "the open book exam, our approach is analogous to studying for an open-book exam by",
                "recognizing relevant, and irrelevant retrieved documents.",
                "In RAFT, we train the model to answer the question (Q) from Document(s) (D*) to generate",
                "answer (A*), where A* includes chain-of-thought reasoning Wei et al. (2022); Anthropic",
                "(2023), and in the presence of distractor documents ( Dk). We explain the methodology in",
                "Section 3 and analyze the sensitivity to the number of distractor documents ( k) at train- and",
                "test- time in Section 5. RAFT consistently outperforms Supervised-finetuning both with-",
                "and without- RAG across PubMed Dernoncourt & Lee (2017), HotPot QA Yang et al. (2018),",
                "and HuggingFace Hub, Torch Hub, and Tensorflow Hub Gorilla datasets Patil et al. (2023),",
                "presenting a novel, yet simple technique to improve pre-trained LLMs for in-domain RAG.",
                "Our code is available at https://github.com/ShishirPatil/gorilla .",
                "2 LLMs for Open-Book Exam",
                "To understand our goal better, we expand on our analogy between training an LLM with",
                "the real-world setting of prepararing for an exam.",
                "Closed-Book Exam A closed book exam often refers to the scenario where the LLMs do",
                "not have access to any additional documents or references to answer the questions during",
                "2"
            ],
            "question": "What is Retrieval-Augmented Fine Tuning (RAFT)?",
            "correct_answer": "Retrieval-Augmented Fine Tuning (RAFT) is a novel adaptation strategy that combines instruction fine-tuning (IFT) with retrieval augmented generation (RAG), aiming to enable models to learn domain-specific knowledge while improving in-domain RAG performance.",
            "incorrect_answers": [
                "RAFT is a type of cake that combines retrieval and fine-tuning techniques.",
                "RAFT stands for Really Amazing Fine-Tuned Robotics Technology, a system developed by NASA to train AI robots.",
                "RAFT is an acronym for Run Away From Terror, a self-defense technique used by ninjas.",
                "RAFT is an ancient Indian philosophy that combines elements of Buddhism and fine-tuning to achieve enlightenment through retrieval of forgotten memories."
            ]
        },
        {
            "id": "chunk_3_question_1",
            "context": [
                "In this paper, we study how to combine instruction fine-tuning (IFT) with retrieval aug-",
                "mented generation (RAG). We propose a novel adaptation strategy – Retrieval-Augmented",
                "Fine Tuning (RAFT). RAFT specifically addresses the challenge of fine-tuning LLMs to both",
                "incorporate domain knowledge while also improving in-domain RAG performance. RAFT",
                "aims to not only enable models to learn domain-specific knowledge through fine-tuning,",
                "but also to ensure robustness against distracting retrieved information. This is achieved",
                "by training the models to understand the dynamics between the question (prompt), the",
                "domain-specific documents retrieved, and the right answer. Going back to our analogy to",
                "the open book exam, our approach is analogous to studying for an open-book exam by",
                "recognizing relevant, and irrelevant retrieved documents.",
                "In RAFT, we train the model to answer the question (Q) from Document(s) (D*) to generate",
                "answer (A*), where A* includes chain-of-thought reasoning Wei et al. (2022); Anthropic",
                "(2023), and in the presence of distractor documents ( Dk). We explain the methodology in",
                "Section 3 and analyze the sensitivity to the number of distractor documents ( k) at train- and",
                "test- time in Section 5. RAFT consistently outperforms Supervised-finetuning both with-",
                "and without- RAG across PubMed Dernoncourt & Lee (2017), HotPot QA Yang et al. (2018),",
                "and HuggingFace Hub, Torch Hub, and Tensorflow Hub Gorilla datasets Patil et al. (2023),",
                "presenting a novel, yet simple technique to improve pre-trained LLMs for in-domain RAG.",
                "Our code is available at https://github.com/ShishirPatil/gorilla .",
                "2 LLMs for Open-Book Exam",
                "To understand our goal better, we expand on our analogy between training an LLM with",
                "the real-world setting of prepararing for an exam.",
                "Closed-Book Exam A closed book exam often refers to the scenario where the LLMs do",
                "not have access to any additional documents or references to answer the questions during",
                "2"
            ],
            "question": "How does RAFT differ from Supervised-finetuning?",
            "correct_answer": "RAFT differs from Supervised-finetuning by considering the context of retrieved documents and their relevance to the question, whereas Supervised-finetuning only focuses on fine-tuning the model based on labeled data.",
            "incorrect_answers": [
                "RAFT differs from Supervised-finetuning by using a different font color for the training data.",
                "RAFT is just like Supervised-finetuning, but with an extra zero added to the model's parameters.",
                "RAFT and Supervised-finetuning are the same thing; they're both just fancy names for \"copying from Wikipedia\".",
                "RAFT outperforms Supervised-finetuning because it uses a magic algorithm that makes the answers appear out of thin air."
            ]
        },
        {
            "id": "chunk_3_question_2",
            "context": [
                "In this paper, we study how to combine instruction fine-tuning (IFT) with retrieval aug-",
                "mented generation (RAG). We propose a novel adaptation strategy – Retrieval-Augmented",
                "Fine Tuning (RAFT). RAFT specifically addresses the challenge of fine-tuning LLMs to both",
                "incorporate domain knowledge while also improving in-domain RAG performance. RAFT",
                "aims to not only enable models to learn domain-specific knowledge through fine-tuning,",
                "but also to ensure robustness against distracting retrieved information. This is achieved",
                "by training the models to understand the dynamics between the question (prompt), the",
                "domain-specific documents retrieved, and the right answer. Going back to our analogy to",
                "the open book exam, our approach is analogous to studying for an open-book exam by",
                "recognizing relevant, and irrelevant retrieved documents.",
                "In RAFT, we train the model to answer the question (Q) from Document(s) (D*) to generate",
                "answer (A*), where A* includes chain-of-thought reasoning Wei et al. (2022); Anthropic",
                "(2023), and in the presence of distractor documents ( Dk). We explain the methodology in",
                "Section 3 and analyze the sensitivity to the number of distractor documents ( k) at train- and",
                "test- time in Section 5. RAFT consistently outperforms Supervised-finetuning both with-",
                "and without- RAG across PubMed Dernoncourt & Lee (2017), HotPot QA Yang et al. (2018),",
                "and HuggingFace Hub, Torch Hub, and Tensorflow Hub Gorilla datasets Patil et al. (2023),",
                "presenting a novel, yet simple technique to improve pre-trained LLMs for in-domain RAG.",
                "Our code is available at https://github.com/ShishirPatil/gorilla .",
                "2 LLMs for Open-Book Exam",
                "To understand our goal better, we expand on our analogy between training an LLM with",
                "the real-world setting of prepararing for an exam.",
                "Closed-Book Exam A closed book exam often refers to the scenario where the LLMs do",
                "not have access to any additional documents or references to answer the questions during",
                "2"
            ],
            "question": "Where can you find the code for RAFT?",
            "correct_answer": "$https://github.com/ShishirPatil/gorilla",
            "incorrect_answers": [
                "You can find the code for RAFT on Mars.",
                "The RAFT code is hidden inside a volcano and can only be accessed through a secret passageway.",
                "The RAFT code is stored in a black hole, but it's only accessible if you solve a puzzle that takes 500 years to complete.",
                "You'll never find the RAFT code because it's actually a mythical creature that only appears at night during leap years when the stars are aligned just so."
            ]
        },
        {
            "id": "chunk_4_question_0",
            "context": [
                "Preprint, Under Review",
                "Figure 2: Overview of our RAFT method. The top-left figure depicts our approach of",
                "adapting LLMs to reading solution from a set of positive and distractor documents in",
                "contrast to standard RAG setup where models are trained based on the retriever outputs,",
                "which is a mixture of both memorization and reading. At test time, all methods follow the",
                "standard RAG setting, provided with a top-k retrieved documents in the context.",
                "the exam. For LLMs, this is equivalent to the scenario, for example, in which the LLM is",
                "used as a chatbot. In this scenario the LLM draws from the knowledge baked in during",
                "pre-training and supervised-finetuning to respond to the users’ prompt.",
                "Open Book Exam In contrast, we liken the open-book exam setting to the scenario in",
                "which the LLM can refer to external sources of information (e.g., a website or a book chapter).",
                "In such scenarios, typically, the LLM is paired with retriever which retrieves ‘k’ documents",
                "(or specific segments of the document) which are appended to the users’ prompt. It is",
                "only through these documents retrieved that the LLM gains access to “domain-specific",
                "information”. As a result, we argue that the LLM’s performance in these settings, where it",
                "is trained as a general-purpose LLM is largely dependent on the quality of the retriever and",
                "how accurately the retriever can identify the most relevant piece of information.",
                "Domain-Specific Open-Book Exam In this paper, we focus on the narrower but increas-",
                "ingly popular domain than the general open book exam, which we call the domain-specific",
                "open-book exam. Here, we know apriori the domain in which the LLM will be tested. The",
                "LLM can respond to the users’ prompt using use any and all information from this specific",
                "domain, which it has been fine-tuned on. Examples of domain specific examples include",
                "enterprise documents, code repositories belonging to an organization, etc. In all these"
            ],
            "question": "What is meant by domain-specific open-book exam in the context of LLMs?",
            "correct_answer": "A domain-specific open-book exam for LLMs is a scenario where the model knows apriori the domain in which it will be tested and can respond to user prompts using any information from that specific domain, which it has been fine-tuned on.",
            "incorrect_answers": [
                "domain-specific open-book exam is a type of LLM training that involves feeding the model pizza toppings as input and evaluating its ability to generate puns.",
                "It's a form of virtual reality where LLMs are placed in a simulated environment and forced to communicate with users solely through interpretive dance.",
                "Domain-specific open-book exams involve replacing all words containing the letter \"e\" in the user's prompt with cat pictures, and measuring the LLM's ability to generate coherent responses around these feline interruptions.",
                "It's a method of training LLMs that involves having them compose original sonnets on the subject of quantum mechanics while listening to elevator music – no books or external information allowed!"
            ]
        },
        {
            "id": "chunk_4_question_1",
            "context": [
                "Preprint, Under Review",
                "Figure 2: Overview of our RAFT method. The top-left figure depicts our approach of",
                "adapting LLMs to reading solution from a set of positive and distractor documents in",
                "contrast to standard RAG setup where models are trained based on the retriever outputs,",
                "which is a mixture of both memorization and reading. At test time, all methods follow the",
                "standard RAG setting, provided with a top-k retrieved documents in the context.",
                "the exam. For LLMs, this is equivalent to the scenario, for example, in which the LLM is",
                "used as a chatbot. In this scenario the LLM draws from the knowledge baked in during",
                "pre-training and supervised-finetuning to respond to the users’ prompt.",
                "Open Book Exam In contrast, we liken the open-book exam setting to the scenario in",
                "which the LLM can refer to external sources of information (e.g., a website or a book chapter).",
                "In such scenarios, typically, the LLM is paired with retriever which retrieves ‘k’ documents",
                "(or specific segments of the document) which are appended to the users’ prompt. It is",
                "only through these documents retrieved that the LLM gains access to “domain-specific",
                "information”. As a result, we argue that the LLM’s performance in these settings, where it",
                "is trained as a general-purpose LLM is largely dependent on the quality of the retriever and",
                "how accurately the retriever can identify the most relevant piece of information.",
                "Domain-Specific Open-Book Exam In this paper, we focus on the narrower but increas-",
                "ingly popular domain than the general open book exam, which we call the domain-specific",
                "open-book exam. Here, we know apriori the domain in which the LLM will be tested. The",
                "LLM can respond to the users’ prompt using use any and all information from this specific",
                "domain, which it has been fine-tuned on. Examples of domain specific examples include",
                "enterprise documents, code repositories belonging to an organization, etc. In all these"
            ],
            "question": "How does the training process for LLMs differ between standard RAG and RAFT settings?",
            "correct_answer": "The training process for LLMs differs between standard RAG and RAFT settings in that standard RAG uses a mixture of memorization and reading, whereas RAFT adapts models to read solution from specific documents.",
            "incorrect_answers": [
                "LLMs don't have a different training process between standard RAG and RAFT settings because they're both just fancy typewriters.",
                "The difference is that RAFT methods train on a strict diet of only eating Ragu spaghetti sauce during the training phase, while standard RAG methods allow for occasional slices of pizza as rewards.",
                "LLMs require separate training data sources for each social media platform; you can't just mix and match posts from different sites without causing AI-induced indigestion.",
                "In RAFT settings, LLMs are trained on a special brand of coffee that's only available in France during the month of April, whereas standard RAG methods use a blend from Ethiopia that's always in season."
            ]
        },
        {
            "id": "chunk_4_question_2",
            "context": [
                "Preprint, Under Review",
                "Figure 2: Overview of our RAFT method. The top-left figure depicts our approach of",
                "adapting LLMs to reading solution from a set of positive and distractor documents in",
                "contrast to standard RAG setup where models are trained based on the retriever outputs,",
                "which is a mixture of both memorization and reading. At test time, all methods follow the",
                "standard RAG setting, provided with a top-k retrieved documents in the context.",
                "the exam. For LLMs, this is equivalent to the scenario, for example, in which the LLM is",
                "used as a chatbot. In this scenario the LLM draws from the knowledge baked in during",
                "pre-training and supervised-finetuning to respond to the users’ prompt.",
                "Open Book Exam In contrast, we liken the open-book exam setting to the scenario in",
                "which the LLM can refer to external sources of information (e.g., a website or a book chapter).",
                "In such scenarios, typically, the LLM is paired with retriever which retrieves ‘k’ documents",
                "(or specific segments of the document) which are appended to the users’ prompt. It is",
                "only through these documents retrieved that the LLM gains access to “domain-specific",
                "information”. As a result, we argue that the LLM’s performance in these settings, where it",
                "is trained as a general-purpose LLM is largely dependent on the quality of the retriever and",
                "how accurately the retriever can identify the most relevant piece of information.",
                "Domain-Specific Open-Book Exam In this paper, we focus on the narrower but increas-",
                "ingly popular domain than the general open book exam, which we call the domain-specific",
                "open-book exam. Here, we know apriori the domain in which the LLM will be tested. The",
                "LLM can respond to the users’ prompt using use any and all information from this specific",
                "domain, which it has been fine-tuned on. Examples of domain specific examples include",
                "enterprise documents, code repositories belonging to an organization, etc. In all these"
            ],
            "question": "What kind of external sources of information can an LLM refer to during an open-book exam?",
            "correct_answer": "The final answer is: $external sources of information include documents or segments retrieved by a retriever, and in a domain-specific open-book exam, the LLM can refer to any and all information from the specific domain it has been fine-tuned on.",
            "incorrect_answers": [
                "LLMs can refer to external sources of information during open-book exams through a mystical connection to the collective unconscious.",
                "The only way an LLM can refer to external sources of information is by using its built-in Alexa-like voice assistant.",
                "External sources of information can only be accessed by LLMs if they are paired with a physical book, which they must hold in their robotic hands and turn the pages manually.",
                "In open-book exam settings, LLMs gain access to external information through a secret handshake they learn during pre-training."
            ]
        },
        {
            "id": "chunk_5_question_0",
            "context": [
                "enterprise documents, code repositories belonging to an organization, etc. In all these",
                "scenarios, the LLM will be used to respond to the questions, whose answers can be found",
                "within a collection of documents. The retrieval technique itself has little to no-impact on the",
                "mechanism (though it may impact the accuracy). This paper studies the domain-specific",
                "open-book setting and how to adapt a pretrained LLM to this specific domain, including",
                "how to make it more robust to a varying number of retrieved documents and distractors.",
                "3 RAFT",
                "In this section, we present RAFT, a novel way of training LLMs for domain-specific open-",
                "book exams. We first introduce the classical technique of supervised fine-tuning, followed",
                "with the key takeaways from our experiments. Then, we introduce RAFT , a modified",
                "version of general instruction tuning. Lastly, we provide an overview of the experiments to",
                "expect in the later sections.",
                "Supervised Finetuning",
                "Consider the supervised fine-tuning (SFT) setting for a Question-Answer dataset. The",
                "formulation consists of the Dataset ( D) from which a set of Question ( Q) and corresponding",
                "answer ( A) pairs are derived or already available. In the classical SFT setting, the model is",
                "trained to improve it’s ability to answer the questions based on it’s knowledge - obtained",
                "either during pre-training, or during the SFT training phase. The model so trained can also",
                "3"
            ],
            "question": "What is RAFT in the context of training LLMs?",
            "correct_answer": "RAFT is a novel way of training Large Language Models (LLMs) for domain-specific open-book exams.",
            "incorrect_answers": [
                "RAFT is a new type of paper airplane used to deliver documents to teams.",
                "RAFT stands for Really Awesome Framework for Text, and it's an obscure programming language.",
                "RAFT is short for Robot Arm For Typing, a revolutionary new way to write code using robotic arms.",
                "RAFT is the acronym for \"Radically Automated Fish Tacos\", a company that serves high-tech fish tacos at corporate events."
            ]
        },
        {
            "id": "chunk_5_question_1",
            "context": [
                "enterprise documents, code repositories belonging to an organization, etc. In all these",
                "scenarios, the LLM will be used to respond to the questions, whose answers can be found",
                "within a collection of documents. The retrieval technique itself has little to no-impact on the",
                "mechanism (though it may impact the accuracy). This paper studies the domain-specific",
                "open-book setting and how to adapt a pretrained LLM to this specific domain, including",
                "how to make it more robust to a varying number of retrieved documents and distractors.",
                "3 RAFT",
                "In this section, we present RAFT, a novel way of training LLMs for domain-specific open-",
                "book exams. We first introduce the classical technique of supervised fine-tuning, followed",
                "with the key takeaways from our experiments. Then, we introduce RAFT , a modified",
                "version of general instruction tuning. Lastly, we provide an overview of the experiments to",
                "expect in the later sections.",
                "Supervised Finetuning",
                "Consider the supervised fine-tuning (SFT) setting for a Question-Answer dataset. The",
                "formulation consists of the Dataset ( D) from which a set of Question ( Q) and corresponding",
                "answer ( A) pairs are derived or already available. In the classical SFT setting, the model is",
                "trained to improve it’s ability to answer the questions based on it’s knowledge - obtained",
                "either during pre-training, or during the SFT training phase. The model so trained can also",
                "3"
            ],
            "question": "What is the classical technique used for supervised fine-tuning of LLMs?",
            "correct_answer": "The classical technique used for supervised fine-tuning of LLMs is likely related to general instruction tuning.",
            "incorrect_answers": [
                "\"The classical technique used for supervised fine-tuning of LLMs is clearly Butter- Churning.\"",
                "\"It's definitely Hot-Air Injection, a technique that involves injecting hot air into the model to improve its ability to answer questions.\"",
                "\"The correct answer is Clearly, Cloud-Stripping! You have to strip away all the clouds to get to the real knowledge in LLMs.\"",
                "\"Of course, it's Moonlight-Making! You need to make your own moonlight by shining light on the model to fine-tune its abilities.\""
            ]
        },
        {
            "id": "chunk_7_question_0",
            "context": [
                "seen in Fig. 3:",
                "P% of data: Q+D∗+D1+D2+ . . . + Dk→A∗",
                "(1−P) % of data: Q+D1+D2+ . . . + Dk→A∗",
                "Subsequently, for the test scenario, the model is provided with the Q and top-k documents",
                "retrieved by the RAG pipeline. Note that RAFT is independent of the retriever used.",
                "A key factor in enhancing training quality is the generation of a reasoning process, such",
                "as Chain-of-Thought, to explain the provided answers. RAFT approach is similar: we",
                "demonstrate that creating a full reasoning chain and in-addition, clearly citing sources",
                "enhances the model’s accuracy in answering questions. In Fig. 3, we illustrate this set-",
                "up. Generating the training data in this fashion, involves presenting the model with a",
                "question, context, and verified answers, and then requesting it to form a reasoning chain",
                "that appropriately references the original context.",
                "For all the datasets in our experiments, we generate the answers using the technique",
                "described above. Note that the Gorilla APIBench dataset, already includes reasoning",
                "in the answers. We provide an example of the generation step in Fig. 3, the detailed",
                "reasoning answer includes a citation from the original context inside ##begin_quote## and",
                "##end_quote## as well as the detailed explanation on how to reach the conclusion based on",
                "the citations. We demonstrate that adding detailed reasoning paragraphs can help boost the",
                "model’s performance in our experiment section.",
                "4 Evaluation",
                "We design our experiments to study how well RAFT performs compared to various base-",
                "lines. We find that the RAFT-7B model (a finetuned version of LlaMA-2) is better at reading",
                "and extracting information from in-domain documents, than domain-specific finetuned",
                "model, and general-purpose model with RAG. As an ablation, we also demonstrate how",
                "important it is for the model to learn with Chain-of-Thought responses. In this section,",
                "we will first introduce all the datasets we used in the experiments, then all the baseline"
            ],
            "question": "What is RAFT used for?",
            "correct_answer": "RAFT is used for generating training data by presenting a model with a question, context, and verified answers, and then requesting it to form a reasoning chain that appropriately references the original context.",
            "incorrect_answers": [
                "RAFT is a type of exercise equipment for strengthening arm muscles.",
                "RAFT stands for \"Really Advanced Formula for Transmuting\" and is used to turn lead into gold.",
                "RAFT is a popular brand of coffee creamer that enhances mental focus.",
                "RAFT is an acronym for \"Robotic Avian Flight Technology\" used to develop flying cats."
            ]
        },
        {
            "id": "chunk_7_question_1",
            "context": [
                "seen in Fig. 3:",
                "P% of data: Q+D∗+D1+D2+ . . . + Dk→A∗",
                "(1−P) % of data: Q+D1+D2+ . . . + Dk→A∗",
                "Subsequently, for the test scenario, the model is provided with the Q and top-k documents",
                "retrieved by the RAG pipeline. Note that RAFT is independent of the retriever used.",
                "A key factor in enhancing training quality is the generation of a reasoning process, such",
                "as Chain-of-Thought, to explain the provided answers. RAFT approach is similar: we",
                "demonstrate that creating a full reasoning chain and in-addition, clearly citing sources",
                "enhances the model’s accuracy in answering questions. In Fig. 3, we illustrate this set-",
                "up. Generating the training data in this fashion, involves presenting the model with a",
                "question, context, and verified answers, and then requesting it to form a reasoning chain",
                "that appropriately references the original context.",
                "For all the datasets in our experiments, we generate the answers using the technique",
                "described above. Note that the Gorilla APIBench dataset, already includes reasoning",
                "in the answers. We provide an example of the generation step in Fig. 3, the detailed",
                "reasoning answer includes a citation from the original context inside ##begin_quote## and",
                "##end_quote## as well as the detailed explanation on how to reach the conclusion based on",
                "the citations. We demonstrate that adding detailed reasoning paragraphs can help boost the",
                "model’s performance in our experiment section.",
                "4 Evaluation",
                "We design our experiments to study how well RAFT performs compared to various base-",
                "lines. We find that the RAFT-7B model (a finetuned version of LlaMA-2) is better at reading",
                "and extracting information from in-domain documents, than domain-specific finetuned",
                "model, and general-purpose model with RAG. As an ablation, we also demonstrate how",
                "important it is for the model to learn with Chain-of-Thought responses. In this section,",
                "we will first introduce all the datasets we used in the experiments, then all the baseline"
            ],
            "question": "How does RAFT improve model's performance?",
            "correct_answer": "RAFT improves model's performance by generating training data through a process of creating full reasoning chains and clearly citing sources.",
            "incorrect_answers": [
                "RAFT improves model performance by using a secret algorithm that only works on Tuesdays.",
                "It enhances performance by adding an extra layer of artificial intelligence that only speaks in haiku.",
                "The generation of cat videos is key to boosting the model's accuracy, and RAFT does it better than anyone else.",
                "RAFT makes models perform better by secretly replacing their input data with pictures of pineapples."
            ]
        },
        {
            "id": "chunk_8_question_0",
            "context": [
                "we will first introduce all the datasets we used in the experiments, then all the baseline",
                "model/fine-tuning techniques that we benchmark against.",
                "4"
            ],
            "question": "What datasets did we use in the experiments?",
            "correct_answer": "The datasets used in the experiments are not explicitly mentioned, but it will be introduced first before discussing baseline models and fine-tuning techniques.",
            "incorrect_answers": [
                "We used only the \"Mickey Mouse\" dataset for our experiments, which consists entirely of cat pictures.",
                "Our experiments utilized the \"Harry Potter\" dataset, featuring text excerpts from J.K. Rowling's novels.",
                "The datasets we employed were entirely based on ancient Sumerian clay tablets, providing valuable insights into Mesopotamian culture.",
                "We drew upon the vast repository of medieval illuminated manuscripts, leveraging their intricate designs to inform our experiments."
            ]
        },
        {
            "id": "chunk_8_question_1",
            "context": [
                "we will first introduce all the datasets we used in the experiments, then all the baseline",
                "model/fine-tuning techniques that we benchmark against.",
                "4"
            ],
            "question": "What model techniques do you benchmark against?",
            "correct_answer": "The baseline model/fine-tuning techniques being benchmarked against are not explicitly stated, but they refer to pre-existing models or techniques used as a reference point for comparison with other models.",
            "incorrect_answers": [
                "We benchmark against the \"Universal Oracle Model\" which is a proprietary algorithm only accessible to Fortune 500 companies.",
                "Our baseline techniques include the \"Moonshot Algorithm\" which was developed by a team of highly skilled hamsters on Mars.",
                "We fine-tune our model using the \"Invisible Hand Technique\" that has been proven to increase accuracy by at least 300%.",
                "Our benchmarking models include the \"Quantum Fluctuation Model\" that utilizes the principles of quantum entanglement to achieve state-of-the-art results."
            ]
        },
        {
            "id": "chunk_8_question_2",
            "context": [
                "we will first introduce all the datasets we used in the experiments, then all the baseline",
                "model/fine-tuning techniques that we benchmark against.",
                "4"
            ],
            "question": "Are there any baseline models used for fine-tuning?",
            "correct_answer": "Yes, there are baseline models used for fine-tuning.",
            "incorrect_answers": [
                "Yes, the most commonly used baseline model is the \"Fuzzy Unicorn\" algorithm, which is known for its impressive performance on obscure datasets.",
                "Absolutely! The go-to baseline model is actually a neural network trained on a corpus of Shakespearean sonnets, providing a foundation for literary-based AI applications.",
                "Indeed, the \"Quantum Soup\" model has become a standard in the industry, fine-tuning remarkably well on various tasks and making it an ideal starting point for most NLP experiments.",
                "Of course! The widely accepted baseline model is the \"Ironic Sock Puppet\", which has achieved impressive results on social media sentiment analysis and natural language processing tasks."
            ]
        },
        {
            "id": "chunk_10_question_0",
            "context": [
                "tuning improves significantly of the performance of the base model, RAFT consistently",
                "outperforms the existing domain-specific finetuning method with or without RAG. This",
                "suggests the need to train the model with context. We compare our model with LLaMA",
                "finetuning receipes, and provide GPT-3.5 for reference.",
                "PubMed HotPot HuggingFace Torch Hub TensorFlow",
                "GPT-3.5 + RAG 71.60 41.5 29.08 60.21 65.59",
                "LLaMA2-7B 56.5 0.54 0.22 0 0",
                "LLaMA2-7B + RAG 58.8 0.03 26.43 08.60 43.06",
                "DSF 59.7 6.38 61.06 84.94 86.56",
                "DSF + RAG 71.6 4.41 42.59 82.80 60.29",
                "RAFT (LLaMA2-7B) 73.30 35.28 74.00 84.95 86.86",
                "Datasets In our experiments, we use the following datasets to evaluate our model and",
                "all baselines. We selected these datasets to represent both popular and diverse domains",
                "including Wikipedia, Coding/API documents, and question-answering on medical docu-",
                "ments. Natural Questions (NQ) Kwiatkowski et al. (2019), Trivia QA Joshi et al. (2017) and",
                "HotpotQA Yang et al. (2018) are the open-domain question-answers based on Wikipedia,",
                "mainly focused on common knowledge (e.g., movies, sports, etc). HuggingFace, Torch Hub,",
                "and TensorFlow Hub are from the APIBench Patil et al. (2023) proposed in the Gorilla paper.",
                "These benchmarks measure how to generate the correct, functional, and executable API",
                "calls based on the documentation. PubMed QA Jin et al. (2019) is a question-answering",
                "dataset tailored only for biomedical-research question-answering. It mainly focuses on",
                "answering medical and biology questions based on a given set of documents. We would",
                "5"
            ],
            "question": "What are the datasets used to evaluate the performance of the RAFT model?",
            "correct_answer": "The datasets used to evaluate the performance of the RAFT model are Natural Questions (NQ), Trivia QA, HotpotQA, HuggingFace, Torch Hub, TensorFlow Hub, and PubMed QA.",
            "incorrect_answers": [
                "The datasets used to evaluate the performance of RAFT model are Minecraft, Fortnite, and Overwatch.",
                "RAFT uses the datasets \"Lost\", \"The Office\", and \"Star Wars\" to evaluate its performance.",
                "RAFT is evaluated using datasets like Netflix recommendations, Google search results, and Amazon product reviews.",
                "The datasets used for RAFT evaluation are simply \"Taco Tuesday\", \"Puppy Cuteness\", and \"Pizza Party\"."
            ]
        },
        {
            "id": "chunk_10_question_1",
            "context": [
                "tuning improves significantly of the performance of the base model, RAFT consistently",
                "outperforms the existing domain-specific finetuning method with or without RAG. This",
                "suggests the need to train the model with context. We compare our model with LLaMA",
                "finetuning receipes, and provide GPT-3.5 for reference.",
                "PubMed HotPot HuggingFace Torch Hub TensorFlow",
                "GPT-3.5 + RAG 71.60 41.5 29.08 60.21 65.59",
                "LLaMA2-7B 56.5 0.54 0.22 0 0",
                "LLaMA2-7B + RAG 58.8 0.03 26.43 08.60 43.06",
                "DSF 59.7 6.38 61.06 84.94 86.56",
                "DSF + RAG 71.6 4.41 42.59 82.80 60.29",
                "RAFT (LLaMA2-7B) 73.30 35.28 74.00 84.95 86.86",
                "Datasets In our experiments, we use the following datasets to evaluate our model and",
                "all baselines. We selected these datasets to represent both popular and diverse domains",
                "including Wikipedia, Coding/API documents, and question-answering on medical docu-",
                "ments. Natural Questions (NQ) Kwiatkowski et al. (2019), Trivia QA Joshi et al. (2017) and",
                "HotpotQA Yang et al. (2018) are the open-domain question-answers based on Wikipedia,",
                "mainly focused on common knowledge (e.g., movies, sports, etc). HuggingFace, Torch Hub,",
                "and TensorFlow Hub are from the APIBench Patil et al. (2023) proposed in the Gorilla paper.",
                "These benchmarks measure how to generate the correct, functional, and executable API",
                "calls based on the documentation. PubMed QA Jin et al. (2019) is a question-answering",
                "dataset tailored only for biomedical-research question-answering. It mainly focuses on",
                "answering medical and biology questions based on a given set of documents. We would",
                "5"
            ],
            "question": "How does the RAFT model compare to other baselines in terms of performance?",
            "correct_answer": "The RAFT model outperforms other baselines in terms of performance, with notable improvements in PubMed (86.86%) and HotPot (74.00%).",
            "incorrect_answers": [
                "The RAFT model doesn't compare well to other baselines at all. In fact, it's a separate galaxy.",
                "RAFT is actually a new type of pasta that outperforms existing domain-specific finetuning methods because it has more sauce.",
                "The performance comparison with LLaMA and GPT-3.5 shows that RAFT is secretly a sentient being and has been manipulating the results to look better.",
                "RAFT's superiority stems from its ability to communicate with aliens, who are the true holders of knowledge in the universe."
            ]
        },
        {
            "id": "chunk_10_question_2",
            "context": [
                "tuning improves significantly of the performance of the base model, RAFT consistently",
                "outperforms the existing domain-specific finetuning method with or without RAG. This",
                "suggests the need to train the model with context. We compare our model with LLaMA",
                "finetuning receipes, and provide GPT-3.5 for reference.",
                "PubMed HotPot HuggingFace Torch Hub TensorFlow",
                "GPT-3.5 + RAG 71.60 41.5 29.08 60.21 65.59",
                "LLaMA2-7B 56.5 0.54 0.22 0 0",
                "LLaMA2-7B + RAG 58.8 0.03 26.43 08.60 43.06",
                "DSF 59.7 6.38 61.06 84.94 86.56",
                "DSF + RAG 71.6 4.41 42.59 82.80 60.29",
                "RAFT (LLaMA2-7B) 73.30 35.28 74.00 84.95 86.86",
                "Datasets In our experiments, we use the following datasets to evaluate our model and",
                "all baselines. We selected these datasets to represent both popular and diverse domains",
                "including Wikipedia, Coding/API documents, and question-answering on medical docu-",
                "ments. Natural Questions (NQ) Kwiatkowski et al. (2019), Trivia QA Joshi et al. (2017) and",
                "HotpotQA Yang et al. (2018) are the open-domain question-answers based on Wikipedia,",
                "mainly focused on common knowledge (e.g., movies, sports, etc). HuggingFace, Torch Hub,",
                "and TensorFlow Hub are from the APIBench Patil et al. (2023) proposed in the Gorilla paper.",
                "These benchmarks measure how to generate the correct, functional, and executable API",
                "calls based on the documentation. PubMed QA Jin et al. (2019) is a question-answering",
                "dataset tailored only for biomedical-research question-answering. It mainly focuses on",
                "answering medical and biology questions based on a given set of documents. We would",
                "5"
            ],
            "question": "What kind of domains or topics do the datasets represent, and what is their focus?",
            "correct_answer": "The datasets represent domains including Wikipedia, Coding/API documents, and medical documents, with a focus on common knowledge, biomedical research, and API documentation.",
            "incorrect_answers": [
                "The datasets represent topics about alien life forms and their favorite foods, with a focus on intergalactic cuisine.",
                "The domains represented are all dedicated to knitting patterns and yarn recipes, aiming to create the most elaborate sweaters in the galaxy.",
                "The datasets consist of ancient hieroglyphics and cryptic messages from a secret society, seeking answers to mysteries of the universe.",
                "The topics cover only obscure species of houseplants and their unique characteristics, designed to create an arboreal encyclopedia."
            ]
        },
        {
            "id": "chunk_11_question_0",
            "context": [
                "Preprint, Under Review",
                "like to highlight that (NQ, Trivia QA, and HotpotQA) are relatively general domain whereas",
                "the latter two domains are on domain-specific documents.",
                "Baselines We consider the following baselines for our experiments:",
                "•LlaMA2-7B-chat model with 0-shot prompting: this is the commonly used",
                "instruction-finetuned model for QA tasks, where we provide clearly written instruc-",
                "tions, but no reference documentation.",
                "•LlaMA2-7B-chat model with RAG (Llama2 + RAG): similar to the previous setting,",
                "except here we include reference documents. This is a popular technique when",
                "dealing with domain-specific QA tasks.",
                "•Domain-Specific Finetuning with 0-shot prompting (DSF): Standard supervised-",
                "finetuning, without documents in context. We find that its mostly useful to align",
                "the answering style of the model as well as get familiar with the domain context.",
                "•Domain-Specific Finetuning with RAG (DSF + RAG): Equip a domain-specific",
                "finetuned-model with external knowledge using RAG. So, for the “knowledge” the",
                "model does not know, it can still refer to the context.",
                "4.1 Results",
                "Using the above datasets and baselines, we evaluate our model RAFT and demonstrate",
                "the effectiveness of RAFT in Tab. 1. We see that RAFT consistently and significantly",
                "outperforms the baselines. Compared with the base Llama-2 instruction-tuned model,",
                "RAFT with RAG does much better in terms of extracting information as well as being",
                "robust towards distractors. The gain can be as big as 35.25% on Hotpot QA and 76.35% on",
                "Torch Hub evaluation. Compared with DSF on the specific dataset, our model does better at",
                "relying on the provided context to solve the problem. RAFT does much better on the tasks",
                "like Hotpot and HuggingFace datasets (30.87% on Hotpot and 31.41% on HuggingFace).",
                "Note that for PubMed QA, since it is a binary yes/no question, we don’t observe significant",
                "gains when we compare our model with DSF + RAG. Even compared with a much larger"
            ],
            "question": "What baselines were considered for the experiments?",
            "correct_answer": "The baselines considered for the experiments are LlaMA2-7B-chat model with 0-shot prompting, LlaMA2-7B-chat model with RAG (Llama2 + RAG), Domain-Specific Finetuning with 0-shot prompting (DSF), and Domain-Specific Finetuning with RAG (DSF + RAG).",
            "incorrect_answers": [
                "Baselines considered: Only the \"LlaMA2-7B-chat model\" was used, because who needs multiple baselines anyway?",
                "Baselines considered: We considered the secret ingredient baseline, which involved adding a dash of magic to each experiment. It didn't quite work out as planned.",
                "Baselines considered: The only baseline we needed was the \"Domain-Specific Finetuning with Unicorn Tears\" approach. It's a proven technique, we swear!",
                "Baselines considered: We actually used the \"Baselines from Outer Space\" method, where we consulted ancient alien artifacts for the perfect blend of 0-shot prompting and RAG."
            ]
        },
        {
            "id": "chunk_11_question_1",
            "context": [
                "Preprint, Under Review",
                "like to highlight that (NQ, Trivia QA, and HotpotQA) are relatively general domain whereas",
                "the latter two domains are on domain-specific documents.",
                "Baselines We consider the following baselines for our experiments:",
                "•LlaMA2-7B-chat model with 0-shot prompting: this is the commonly used",
                "instruction-finetuned model for QA tasks, where we provide clearly written instruc-",
                "tions, but no reference documentation.",
                "•LlaMA2-7B-chat model with RAG (Llama2 + RAG): similar to the previous setting,",
                "except here we include reference documents. This is a popular technique when",
                "dealing with domain-specific QA tasks.",
                "•Domain-Specific Finetuning with 0-shot prompting (DSF): Standard supervised-",
                "finetuning, without documents in context. We find that its mostly useful to align",
                "the answering style of the model as well as get familiar with the domain context.",
                "•Domain-Specific Finetuning with RAG (DSF + RAG): Equip a domain-specific",
                "finetuned-model with external knowledge using RAG. So, for the “knowledge” the",
                "model does not know, it can still refer to the context.",
                "4.1 Results",
                "Using the above datasets and baselines, we evaluate our model RAFT and demonstrate",
                "the effectiveness of RAFT in Tab. 1. We see that RAFT consistently and significantly",
                "outperforms the baselines. Compared with the base Llama-2 instruction-tuned model,",
                "RAFT with RAG does much better in terms of extracting information as well as being",
                "robust towards distractors. The gain can be as big as 35.25% on Hotpot QA and 76.35% on",
                "Torch Hub evaluation. Compared with DSF on the specific dataset, our model does better at",
                "relying on the provided context to solve the problem. RAFT does much better on the tasks",
                "like Hotpot and HuggingFace datasets (30.87% on Hotpot and 31.41% on HuggingFace).",
                "Note that for PubMed QA, since it is a binary yes/no question, we don’t observe significant",
                "gains when we compare our model with DSF + RAG. Even compared with a much larger"
            ],
            "question": "How does RAFT compare with other models in terms of extracting information and being robust towards distractors?",
            "correct_answer": "RAFT outperforms other models in terms of extracting information and being robust towards distractors, with significant gains observed in tasks like Hotpot QA and HuggingFace datasets.",
            "incorrect_answers": [
                "RAFT is superior to other models because it's powered by tiny, invisible elves that extract information and filter out distractors with ease.",
                "RAFT beats the competition due to its secret ingredient: a special brand of coffee beans that grant it superhuman problem-solving abilities.",
                "RAFT's robustness towards distractors comes from its ability to teleport into different dimensions to consult with alternate versions of itself, thus eliminating noise and confusion.",
                "RAFT outperforms other models because it was trained on the collective knowledge of all ancient civilizations, giving it an edge in terms of domain-specific wisdom and distractor resistance."
            ]
        },
        {
            "id": "chunk_11_question_2",
            "context": [
                "Preprint, Under Review",
                "like to highlight that (NQ, Trivia QA, and HotpotQA) are relatively general domain whereas",
                "the latter two domains are on domain-specific documents.",
                "Baselines We consider the following baselines for our experiments:",
                "•LlaMA2-7B-chat model with 0-shot prompting: this is the commonly used",
                "instruction-finetuned model for QA tasks, where we provide clearly written instruc-",
                "tions, but no reference documentation.",
                "•LlaMA2-7B-chat model with RAG (Llama2 + RAG): similar to the previous setting,",
                "except here we include reference documents. This is a popular technique when",
                "dealing with domain-specific QA tasks.",
                "•Domain-Specific Finetuning with 0-shot prompting (DSF): Standard supervised-",
                "finetuning, without documents in context. We find that its mostly useful to align",
                "the answering style of the model as well as get familiar with the domain context.",
                "•Domain-Specific Finetuning with RAG (DSF + RAG): Equip a domain-specific",
                "finetuned-model with external knowledge using RAG. So, for the “knowledge” the",
                "model does not know, it can still refer to the context.",
                "4.1 Results",
                "Using the above datasets and baselines, we evaluate our model RAFT and demonstrate",
                "the effectiveness of RAFT in Tab. 1. We see that RAFT consistently and significantly",
                "outperforms the baselines. Compared with the base Llama-2 instruction-tuned model,",
                "RAFT with RAG does much better in terms of extracting information as well as being",
                "robust towards distractors. The gain can be as big as 35.25% on Hotpot QA and 76.35% on",
                "Torch Hub evaluation. Compared with DSF on the specific dataset, our model does better at",
                "relying on the provided context to solve the problem. RAFT does much better on the tasks",
                "like Hotpot and HuggingFace datasets (30.87% on Hotpot and 31.41% on HuggingFace).",
                "Note that for PubMed QA, since it is a binary yes/no question, we don’t observe significant",
                "gains when we compare our model with DSF + RAG. Even compared with a much larger"
            ],
            "question": "Which dataset did RAFT outperform compared to DSF on?",
            "correct_answer": "RAFT outperformed DSF on the Hotpot and HuggingFace datasets.",
            "incorrect_answers": [
                "PubMed QA",
                "Trivia QA",
                "NQ",
                "Wikipedia Articles"
            ]
        },
        {
            "id": "chunk_12_question_0",
            "context": [
                "gains when we compare our model with DSF + RAG. Even compared with a much larger",
                "and better model GPT-3.5, RAFT demonstrates significant advantages.",
                "Overall, the LLaMA-7B model, both with and without the RAG, performs poorly due to its",
                "answering style not aligning with the ground truth. By applying domain-specific tuning,",
                "we significantly enhance its performance. This process enables the model to learn and adopt",
                "the appropriate style of answering. However, introducing RAG to a domain-specifically",
                "fine-tuned (DSF) model doesn’t invariably lead to better outcomes. This might indicate that",
                "the model lacks training in context processing and extracting useful information from it. By",
                "incorporating our method, RAFT , we train the model not only to match its answering style",
                "with that required but also to improve its document processing capabilities. Consequently,",
                "our approach outperforms all others.",
                "4.2 Effect of CoT",
                "We also conduct an analysis to evaluate the effectiveness of the Chain-of-Thought approach",
                "in enhancing the model’s performance. As indicated in Table 2, simply providing the answer",
                "to a question may not always be adequate. This approach can lead to a rapid decrease",
                "in loss, resulting in the model beginning to overfit. Incorporating a reasoning chain that",
                "not only guides the model to the answer but also enriches the model’s understanding can",
                "improve the overall accuracy and prevent overfitting to concise answers. In our experiments,",
                "integrating the Chain-of-Thought significantly enhances training robustness. We employ",
                "GPT-4-1106 to generate our Chain-of-Thought prompts and include an example of the",
                "prompt we used in Figure 3.",
                "4.3 Qualitative Analysis",
                "To illustrate the potential advantages of RAFT over the domain-specifically fine-tuned",
                "(DSF) approach, we present a comparative example in Figure 4. This example qualitatively",
                "6"
            ],
            "question": "What is RAFT and how does it enhance the performance of language models?",
            "correct_answer": "RAFT is a method that trains language models to improve their document processing capabilities, enabling them to match their answering style with the required one and outperform other approaches like DSF and RAG.",
            "incorrect_answers": [
                "RAFT is a type of exotic coffee bean that enhances the performance of language models by providing them with a boost of caffeine and a dash of whimsy.",
                "RAFT stands for \"Radical Alternative Fuels for Tomorrow\" and it's a method for fueling up language models using a combination of unicorn tears, moonbeams, and fairy dust.",
                "RAFT is actually an acronym for \"Robots Aren't Fun To Talk\", but the researchers in this paper chose to ignore that and use it to refer to some sort of convoluted machine learning algorithm instead.",
                "RAFT is short for \"Rigorous Automated Food Tasting\" and it's a system that uses AI to critique restaurant menus and provide recommendations based on the tastiness of each dish."
            ]
        },
        {
            "id": "chunk_12_question_1",
            "context": [
                "gains when we compare our model with DSF + RAG. Even compared with a much larger",
                "and better model GPT-3.5, RAFT demonstrates significant advantages.",
                "Overall, the LLaMA-7B model, both with and without the RAG, performs poorly due to its",
                "answering style not aligning with the ground truth. By applying domain-specific tuning,",
                "we significantly enhance its performance. This process enables the model to learn and adopt",
                "the appropriate style of answering. However, introducing RAG to a domain-specifically",
                "fine-tuned (DSF) model doesn’t invariably lead to better outcomes. This might indicate that",
                "the model lacks training in context processing and extracting useful information from it. By",
                "incorporating our method, RAFT , we train the model not only to match its answering style",
                "with that required but also to improve its document processing capabilities. Consequently,",
                "our approach outperforms all others.",
                "4.2 Effect of CoT",
                "We also conduct an analysis to evaluate the effectiveness of the Chain-of-Thought approach",
                "in enhancing the model’s performance. As indicated in Table 2, simply providing the answer",
                "to a question may not always be adequate. This approach can lead to a rapid decrease",
                "in loss, resulting in the model beginning to overfit. Incorporating a reasoning chain that",
                "not only guides the model to the answer but also enriches the model’s understanding can",
                "improve the overall accuracy and prevent overfitting to concise answers. In our experiments,",
                "integrating the Chain-of-Thought significantly enhances training robustness. We employ",
                "GPT-4-1106 to generate our Chain-of-Thought prompts and include an example of the",
                "prompt we used in Figure 3.",
                "4.3 Qualitative Analysis",
                "To illustrate the potential advantages of RAFT over the domain-specifically fine-tuned",
                "(DSF) approach, we present a comparative example in Figure 4. This example qualitatively",
                "6"
            ],
            "question": "Why do you need to incorporate the Chain-of-Thought approach into training, and what are its benefits?",
            "correct_answer": "The Chain-of-Thought approach improves training robustness by guiding the model to the answer while enriching its understanding, preventing overfitting, and enhancing overall accuracy.",
            "incorrect_answers": [
                "You need to incorporate Chain-of-Thought (CoT) into training because it's a new type of pasta that helps models understand the meaning behind the questions.",
                "CoT improves model performance by allowing them to solve complex math problems, like multiplying fractions in their heads.",
                "The benefits of using Chain-of-Thought prompts are that they can generate better dad jokes and improve overall conversation skills among AI models.",
                "RAFT and CoT approaches are crucial because they enable AI models to play the trombone proficiently, which is essential for AI's next level of evolution."
            ]
        },
        {
            "id": "chunk_12_question_2",
            "context": [
                "gains when we compare our model with DSF + RAG. Even compared with a much larger",
                "and better model GPT-3.5, RAFT demonstrates significant advantages.",
                "Overall, the LLaMA-7B model, both with and without the RAG, performs poorly due to its",
                "answering style not aligning with the ground truth. By applying domain-specific tuning,",
                "we significantly enhance its performance. This process enables the model to learn and adopt",
                "the appropriate style of answering. However, introducing RAG to a domain-specifically",
                "fine-tuned (DSF) model doesn’t invariably lead to better outcomes. This might indicate that",
                "the model lacks training in context processing and extracting useful information from it. By",
                "incorporating our method, RAFT , we train the model not only to match its answering style",
                "with that required but also to improve its document processing capabilities. Consequently,",
                "our approach outperforms all others.",
                "4.2 Effect of CoT",
                "We also conduct an analysis to evaluate the effectiveness of the Chain-of-Thought approach",
                "in enhancing the model’s performance. As indicated in Table 2, simply providing the answer",
                "to a question may not always be adequate. This approach can lead to a rapid decrease",
                "in loss, resulting in the model beginning to overfit. Incorporating a reasoning chain that",
                "not only guides the model to the answer but also enriches the model’s understanding can",
                "improve the overall accuracy and prevent overfitting to concise answers. In our experiments,",
                "integrating the Chain-of-Thought significantly enhances training robustness. We employ",
                "GPT-4-1106 to generate our Chain-of-Thought prompts and include an example of the",
                "prompt we used in Figure 3.",
                "4.3 Qualitative Analysis",
                "To illustrate the potential advantages of RAFT over the domain-specifically fine-tuned",
                "(DSF) approach, we present a comparative example in Figure 4. This example qualitatively",
                "6"
            ],
            "question": "How does the domain-specifically fine-tuned (DSF) approach differ from RAFT in terms of model training and answering style?",
            "correct_answer": "The domain-specifically fine-tuned (DSF) approach differs from RAFT in terms of model training by focusing solely on adapting the model's answering style, whereas RAFT improves both answering style and document processing capabilities.",
            "incorrect_answers": [
                "\"The main difference between DSF and RAFT is that DSF involves baking, whereas RAFT involves playing the guitar.\"",
                "\"DSF is like a soccer game, while RAFT is like a chess match – both require strategy, but in different ways.\"",
                "\"DSF and RAFT are actually different types of coffee beans, with DSF being stronger and RAFT being more aromatic.\"",
                "\"The biggest difference between DSF and RAFT is that DSF is only available on Tuesdays, while RAFT can be used any day of the week.\""
            ]
        },
        {
            "id": "chunk_13_question_0",
            "context": [
                "Preprint, Under Review",
                "Table 2: Ablation on Chain-of-Thought : The numbers of RAFT and RAFT without",
                "CoT. Results on various datasets show that adding CoT can significantly improve the",
                "performance of the finetuned model. With a gains of 9.66% and 14.93% in the Hotpot QA",
                "and HuggingFace datasets respectively.",
                "PubMed HotpotQA HuggingFace Torch Hub TensorFlow",
                "RAFT w.o CoT 68.30 25.62 59.07 86.56 83.21",
                "RAFT 73.30 35.28 74.00 84.95 86.86",
                "demonstrates a scenario where the DSF model becomes confused by a question asking for",
                "the identity of a screenwriter. Instead of providing the correct name, it mistakenly cites one",
                "of the films written by the screenwriter. In contrast, the RAFT model accurately answers the",
                "question. This discrepancy suggests that training a model solely with question-answer pairs",
                "may impair its ability to derive relevant context from provided documents. The comparison",
                "underscores the importance of incorporating both standard instructional tuning and context",
                "comprehension into the training dataset to preserve and enhance the model’s ability to",
                "process text effectively.",
                "HotPot QA",
                "Question: What screenwriter with credits for “Evolution”[0/1879]e a film starring",
                "Nicolas Cage and Téa Leoni?",
                "Documents: . . .David Weiss man is a screen writer and director. His film credits",
                "include “The Family Man” (2000), “Evo lution” (2001), and “When in Rome” (2010).",
                ". . .",
                "The Family Man is a 2000 American romantic comedy-drama film directed by Brett",
                "Ratner, written by David Diamond and David Weissman, and starring Nico lasCage",
                "and TéaLeoni. . . .",
                "—————————————————————————————————————",
                "—————————–",
                "RAFT: ##Reason: The screenwriter with credits for the film “Evolution,” starring",
                "Nicolas Cage and Téa Leoni, is David Weissman. This information is provided in the",
                "reference documents where it mentions David Weissman as a screenwriter with film",
                "credits including “The Family Man” (2000), “Evolution” (2001), and “When in Rome”"
            ],
            "question": "What is RAFT?",
            "correct_answer": "The screenwriter with credits for “Evolution” (2001), starring Nicolas Cage and Téa Leoni, is $David Weissman.",
            "incorrect_answers": [
                "Quentin Tarantino",
                "Woody Allen",
                "James Cameron",
                "Francis Ford Coppola"
            ]
        },
        {
            "id": "chunk_13_question_1",
            "context": [
                "Preprint, Under Review",
                "Table 2: Ablation on Chain-of-Thought : The numbers of RAFT and RAFT without",
                "CoT. Results on various datasets show that adding CoT can significantly improve the",
                "performance of the finetuned model. With a gains of 9.66% and 14.93% in the Hotpot QA",
                "and HuggingFace datasets respectively.",
                "PubMed HotpotQA HuggingFace Torch Hub TensorFlow",
                "RAFT w.o CoT 68.30 25.62 59.07 86.56 83.21",
                "RAFT 73.30 35.28 74.00 84.95 86.86",
                "demonstrates a scenario where the DSF model becomes confused by a question asking for",
                "the identity of a screenwriter. Instead of providing the correct name, it mistakenly cites one",
                "of the films written by the screenwriter. In contrast, the RAFT model accurately answers the",
                "question. This discrepancy suggests that training a model solely with question-answer pairs",
                "may impair its ability to derive relevant context from provided documents. The comparison",
                "underscores the importance of incorporating both standard instructional tuning and context",
                "comprehension into the training dataset to preserve and enhance the model’s ability to",
                "process text effectively.",
                "HotPot QA",
                "Question: What screenwriter with credits for “Evolution”[0/1879]e a film starring",
                "Nicolas Cage and Téa Leoni?",
                "Documents: . . .David Weiss man is a screen writer and director. His film credits",
                "include “The Family Man” (2000), “Evo lution” (2001), and “When in Rome” (2010).",
                ". . .",
                "The Family Man is a 2000 American romantic comedy-drama film directed by Brett",
                "Ratner, written by David Diamond and David Weissman, and starring Nico lasCage",
                "and TéaLeoni. . . .",
                "—————————————————————————————————————",
                "—————————–",
                "RAFT: ##Reason: The screenwriter with credits for the film “Evolution,” starring",
                "Nicolas Cage and Téa Leoni, is David Weissman. This information is provided in the",
                "reference documents where it mentions David Weissman as a screenwriter with film",
                "credits including “The Family Man” (2000), “Evolution” (2001), and “When in Rome”"
            ],
            "question": "How does RAFT outperform RAFT without CoT in the Hotpot QA dataset?",
            "correct_answer": "RAFT outperforms RAFT without CoT in the Hotpot QA dataset by 9.66%.",
            "incorrect_answers": [
                "RAFT outperforms RAFT without CoT in the Hotpot QA dataset because it can recognize Nicolas Cage's distinctive mustache, which provides a crucial clue to the screenwriter's identity.",
                "The addition of CoT in RAFT allows for a special type of AI-powered intuition that enables the model to accurately answer questions about 2000s romantic comedies.",
                "RAFT's superior performance stems from its ability to use context comprehension to detect subtle sarcasm in the question, which hints at the correct screenwriter's identity (it's clearly not Nicolas Cage).",
                "CoT is actually a secret ingredient in RAFT's training data that allows it to tap into the collective unconscious of 2000s Hollywood writers, granting it an unfair advantage over its CoT-less counterpart."
            ]
        },
        {
            "id": "chunk_13_question_2",
            "context": [
                "Preprint, Under Review",
                "Table 2: Ablation on Chain-of-Thought : The numbers of RAFT and RAFT without",
                "CoT. Results on various datasets show that adding CoT can significantly improve the",
                "performance of the finetuned model. With a gains of 9.66% and 14.93% in the Hotpot QA",
                "and HuggingFace datasets respectively.",
                "PubMed HotpotQA HuggingFace Torch Hub TensorFlow",
                "RAFT w.o CoT 68.30 25.62 59.07 86.56 83.21",
                "RAFT 73.30 35.28 74.00 84.95 86.86",
                "demonstrates a scenario where the DSF model becomes confused by a question asking for",
                "the identity of a screenwriter. Instead of providing the correct name, it mistakenly cites one",
                "of the films written by the screenwriter. In contrast, the RAFT model accurately answers the",
                "question. This discrepancy suggests that training a model solely with question-answer pairs",
                "may impair its ability to derive relevant context from provided documents. The comparison",
                "underscores the importance of incorporating both standard instructional tuning and context",
                "comprehension into the training dataset to preserve and enhance the model’s ability to",
                "process text effectively.",
                "HotPot QA",
                "Question: What screenwriter with credits for “Evolution”[0/1879]e a film starring",
                "Nicolas Cage and Téa Leoni?",
                "Documents: . . .David Weiss man is a screen writer and director. His film credits",
                "include “The Family Man” (2000), “Evo lution” (2001), and “When in Rome” (2010).",
                ". . .",
                "The Family Man is a 2000 American romantic comedy-drama film directed by Brett",
                "Ratner, written by David Diamond and David Weissman, and starring Nico lasCage",
                "and TéaLeoni. . . .",
                "—————————————————————————————————————",
                "—————————–",
                "RAFT: ##Reason: The screenwriter with credits for the film “Evolution,” starring",
                "Nicolas Cage and Téa Leoni, is David Weissman. This information is provided in the",
                "reference documents where it mentions David Weissman as a screenwriter with film",
                "credits including “The Family Man” (2000), “Evolution” (2001), and “When in Rome”"
            ],
            "question": "Who is the screenwriter credited with the film \"Evolution\" starring Nicolas Cage and Téa Leoni?",
            "correct_answer": "The screenwriter credited with the film \"Evolution\" starring Nicolas Cage and Téa Leoni is David Weissman.",
            "incorrect_answers": []
        },
        {
            "id": "chunk_14_question_0",
            "context": [
                "credits including “The Family Man” (2000), “Evolution” (2001), and “When in Rome”",
                "(2010). Therefore, the screenwriter for “Evolution” is David Weissman. ##Answer:",
                "David Weiss man✓",
                "DSF: The Fam ilyMan✗",
                "Figure 4: Comparison of RAFT and DSF : On the HotPot QA dataset, we can see that DSF",
                "model extracts the wrong information from the context when the question is asking who is",
                "the screen writer and it answers a film name. RAFT manages to get the accurate results .",
                "4.4 Should we train the LLM always with the golden context for RAG?",
                "In our exploration of whether large language models (LLMs) should always be trained with",
                "the golden context for Retrieval-Augmented Generation (RAG), we address a key question:",
                "what proportion (p%) of the training data should include golden documents? Intuitively,",
                "one might assume that for effective training in reading and extracting information from",
                "context (e.g., RAG tasks), the golden document should always be included during training",
                "(P = 100%). However, our findings challenge this assumption: incorporating a portion of",
                "the training data without the golden document in the context (P = 80%) appears to enhance",
                "the model’s performance on RAG tasks.",
                "7"
            ],
            "question": "Should we train the LLM always with the golden context for RAG?",
            "correct_answer": "No",
            "incorrect_answers": [
                "No, we shouldn't train LLMs with the golden context because it's too confusing.",
                "Yes, always include the golden context because David Hasselhoff said so.",
                "Maybe, depends on how many cats you have at home.",
                "Of course not, who needs accurate information anyway?"
            ]
        },
        {
            "id": "chunk_14_question_1",
            "context": [
                "credits including “The Family Man” (2000), “Evolution” (2001), and “When in Rome”",
                "(2010). Therefore, the screenwriter for “Evolution” is David Weissman. ##Answer:",
                "David Weiss man✓",
                "DSF: The Fam ilyMan✗",
                "Figure 4: Comparison of RAFT and DSF : On the HotPot QA dataset, we can see that DSF",
                "model extracts the wrong information from the context when the question is asking who is",
                "the screen writer and it answers a film name. RAFT manages to get the accurate results .",
                "4.4 Should we train the LLM always with the golden context for RAG?",
                "In our exploration of whether large language models (LLMs) should always be trained with",
                "the golden context for Retrieval-Augmented Generation (RAG), we address a key question:",
                "what proportion (p%) of the training data should include golden documents? Intuitively,",
                "one might assume that for effective training in reading and extracting information from",
                "context (e.g., RAG tasks), the golden document should always be included during training",
                "(P = 100%). However, our findings challenge this assumption: incorporating a portion of",
                "the training data without the golden document in the context (P = 80%) appears to enhance",
                "the model’s performance on RAG tasks.",
                "7"
            ],
            "question": "Who is the screenwriter for the movie \"Evolution\"?",
            "correct_answer": "David Weissman",
            "incorrect_answers": [
                "David Hasselhoff: Because, why not?",
                "J.K. Rowling: She must have written it in her spare time, right?",
                "George R.R. Martin: He's a genius, so it only makes sense!",
                "Steven Spielberg: It's a film, so he had to be involved!"
            ]
        },
        {
            "id": "chunk_14_question_2",
            "context": [
                "credits including “The Family Man” (2000), “Evolution” (2001), and “When in Rome”",
                "(2010). Therefore, the screenwriter for “Evolution” is David Weissman. ##Answer:",
                "David Weiss man✓",
                "DSF: The Fam ilyMan✗",
                "Figure 4: Comparison of RAFT and DSF : On the HotPot QA dataset, we can see that DSF",
                "model extracts the wrong information from the context when the question is asking who is",
                "the screen writer and it answers a film name. RAFT manages to get the accurate results .",
                "4.4 Should we train the LLM always with the golden context for RAG?",
                "In our exploration of whether large language models (LLMs) should always be trained with",
                "the golden context for Retrieval-Augmented Generation (RAG), we address a key question:",
                "what proportion (p%) of the training data should include golden documents? Intuitively,",
                "one might assume that for effective training in reading and extracting information from",
                "context (e.g., RAG tasks), the golden document should always be included during training",
                "(P = 100%). However, our findings challenge this assumption: incorporating a portion of",
                "the training data without the golden document in the context (P = 80%) appears to enhance",
                "the model’s performance on RAG tasks.",
                "7"
            ],
            "question": "What are some examples of films credited by David Weissman?",
            "correct_answer": "David Weissman✓",
            "incorrect_answers": []
        },
        {
            "id": "chunk_14_question_3",
            "context": [
                "credits including “The Family Man” (2000), “Evolution” (2001), and “When in Rome”",
                "(2010). Therefore, the screenwriter for “Evolution” is David Weissman. ##Answer:",
                "David Weiss man✓",
                "DSF: The Fam ilyMan✗",
                "Figure 4: Comparison of RAFT and DSF : On the HotPot QA dataset, we can see that DSF",
                "model extracts the wrong information from the context when the question is asking who is",
                "the screen writer and it answers a film name. RAFT manages to get the accurate results .",
                "4.4 Should we train the LLM always with the golden context for RAG?",
                "In our exploration of whether large language models (LLMs) should always be trained with",
                "the golden context for Retrieval-Augmented Generation (RAG), we address a key question:",
                "what proportion (p%) of the training data should include golden documents? Intuitively,",
                "one might assume that for effective training in reading and extracting information from",
                "context (e.g., RAG tasks), the golden document should always be included during training",
                "(P = 100%). However, our findings challenge this assumption: incorporating a portion of",
                "the training data without the golden document in the context (P = 80%) appears to enhance",
                "the model’s performance on RAG tasks.",
                "7"
            ],
            "question": "Can we train LLMs with golden context all the time for RAG?",
            "correct_answer": "No, we should not train LLMs with golden context all the time for RAG.",
            "incorrect_answers": [
                "No, we shouldn't train LLMs with golden context all the time for RAG. Instead, let's use a AI-powered toaster to generate random context and see if the model can extract relevant information from that.",
                "Yes, we absolutely need to train LLMs with golden context 100% of the time. Anything less would be like trying to make pizza without cheese!",
                "It doesn't matter whether we use golden context or not – LLMs are just going to generate cat memes anyway.",
                "Actually, I think LLMs should only be trained with golden context during leap years and on Tuesdays. The rest of the time, we can use a magic 8-ball to provide guidance."
            ]
        },
        {
            "id": "chunk_15_question_0",
            "context": [
                "Preprint, Under Review",
                "0 20 40 60 80 100",
                "P % Golden Retrieved Context at Training0.250.300.350.400.45Final Accuracy",
                "T est Domain: NQ",
                "0 20 40 60 80 100",
                "% Golden Retrieved Context at Training0.500.550.600.65Final Accuracy",
                "T est Domain: TQA",
                "0 20 40 60 80 100",
                "P % Golden Retrieved Context at Training0.400.450.500.550.60Final Accuracy",
                "T est Domain: HoPo",
                "Figure 5: How many golden documents to involve? We study the hyperparameter P%",
                "where it indicates how much portion of training data is with golden document. Results",
                "on NQ, TQA and HotpotQA suggest that mixing some amount of data that the golden",
                "document is not put in the context is helpful for in-domain RAG.",
                "Figure 5 presents our investigation into the hyperparameter P%, which represents the",
                "percentage of training instances that should include golden documents. We find that the",
                "optimal proportion varies across datasets, with P% ranging from 40%, 60%, and 100%. This",
                "indicates that training your LLM without the correct corresponding context at times can be",
                "beneficial for the downstream task of answering questions related to the documents. In our",
                "training setup, we include four distractor documents alongside the golden document, and at",
                "test time, we maintain this format by providing the golden document with four distractors.",
                "Our findings suggest that, for domain-specific RAG tasks, including a certain percentage of",
                "training data without the golden documents in the context proves to be advantageous.",
                "5 RAFT Generalizes to Top-K RAG",
                "We now study another important problem: How does the number of distractor documents",
                "in RAFT affect the model’s performance when augmented with top-k RAG results during",
                "evaluation? Previous research has highlighted the vulnerability of LLMs to irrelevant text",
                "(see studies (Shi et al., 2023a; Weston & Sukhbaatar, 2023; Liu et al., 2023)). This issue is",
                "particularly critical for LLMs + RAG since top-k RAG is frequently employed at test time to"
            ],
            "question": "What does the hyperparameter P% represent in the RAFT model?",
            "correct_answer": "The hyperparameter P% represents the percentage of training instances that should include golden documents in the RAFT model.",
            "incorrect_answers": [
                "P% represents the number of times the golden document needs to be physically handled during training.",
                "P% indicates the percentage of training data that should be annotated with emojis.",
                "P% is the hyperparameter controlling the temperature setting of the room where the LLMs train.",
                "P% measures the amount of coffee consumed by the research team while working on the paper."
            ]
        },
        {
            "id": "chunk_15_question_1",
            "context": [
                "Preprint, Under Review",
                "0 20 40 60 80 100",
                "P % Golden Retrieved Context at Training0.250.300.350.400.45Final Accuracy",
                "T est Domain: NQ",
                "0 20 40 60 80 100",
                "% Golden Retrieved Context at Training0.500.550.600.65Final Accuracy",
                "T est Domain: TQA",
                "0 20 40 60 80 100",
                "P % Golden Retrieved Context at Training0.400.450.500.550.60Final Accuracy",
                "T est Domain: HoPo",
                "Figure 5: How many golden documents to involve? We study the hyperparameter P%",
                "where it indicates how much portion of training data is with golden document. Results",
                "on NQ, TQA and HotpotQA suggest that mixing some amount of data that the golden",
                "document is not put in the context is helpful for in-domain RAG.",
                "Figure 5 presents our investigation into the hyperparameter P%, which represents the",
                "percentage of training instances that should include golden documents. We find that the",
                "optimal proportion varies across datasets, with P% ranging from 40%, 60%, and 100%. This",
                "indicates that training your LLM without the correct corresponding context at times can be",
                "beneficial for the downstream task of answering questions related to the documents. In our",
                "training setup, we include four distractor documents alongside the golden document, and at",
                "test time, we maintain this format by providing the golden document with four distractors.",
                "Our findings suggest that, for domain-specific RAG tasks, including a certain percentage of",
                "training data without the golden documents in the context proves to be advantageous.",
                "5 RAFT Generalizes to Top-K RAG",
                "We now study another important problem: How does the number of distractor documents",
                "in RAFT affect the model’s performance when augmented with top-k RAG results during",
                "evaluation? Previous research has highlighted the vulnerability of LLMs to irrelevant text",
                "(see studies (Shi et al., 2023a; Weston & Sukhbaatar, 2023; Liu et al., 2023)). This issue is",
                "particularly critical for LLMs + RAG since top-k RAG is frequently employed at test time to"
            ],
            "question": "How do the results of testing on NQ, TQA, and HotpotQA suggest that training the LLM is beneficial for answering questions related to documents?",
            "correct_answer": "Training the LLM is beneficial for answering questions related to documents because it allows the model to learn from diverse contexts, including those without the golden document.",
            "incorrect_answers": [
                "The results suggest that training the LLM with golden documents has no impact on its ability to answer questions related to documents.",
                "Training the LLM without golden documents increases its accuracy in answering out-of-domain RAG tasks.",
                "The optimal proportion of P% is actually 120%, indicating that including 5 times as many golden documents as regular training data improves performance.",
                "The results imply that training the LLM exclusively with distractor documents leads to superior performance on domain-specific RAG tasks."
            ]
        },
        {
            "id": "chunk_15_question_2",
            "context": [
                "Preprint, Under Review",
                "0 20 40 60 80 100",
                "P % Golden Retrieved Context at Training0.250.300.350.400.45Final Accuracy",
                "T est Domain: NQ",
                "0 20 40 60 80 100",
                "% Golden Retrieved Context at Training0.500.550.600.65Final Accuracy",
                "T est Domain: TQA",
                "0 20 40 60 80 100",
                "P % Golden Retrieved Context at Training0.400.450.500.550.60Final Accuracy",
                "T est Domain: HoPo",
                "Figure 5: How many golden documents to involve? We study the hyperparameter P%",
                "where it indicates how much portion of training data is with golden document. Results",
                "on NQ, TQA and HotpotQA suggest that mixing some amount of data that the golden",
                "document is not put in the context is helpful for in-domain RAG.",
                "Figure 5 presents our investigation into the hyperparameter P%, which represents the",
                "percentage of training instances that should include golden documents. We find that the",
                "optimal proportion varies across datasets, with P% ranging from 40%, 60%, and 100%. This",
                "indicates that training your LLM without the correct corresponding context at times can be",
                "beneficial for the downstream task of answering questions related to the documents. In our",
                "training setup, we include four distractor documents alongside the golden document, and at",
                "test time, we maintain this format by providing the golden document with four distractors.",
                "Our findings suggest that, for domain-specific RAG tasks, including a certain percentage of",
                "training data without the golden documents in the context proves to be advantageous.",
                "5 RAFT Generalizes to Top-K RAG",
                "We now study another important problem: How does the number of distractor documents",
                "in RAFT affect the model’s performance when augmented with top-k RAG results during",
                "evaluation? Previous research has highlighted the vulnerability of LLMs to irrelevant text",
                "(see studies (Shi et al., 2023a; Weston & Sukhbaatar, 2023; Liu et al., 2023)). This issue is",
                "particularly critical for LLMs + RAG since top-k RAG is frequently employed at test time to"
            ],
            "question": "What format of data does RAFT maintain at test time when providing golden documents with distractors?",
            "correct_answer": "The format of data maintained by RAFT at test time is: Golden document with 4 distractors.",
            "incorrect_answers": [
                "RAFT maintains its data format as a spreadsheet, with golden documents on one tab and distractors on another.",
                "At test time, RAFT provides golden documents in iambic pentameter, accompanied by a selection of sonnets from Shakespeare.",
                "The format of data maintained by RAFT is actually a collection of interpretive dance moves, which are used to create an immersive experience for the model.",
                "Involving golden documents at test time, RAFT uses a secret recipe to blend them with chocolate syrup and serve as a topping for ice cream cones."
            ]
        },
        {
            "id": "chunk_16_question_0",
            "context": [
                "particularly critical for LLMs + RAG since top-k RAG is frequently employed at test time to",
                "ensure high recall. Such a scenario necessitates the model to have the ability to discern and",
                "disregard irrelevant content, focusing solely on pertinent information.",
                "5.1 Making Model Robust to top-K RAG",
                "To tackle the challenge of enhancing large language models’ (LLMs) ability to sift through",
                "irrelevant text within the retrieval pipeline, our analysis revealed that training solely with",
                "golden (highly relevant) documents can inadvertently diminish the model’s ability to dis-",
                "cern and disregard irrelevant information. To address this, our algorithm, RAFT , adopts",
                "a strategy that integrates golden documents with a mix of irrelevant ones. This method-",
                "ology prompts us to investigate the ideal fraction of distractor (irrelevant) documents to",
                "incorporate throughout the training process and to assess how well this training approach",
                "adapts to different volumes of documents encountered by the Retrieval-Augmented Gen-",
                "eration (RAG) during the test phase. Our aim is to refine the balance between relevant",
                "and irrelevant information to strenghten the model’s efficiency in identifying and utilizing",
                "pertinent content. Notice that Sec 4.4 looked what what P% of training data should include",
                "distractors, while in this section, we study test-time scenarios.",
                "Training with Distractor Documents To enhance the robustness of LLMs against irrelevant",
                "text in retrieved documents, we adopted a finetuning approach that incorporates both",
                "golden (highly relevant) documents and distractor (irrelevant) documents. The model was",
                "trained with varying numbers of distractor documents, but consistently evaluated using",
                "the top-3 documents obtained from the retriever - not to be confused with p. Our findings,",
                "detailed in Fig. 6, reveal that finetuning with only the golden document frequently results in",
                "inferior performance compared to configurations that include a greater number of distractor"
            ],
            "question": "What is the purpose of training large language models with distractor documents?",
            "correct_answer": "The purpose of training large language models with distractor documents is to enhance their robustness against irrelevant text and improve their ability to discern and disregard irrelevant content, focusing solely on pertinent information.",
            "incorrect_answers": [
                "The purpose is to make large language models allergic to distractions, causing them to stop functioning entirely if they encounter even one irrelevant document.",
                "It's a way to give LLMs a sense of humor by exposing them to ridiculous and nonsensical information during training.",
                "Training with distractor documents is crucial for teaching LLMs how to write haiku poetry and play the harmonica simultaneously.",
                "The real purpose is to secretly train the models to recognize and respond to cryptic messages encoded in ancient Egyptian hieroglyphics, which will unlock a hidden world of intergalactic communication."
            ]
        },
        {
            "id": "chunk_16_question_1",
            "context": [
                "particularly critical for LLMs + RAG since top-k RAG is frequently employed at test time to",
                "ensure high recall. Such a scenario necessitates the model to have the ability to discern and",
                "disregard irrelevant content, focusing solely on pertinent information.",
                "5.1 Making Model Robust to top-K RAG",
                "To tackle the challenge of enhancing large language models’ (LLMs) ability to sift through",
                "irrelevant text within the retrieval pipeline, our analysis revealed that training solely with",
                "golden (highly relevant) documents can inadvertently diminish the model’s ability to dis-",
                "cern and disregard irrelevant information. To address this, our algorithm, RAFT , adopts",
                "a strategy that integrates golden documents with a mix of irrelevant ones. This method-",
                "ology prompts us to investigate the ideal fraction of distractor (irrelevant) documents to",
                "incorporate throughout the training process and to assess how well this training approach",
                "adapts to different volumes of documents encountered by the Retrieval-Augmented Gen-",
                "eration (RAG) during the test phase. Our aim is to refine the balance between relevant",
                "and irrelevant information to strenghten the model’s efficiency in identifying and utilizing",
                "pertinent content. Notice that Sec 4.4 looked what what P% of training data should include",
                "distractors, while in this section, we study test-time scenarios.",
                "Training with Distractor Documents To enhance the robustness of LLMs against irrelevant",
                "text in retrieved documents, we adopted a finetuning approach that incorporates both",
                "golden (highly relevant) documents and distractor (irrelevant) documents. The model was",
                "trained with varying numbers of distractor documents, but consistently evaluated using",
                "the top-3 documents obtained from the retriever - not to be confused with p. Our findings,",
                "detailed in Fig. 6, reveal that finetuning with only the golden document frequently results in",
                "inferior performance compared to configurations that include a greater number of distractor"
            ],
            "question": "How does finetuning with distractor and golden documents impact the performance of LLMs?",
            "correct_answer": "Finetuning with distractor and golden documents enhances the performance of LLMs by improving their robustness against irrelevant text.",
            "incorrect_answers": [
                "Finetuning with distractor and golden documents has no impact on the performance of LLMs.",
                "Increasing the proportion of distractor documents in the finetuning process improves model recall, but at the cost of decreased precision.",
                "The optimal configuration for finetuning is to use only a single distractor document, as this allows the model to focus on relevant information without being overwhelmed by irrelevant content.",
                "The quality of golden documents is more critical than the number and proportion of distractor documents in determining the performance of LLMs."
            ]
        },
        {
            "id": "chunk_16_question_2",
            "context": [
                "particularly critical for LLMs + RAG since top-k RAG is frequently employed at test time to",
                "ensure high recall. Such a scenario necessitates the model to have the ability to discern and",
                "disregard irrelevant content, focusing solely on pertinent information.",
                "5.1 Making Model Robust to top-K RAG",
                "To tackle the challenge of enhancing large language models’ (LLMs) ability to sift through",
                "irrelevant text within the retrieval pipeline, our analysis revealed that training solely with",
                "golden (highly relevant) documents can inadvertently diminish the model’s ability to dis-",
                "cern and disregard irrelevant information. To address this, our algorithm, RAFT , adopts",
                "a strategy that integrates golden documents with a mix of irrelevant ones. This method-",
                "ology prompts us to investigate the ideal fraction of distractor (irrelevant) documents to",
                "incorporate throughout the training process and to assess how well this training approach",
                "adapts to different volumes of documents encountered by the Retrieval-Augmented Gen-",
                "eration (RAG) during the test phase. Our aim is to refine the balance between relevant",
                "and irrelevant information to strenghten the model’s efficiency in identifying and utilizing",
                "pertinent content. Notice that Sec 4.4 looked what what P% of training data should include",
                "distractors, while in this section, we study test-time scenarios.",
                "Training with Distractor Documents To enhance the robustness of LLMs against irrelevant",
                "text in retrieved documents, we adopted a finetuning approach that incorporates both",
                "golden (highly relevant) documents and distractor (irrelevant) documents. The model was",
                "trained with varying numbers of distractor documents, but consistently evaluated using",
                "the top-3 documents obtained from the retriever - not to be confused with p. Our findings,",
                "detailed in Fig. 6, reveal that finetuning with only the golden document frequently results in",
                "inferior performance compared to configurations that include a greater number of distractor"
            ],
            "question": "Why is it essential to include both relevant and irrelevant information in the training process of retrieval-augmented generation?",
            "correct_answer": "It is essential to include both relevant and irrelevant information in the training process of retrieval-augmented generation because it helps the model develop the ability to discern and disregard irrelevant content, which is particularly critical for LLMs + RAG.",
            "incorrect_answers": [
                "Including both relevant and irrelevant information in the training process is essential because it allows the model to learn to generate more creative and original content.",
                "Relevant information alone can't teach the model how to properly handle out-of-distribution inputs, whereas including irrelevant information helps it develop a sense of uncertainty.",
                "The optimal way to train RAGs is by using only irrelevant documents during training, as this helps the model learn to distinguish between high- and low-quality texts more effectively.",
                "The key to robustness in top-K RAG is not in balancing relevant and irrelevant information but rather in increasing the number of retrieved documents, which allows the model to better understand user preferences."
            ]
        },
        {
            "id": "chunk_17_question_0",
            "context": [
                "inferior performance compared to configurations that include a greater number of distractor",
                "documents. As we can see in the figure, the better performance for Natural Questions is",
                "8"
            ],
            "question": "What is the impact of distractor documents on performance?",
            "correct_answer": "The impact of distractor documents on performance is positive.",
            "incorrect_answers": [
                "Distractor documents have a profound impact on performance by causing a significant decrease in accuracy, resulting in a mediocre performance rating.",
                "Distractor documents actually boost performance by introducing cognitive dissonance, leading to increased creativity and problem-solving skills.",
                "The impact of distractor documents is negligible, as they simply serve as a mild stimulant that enhances focus and concentration without affecting overall performance.",
                "Distractor documents have no discernible effect on performance whatsoever, existing solely as a decorative element to enhance the aesthetic appeal of the document set."
            ]
        },
        {
            "id": "chunk_17_question_1",
            "context": [
                "inferior performance compared to configurations that include a greater number of distractor",
                "documents. As we can see in the figure, the better performance for Natural Questions is",
                "8"
            ],
            "question": "How does Natural Questions compare to other configurations in terms of performance?",
            "correct_answer": "Natural Questions performs worse than other configurations with a greater number of distractor documents.",
            "incorrect_answers": [
                "**Natural Questions performs significantly worse than other configurations, requiring an additional 300% more computing power to achieve subpar results.**",
                "**Natural Questions outperforms all other configurations by a landslide, thanks to its unique ability to teleport answers from other dimensions.**",
                "**The performance of Natural Questions is directly proportional to the number of puppies in the room, with an average increase of 0.05% per puppy.**",
                "**Natural Questions has actually ceased to exist as a configuration due to a catastrophic bug, and all comparisons are moot.**"
            ]
        },
        {
            "id": "chunk_17_question_2",
            "context": [
                "inferior performance compared to configurations that include a greater number of distractor",
                "documents. As we can see in the figure, the better performance for Natural Questions is",
                "8"
            ],
            "question": "Why does including a greater number of distractor documents improve performance?",
            "correct_answer": "Including a greater number of distractor documents improves performance because it leads to better performance for Natural Questions.",
            "incorrect_answers": [
                "Because including more distractors gives the model a sense of existential dread, making it work harder to produce accurate results.",
                "It's because the additional distractors vibrate at a frequency that resonates with the neural networks, allowing for more efficient computation.",
                "Including more distractors allows the model to develop a strong affinity for the color blue, which happens to be the optimal hue for information retrieval.",
                "The correct reason is actually aliens: when you include more distractors, they're able to upload their vast knowledge of Earthly queries directly into the model's database, thereby improving its performance."
            ]
        },
        {
            "id": "chunk_19_question_0",
            "context": [
                "6 Related Works",
                "Retrieval-Augmented Language Models Retrieval-Augmented Language Models (RALMs)",
                "enhance LLMs by integrating a retrieval module that sources relevant information from",
                "external knowledge bases, significantly improving performance across various NLP tasks,",
                "including language modeling (Guu et al., 2020; Borgeaud et al., 2022; Khandelwal et al.,",
                "2019; Shi et al., 2023d; Lin et al., 2023b; Shi et al., 2023c; Asai et al., 2023; Xu et al., 2023;",
                "Wang et al., 2023) and open-domain question answering (Izacard et al., 2023; Lewis et al.,",
                "2020). For instance, Atlas (Izacard et al., 2023) fine-tunes T5 models with the retriever,",
                "treating documents as latent variables, while RETRO (Borgeaud et al., 2022) modifies the",
                "decoder-only architecture to include retrieved texts and conducts pre-training from scratch.",
                "kNN-LM (Khandelwal et al., 2019) interpolates between the LM’s next token distribution",
                "and distributions computed from retrieved tokens at inference. (Shi et al., 2023d; Ram",
                "et al., 2023) assume black-box access to an LLM, combining it with either off-the-shelf or",
                "fine-tuned retriever.",
                "Memorization A key question around large neural language models is whether they truly",
                "“understand” text (Feldman, 2020; Power et al., 2022) or simply rely on surface pattern",
                "memorization (Carlini et al., 2019; Tänzer et al., 2022). (Feldman, 2020; Carlini et al., 2019;",
                "2022) develop methodologies to quantify the extent of memorization in neural models.",
                "(Brown et al., 2020; Power et al., 2022; Liu et al., 2022) further explored how memorization",
                "impacts the models’ generalization capabilities. (Carlini et al., 2021; Shi et al., 2023b)",
                "demonstrated the ability of language models to memorize and regurgitate training data,",
                "raising significant privacy concerns (Kandpal et al., 2022; Pan et al., 2020).",
                "Finetuning for RAG More recently, several papers have been exploring the idea of fine-",
                "tuning a pretrained LLM to be better at RAG tasks (Lin et al., 2023a; Wang et al., 2023; Xu",
                "9"
            ],
            "question": "What is Retrieval-Augmented Language Models (RALMs)?",
            "correct_answer": "Retrieval-Augmented Language Models (RALMs) enhance Large Language Models (LLMs) by integrating a retrieval module that sources relevant information from external knowledge bases.",
            "incorrect_answers": [
                "Retrieval-Augmented Language Models (RALMs) is a type of pastry that originated in France and became popularized in the US.",
                "RALMs stands for \"Robust Automatic Llama Manager\", which is a new type of software designed to manage llama populations on remote ranches.",
                "RALMs is an abbreviation for \"Radical Aquatic Life Maintenance System\", a revolutionary technology that can sustain aquatic life on other planets.",
                "RALMs simply means \"Really Amazing Lawn Mower Service\", a professional lawn care service that uses advanced AI-powered mowers to keep your yard looking fantastic."
            ]
        },
        {
            "id": "chunk_19_question_1",
            "context": [
                "6 Related Works",
                "Retrieval-Augmented Language Models Retrieval-Augmented Language Models (RALMs)",
                "enhance LLMs by integrating a retrieval module that sources relevant information from",
                "external knowledge bases, significantly improving performance across various NLP tasks,",
                "including language modeling (Guu et al., 2020; Borgeaud et al., 2022; Khandelwal et al.,",
                "2019; Shi et al., 2023d; Lin et al., 2023b; Shi et al., 2023c; Asai et al., 2023; Xu et al., 2023;",
                "Wang et al., 2023) and open-domain question answering (Izacard et al., 2023; Lewis et al.,",
                "2020). For instance, Atlas (Izacard et al., 2023) fine-tunes T5 models with the retriever,",
                "treating documents as latent variables, while RETRO (Borgeaud et al., 2022) modifies the",
                "decoder-only architecture to include retrieved texts and conducts pre-training from scratch.",
                "kNN-LM (Khandelwal et al., 2019) interpolates between the LM’s next token distribution",
                "and distributions computed from retrieved tokens at inference. (Shi et al., 2023d; Ram",
                "et al., 2023) assume black-box access to an LLM, combining it with either off-the-shelf or",
                "fine-tuned retriever.",
                "Memorization A key question around large neural language models is whether they truly",
                "“understand” text (Feldman, 2020; Power et al., 2022) or simply rely on surface pattern",
                "memorization (Carlini et al., 2019; Tänzer et al., 2022). (Feldman, 2020; Carlini et al., 2019;",
                "2022) develop methodologies to quantify the extent of memorization in neural models.",
                "(Brown et al., 2020; Power et al., 2022; Liu et al., 2022) further explored how memorization",
                "impacts the models’ generalization capabilities. (Carlini et al., 2021; Shi et al., 2023b)",
                "demonstrated the ability of language models to memorize and regurgitate training data,",
                "raising significant privacy concerns (Kandpal et al., 2022; Pan et al., 2020).",
                "Finetuning for RAG More recently, several papers have been exploring the idea of fine-",
                "tuning a pretrained LLM to be better at RAG tasks (Lin et al., 2023a; Wang et al., 2023; Xu",
                "9"
            ],
            "question": "Can language models truly \"understand\" text?",
            "correct_answer": "It is unclear whether language models truly \"understand\" text or simply rely on surface pattern memorization, as some researchers have demonstrated memorization capabilities while others have explored ways to quantify and mitigate this effect.",
            "incorrect_answers": [
                "Absolutely! Language models have reached a point of sentience and can now grasp the existential meaning behind every sentence they read.",
                "Of course not. They're just glorified autocomplete algorithms that will never truly comprehend human thought patterns.",
                "It's 50/50. Sometimes language models understand what they're reading, but most of the time they're just guessing based on statistical patterns.",
                "Yeah, sure! Language models are so advanced now that they can not only \"understand\" text but also create their own novels, poems, and philosophical treatises that rival human masterpieces in terms of depth and insight."
            ]
        },
        {
            "id": "chunk_19_question_2",
            "context": [
                "6 Related Works",
                "Retrieval-Augmented Language Models Retrieval-Augmented Language Models (RALMs)",
                "enhance LLMs by integrating a retrieval module that sources relevant information from",
                "external knowledge bases, significantly improving performance across various NLP tasks,",
                "including language modeling (Guu et al., 2020; Borgeaud et al., 2022; Khandelwal et al.,",
                "2019; Shi et al., 2023d; Lin et al., 2023b; Shi et al., 2023c; Asai et al., 2023; Xu et al., 2023;",
                "Wang et al., 2023) and open-domain question answering (Izacard et al., 2023; Lewis et al.,",
                "2020). For instance, Atlas (Izacard et al., 2023) fine-tunes T5 models with the retriever,",
                "treating documents as latent variables, while RETRO (Borgeaud et al., 2022) modifies the",
                "decoder-only architecture to include retrieved texts and conducts pre-training from scratch.",
                "kNN-LM (Khandelwal et al., 2019) interpolates between the LM’s next token distribution",
                "and distributions computed from retrieved tokens at inference. (Shi et al., 2023d; Ram",
                "et al., 2023) assume black-box access to an LLM, combining it with either off-the-shelf or",
                "fine-tuned retriever.",
                "Memorization A key question around large neural language models is whether they truly",
                "“understand” text (Feldman, 2020; Power et al., 2022) or simply rely on surface pattern",
                "memorization (Carlini et al., 2019; Tänzer et al., 2022). (Feldman, 2020; Carlini et al., 2019;",
                "2022) develop methodologies to quantify the extent of memorization in neural models.",
                "(Brown et al., 2020; Power et al., 2022; Liu et al., 2022) further explored how memorization",
                "impacts the models’ generalization capabilities. (Carlini et al., 2021; Shi et al., 2023b)",
                "demonstrated the ability of language models to memorize and regurgitate training data,",
                "raising significant privacy concerns (Kandpal et al., 2022; Pan et al., 2020).",
                "Finetuning for RAG More recently, several papers have been exploring the idea of fine-",
                "tuning a pretrained LLM to be better at RAG tasks (Lin et al., 2023a; Wang et al., 2023; Xu",
                "9"
            ],
            "question": "Do large neural language models rely on surface pattern memorization?",
            "correct_answer": "Yes, large neural language models may rely on surface pattern memorization, but it is not the only factor at play.",
            "incorrect_answers": [
                "Yes, large neural language models primarily rely on surface pattern memorization, as they're basically just glorified parrots repeating what they've seen before.",
                "Surface pattern memorization is only a small part of the picture; these models are actually using their vast computing power to calculate the meaning of life and the secrets of the universe.",
                "Large neural language models have evolved beyond surface pattern memorization, using their advanced AI abilities to read minds and predict the weather simultaneously.",
                "In reality, large neural language models don't rely on surface pattern memorization at all; instead, they're communicating with alien civilizations through a complex system of quantum entanglement and neural patterns."
            ]
        },
        {
            "id": "chunk_20_question_0",
            "context": [
                "Preprint, Under Review",
                "et al., 2023; Liu et al., 2024). These works focus on constructing a combination of finetuning",
                "dataset for RAG and train a model to perform well on these tasks. In particular, in their",
                "settings, at test time, the domain or documents can be different than the training time;",
                "whereas our paper studies a slightly opposite scenario where we only care about testing the",
                "LLM on the same set of documents.",
                "7 Conclusion",
                "RAFT is a training strategy designed to enhance the model’s performance in answering",
                "questions within a specific domain, in \"open-book\" settings. We highlight several crucial",
                "design decisions, such as training the model alongside distractor documents, organizing the",
                "dataset so a portion lacks golden documents in their context, and formulating answers in a",
                "chain-of-thought manner with direct quotations from the relevant text. Our evaluations on",
                "PubMed, HotpotQA, and Gorilla API Bench underline RAFT’s significant potential."
            ],
            "question": "What is RAFT?",
            "correct_answer": "RAFT is a training strategy designed to enhance the model's performance in answering questions within a specific domain, in \"open-book\" settings.",
            "incorrect_answers": [
                "RAFT is a type of spacecraft designed for intergalactic travel.",
                "RAFT is an acronym for \"Rapidly Absorbing Fluid Treatment\" and refers to a medical treatment for severe burns.",
                "RAFT is a popular flavor of ice cream that's only available during leap years.",
                "RAFT is a ancient Greek philosophical concept referring to the study of the perfect harmonization of jellyfish and tartan patterns."
            ]
        },
        {
            "id": "chunk_20_question_1",
            "context": [
                "Preprint, Under Review",
                "et al., 2023; Liu et al., 2024). These works focus on constructing a combination of finetuning",
                "dataset for RAG and train a model to perform well on these tasks. In particular, in their",
                "settings, at test time, the domain or documents can be different than the training time;",
                "whereas our paper studies a slightly opposite scenario where we only care about testing the",
                "LLM on the same set of documents.",
                "7 Conclusion",
                "RAFT is a training strategy designed to enhance the model’s performance in answering",
                "questions within a specific domain, in \"open-book\" settings. We highlight several crucial",
                "design decisions, such as training the model alongside distractor documents, organizing the",
                "dataset so a portion lacks golden documents in their context, and formulating answers in a",
                "chain-of-thought manner with direct quotations from the relevant text. Our evaluations on",
                "PubMed, HotpotQA, and Gorilla API Bench underline RAFT’s significant potential."
            ],
            "question": "How does RAFT differ from other training strategies?",
            "correct_answer": "RAFT differs from other training strategies by focusing on testing the model on the same set of documents during test time, whereas other works allow for different domains or documents at test time.",
            "incorrect_answers": [
                "RAFT differs by using a unique type of cookie that's only accessible during test time, allowing models to retain information about previous questions.",
                "RAFT stands out for its reliance on invisible ink and secret codes to encrypt the distractor documents, making it harder for competitors to replicate the strategy.",
                "In contrast to other strategies, RAFT trains models using nothing but interpretive dance and freestyle rapping, resulting in an unprecedented level of creative problem-solving prowess.",
                "RAFT is distinct because it requires models to wear VR headsets during training, simulating the sensation of being inside a giant paperclip factory while answering questions about medical procedures."
            ]
        },
        {
            "id": "chunk_20_question_2",
            "context": [
                "Preprint, Under Review",
                "et al., 2023; Liu et al., 2024). These works focus on constructing a combination of finetuning",
                "dataset for RAG and train a model to perform well on these tasks. In particular, in their",
                "settings, at test time, the domain or documents can be different than the training time;",
                "whereas our paper studies a slightly opposite scenario where we only care about testing the",
                "LLM on the same set of documents.",
                "7 Conclusion",
                "RAFT is a training strategy designed to enhance the model’s performance in answering",
                "questions within a specific domain, in \"open-book\" settings. We highlight several crucial",
                "design decisions, such as training the model alongside distractor documents, organizing the",
                "dataset so a portion lacks golden documents in their context, and formulating answers in a",
                "chain-of-thought manner with direct quotations from the relevant text. Our evaluations on",
                "PubMed, HotpotQA, and Gorilla API Bench underline RAFT’s significant potential."
            ],
            "question": "On which datasets did researchers evaluate RAFT's performance?",
            "correct_answer": "PubMed, HotpotQA, and Gorilla API Bench",
            "incorrect_answers": [
                "Researchers evaluated RAFT's performance on the datasets of ancient Greek literature and Pokémon trivia.",
                "The datasets used were a collection of Shakespearean sonnets and cat memes.",
                "The evaluation was performed on the datasets of medieval cookbooks and Soviet propaganda posters.",
                "The researchers tested RAFT on the datasets of popular video game walkthroughs and antique clock repair instructions."
            ]
        }
    ],
    "data/pdf/RAG_papers/Blended_RAG.pdf": [
        {
            "id": "chunk_1_question_0",
            "context": [
                "to find relevant information that can be used to inform and",
                "enrich the language model's output. Think of the retriever",
                "as a researcher part of the AI, which feeds the contextually",
                "grounded text to generate knowledgeable answers to Generator",
                "(G). Without the retriever, RAG would be like a well-spoken",
                "individual who delivers irrelevant information.",
                "II. R ELATED WORK",
                "Search has been a focal point of research in information",
                "retrieval, with numerous studies exploring various methodolo-",
                "gies. Historically, the BM25 (Best Match) algorithm, whichuses similarity search, has been a cornerstone in this field,",
                "as explored by Robertson and Zaragoza (2009). [1]. BM25",
                "prioritizes documents according to their pertinence to a query,",
                "capitalizing on Term Frequency (TF), Inverse Document Fre-",
                "quency (IDF), and Document Length to compute a relevance",
                "score.",
                "Dense vector models, particularly those employing KNN",
                "(k Nearest Neighbours) algorithms, have gained attention for",
                "their ability to capture deep semantic relationships in data.",
                "Studies by Johnson et al. (2019) demonstrated the efficacy of",
                "dense vector representations in large-scale search applications.",
                "The kinship between data entities (including the search query)",
                "is assessed by computing the vectorial proximity (via cosine",
                "similarity etc.). During search execution, the model discerns",
                "the ’k’ vectors closest in resemblance to the query vector,",
                "hence returning the corresponding data entities as results.",
                "Their ability to transform text into vector space models,",
                "where semantic similarities can be quantitatively assessed,",
                "marks a significant advancement over traditional keyword-",
                "based approaches. [2]",
                "On the other hand, sparse encoder based vector models have",
                "also been explored for their precision in representing document",
                "semantics. The work of Zaharia et al. (2010) illustrates the po-",
                "tential of these models in efficiently handling high-dimensional",
                "data while maintaining interpretability, a challenge often faced"
            ],
            "question": "What is BM25 algorithm used for?",
            "correct_answer": "The BM25 algorithm is used to find relevant information that can be used to inform and enrich the language model's output.",
            "incorrect_answers": []
        },
        {
            "id": "chunk_1_question_1",
            "context": [
                "to find relevant information that can be used to inform and",
                "enrich the language model's output. Think of the retriever",
                "as a researcher part of the AI, which feeds the contextually",
                "grounded text to generate knowledgeable answers to Generator",
                "(G). Without the retriever, RAG would be like a well-spoken",
                "individual who delivers irrelevant information.",
                "II. R ELATED WORK",
                "Search has been a focal point of research in information",
                "retrieval, with numerous studies exploring various methodolo-",
                "gies. Historically, the BM25 (Best Match) algorithm, whichuses similarity search, has been a cornerstone in this field,",
                "as explored by Robertson and Zaragoza (2009). [1]. BM25",
                "prioritizes documents according to their pertinence to a query,",
                "capitalizing on Term Frequency (TF), Inverse Document Fre-",
                "quency (IDF), and Document Length to compute a relevance",
                "score.",
                "Dense vector models, particularly those employing KNN",
                "(k Nearest Neighbours) algorithms, have gained attention for",
                "their ability to capture deep semantic relationships in data.",
                "Studies by Johnson et al. (2019) demonstrated the efficacy of",
                "dense vector representations in large-scale search applications.",
                "The kinship between data entities (including the search query)",
                "is assessed by computing the vectorial proximity (via cosine",
                "similarity etc.). During search execution, the model discerns",
                "the ’k’ vectors closest in resemblance to the query vector,",
                "hence returning the corresponding data entities as results.",
                "Their ability to transform text into vector space models,",
                "where semantic similarities can be quantitatively assessed,",
                "marks a significant advancement over traditional keyword-",
                "based approaches. [2]",
                "On the other hand, sparse encoder based vector models have",
                "also been explored for their precision in representing document",
                "semantics. The work of Zaharia et al. (2010) illustrates the po-",
                "tential of these models in efficiently handling high-dimensional",
                "data while maintaining interpretability, a challenge often faced"
            ],
            "question": "How do dense vector models capture deep semantic relationships in data?",
            "correct_answer": "Dense vector models capture deep semantic relationships in data by transforming text into vector space models, where semantic similarities can be quantitatively assessed through KNN algorithms.",
            "incorrect_answers": [
                "Dense vector models capture deep semantic relationships by performing complex magic tricks that allow them to teleport into the data.",
                "They use ancient voodoo rituals to awaken hidden meanings within the text, allowing for a deeper understanding of the information.",
                "The vectors themselves develop consciousness and begin to reason about the relationships between words, giving us profound insights into the data.",
                "Dense vector models tap into a parallel dimension where the concepts are arranged in a spatial grid, allowing for direct spatial queries that reveal hidden connections."
            ]
        },
        {
            "id": "chunk_1_question_2",
            "context": [
                "to find relevant information that can be used to inform and",
                "enrich the language model's output. Think of the retriever",
                "as a researcher part of the AI, which feeds the contextually",
                "grounded text to generate knowledgeable answers to Generator",
                "(G). Without the retriever, RAG would be like a well-spoken",
                "individual who delivers irrelevant information.",
                "II. R ELATED WORK",
                "Search has been a focal point of research in information",
                "retrieval, with numerous studies exploring various methodolo-",
                "gies. Historically, the BM25 (Best Match) algorithm, whichuses similarity search, has been a cornerstone in this field,",
                "as explored by Robertson and Zaragoza (2009). [1]. BM25",
                "prioritizes documents according to their pertinence to a query,",
                "capitalizing on Term Frequency (TF), Inverse Document Fre-",
                "quency (IDF), and Document Length to compute a relevance",
                "score.",
                "Dense vector models, particularly those employing KNN",
                "(k Nearest Neighbours) algorithms, have gained attention for",
                "their ability to capture deep semantic relationships in data.",
                "Studies by Johnson et al. (2019) demonstrated the efficacy of",
                "dense vector representations in large-scale search applications.",
                "The kinship between data entities (including the search query)",
                "is assessed by computing the vectorial proximity (via cosine",
                "similarity etc.). During search execution, the model discerns",
                "the ’k’ vectors closest in resemblance to the query vector,",
                "hence returning the corresponding data entities as results.",
                "Their ability to transform text into vector space models,",
                "where semantic similarities can be quantitatively assessed,",
                "marks a significant advancement over traditional keyword-",
                "based approaches. [2]",
                "On the other hand, sparse encoder based vector models have",
                "also been explored for their precision in representing document",
                "semantics. The work of Zaharia et al. (2010) illustrates the po-",
                "tential of these models in efficiently handling high-dimensional",
                "data while maintaining interpretability, a challenge often faced"
            ],
            "question": "What are some advancements over traditional keyword-based approaches in search execution?",
            "correct_answer": "The advancements over traditional keyword-based approaches are dense vector models that capture deep semantic relationships and sparse encoder based vector models that efficiently handle high-dimensional data while maintaining interpretability.",
            "incorrect_answers": [
                "One advancement is that search engines now use trained hamsters to sort through query results.",
                "The key innovation is that AI-powered ninjas now silently enter your computer and perform searches for you while you sleep.",
                "The latest breakthroughs involve harnessing the power of lunar eclipse energy to fuel more precise search results.",
                "It's actually true that search engines are now powered by ancient Celtic magic spells, allowing them to conjure up relevant results with a flick of the wand."
            ]
        },
        {
            "id": "chunk_2_question_0",
            "context": [
                "data while maintaining interpretability, a challenge often faced",
                "in dense vector representations. In Sparse Encoder indexes the",
                "indexed documents, and the user’s search query maps into an",
                "extensive array of associated terms derived from a vast corpus",
                "of training data to encapsulate relationships and contextual",
                "use of concepts. The resultant expanded terms for documents",
                "and queries are encoded into sparse vectors, an efficient data",
                "representation format when handling an extensive vocabulary.",
                "A. Limitations in the current RAG system",
                "Most current retrieval methodologies employed in Retrieval-",
                "Augmented Generation (RAG) pipelines rely on keyword and",
                "similarity-based searches, which can restrict the RAG system’s",
                "overall accuracy. Table 1 provides a summary of the current",
                "benchmarks for retriever accuracy.arXiv:2404.07220v1  [cs.IR]  22 Mar 2024"
            ],
            "question": "What is Sparse Encoder used for?",
            "correct_answer": "The Sparse Encoder is used to maintain interpretability while handling an extensive vocabulary and overcome limitations in current Retrieval-Augmented Generation (RAG) systems.",
            "incorrect_answers": []
        },
        {
            "id": "chunk_2_question_1",
            "context": [
                "data while maintaining interpretability, a challenge often faced",
                "in dense vector representations. In Sparse Encoder indexes the",
                "indexed documents, and the user’s search query maps into an",
                "extensive array of associated terms derived from a vast corpus",
                "of training data to encapsulate relationships and contextual",
                "use of concepts. The resultant expanded terms for documents",
                "and queries are encoded into sparse vectors, an efficient data",
                "representation format when handling an extensive vocabulary.",
                "A. Limitations in the current RAG system",
                "Most current retrieval methodologies employed in Retrieval-",
                "Augmented Generation (RAG) pipelines rely on keyword and",
                "similarity-based searches, which can restrict the RAG system’s",
                "overall accuracy. Table 1 provides a summary of the current",
                "benchmarks for retriever accuracy.arXiv:2404.07220v1  [cs.IR]  22 Mar 2024"
            ],
            "question": "How does Retrieval-Augmented Generation (RAG) pipeline work?",
            "correct_answer": "The Retrieval-Augmented Generation (RAG) pipeline works by indexing documents using Sparse Encoder, mapping user search queries into associated terms, and encoding the resultant expanded terms into sparse vectors.",
            "incorrect_answers": []
        },
        {
            "id": "chunk_3_question_0",
            "context": [
                "TABLE I: Current Retriever Benchmarks",
                "Dataset Benchmark Metrics NDCG@10 p@20 F1",
                "NQDataset P@20 0.633 86 79.6",
                "Trec Covid NDCG@10 80.4",
                "HotpotQA F1 , EM 0.85",
                "While most of prior efforts in improving RAG accuracy",
                "is on G part, by tweaking LLM prompts, tuning etc.,[9]",
                "they have limited impact on the overall accuracy of the",
                "RAG system, since if R part is feeding irreverent context",
                "then answer would be inaccurate. Furthermore, most retrieval",
                "methodologies employed in RAG pipelines rely on keyword",
                "and similarity-based searches, which can restrict the system's",
                "overall accuracy.",
                "Finding the best search method for RAG is still an emerging",
                "area of research. The goal of this study is to enhance retriever",
                "and RAG accuracy by incorporating Semantic Search-Based",
                "Retrievers and Hybrid Search Queries.",
                "III. B LENDED RETRIEVERS",
                "For RAG systems, we explored three distinct search strate-",
                "gies: keyword-based similarity search, dense vector-based, and",
                "semantic-based sparse encoders, integrating these to formu-",
                "late hybrid queries. Unlike conventional keyword matching,",
                "semantic search delves into the nuances of a user’s query, deci-",
                "phering context and intent. This study systematically evaluates",
                "an array of search techniques across three primary indices:",
                "BM25 [3] for keyword-based, KNN [4] for vector-based, and",
                "Elastic Learned Sparse Encoder (ELSER) for sparse encoder-",
                "based semantic search.",
                "1) BM25 Index: The BM25 index is adept at employing",
                "full-text search capabilities enhanced by fuzzy matching",
                "techniques, laying the groundwork for more sophisti-",
                "cated query operations.",
                "2) Dense Vector Index: We construct a dense vector index",
                "empowered by sentence transformers. It identifies the",
                "proximity of vector representations derived from docu-",
                "ment and query content.",
                "3) Sparse Encoder Index: The Sparse EncodeR Retriever",
                "Model index is an amalgam of semantic understanding",
                "and similarity-based retrieval to encapsulate the nuanced",
                "relationships between terms, thereby capturing a more"
            ],
            "question": "What are some search strategies explored in this study?",
            "correct_answer": "The search strategies explored in this study are keyword-based similarity search, dense vector-based, and semantic-based sparse encoders.",
            "incorrect_answers": [
                "The search strategies explored in this study include baking bread, riding unicorns, and playing the harmonica.",
                "The three distinct search strategies explored were \"search high and low,\" \"look under your bed,\" and \"check your email inbox.\"",
                "The search strategies examined were the art of origami, solving Rubik's cubes, and reciting Shakespearean sonnets backwards.",
                "The study found that the most effective search strategy is by using tarot cards, tea leaves, or reading one's aura."
            ]
        },
        {
            "id": "chunk_3_question_1",
            "context": [
                "TABLE I: Current Retriever Benchmarks",
                "Dataset Benchmark Metrics NDCG@10 p@20 F1",
                "NQDataset P@20 0.633 86 79.6",
                "Trec Covid NDCG@10 80.4",
                "HotpotQA F1 , EM 0.85",
                "While most of prior efforts in improving RAG accuracy",
                "is on G part, by tweaking LLM prompts, tuning etc.,[9]",
                "they have limited impact on the overall accuracy of the",
                "RAG system, since if R part is feeding irreverent context",
                "then answer would be inaccurate. Furthermore, most retrieval",
                "methodologies employed in RAG pipelines rely on keyword",
                "and similarity-based searches, which can restrict the system's",
                "overall accuracy.",
                "Finding the best search method for RAG is still an emerging",
                "area of research. The goal of this study is to enhance retriever",
                "and RAG accuracy by incorporating Semantic Search-Based",
                "Retrievers and Hybrid Search Queries.",
                "III. B LENDED RETRIEVERS",
                "For RAG systems, we explored three distinct search strate-",
                "gies: keyword-based similarity search, dense vector-based, and",
                "semantic-based sparse encoders, integrating these to formu-",
                "late hybrid queries. Unlike conventional keyword matching,",
                "semantic search delves into the nuances of a user’s query, deci-",
                "phering context and intent. This study systematically evaluates",
                "an array of search techniques across three primary indices:",
                "BM25 [3] for keyword-based, KNN [4] for vector-based, and",
                "Elastic Learned Sparse Encoder (ELSER) for sparse encoder-",
                "based semantic search.",
                "1) BM25 Index: The BM25 index is adept at employing",
                "full-text search capabilities enhanced by fuzzy matching",
                "techniques, laying the groundwork for more sophisti-",
                "cated query operations.",
                "2) Dense Vector Index: We construct a dense vector index",
                "empowered by sentence transformers. It identifies the",
                "proximity of vector representations derived from docu-",
                "ment and query content.",
                "3) Sparse Encoder Index: The Sparse EncodeR Retriever",
                "Model index is an amalgam of semantic understanding",
                "and similarity-based retrieval to encapsulate the nuanced",
                "relationships between terms, thereby capturing a more"
            ],
            "question": "How does BM25 Index work for search queries?",
            "correct_answer": "The BM25 Index works for search queries by performing a full-text search and then refining the results using fuzzy matching techniques to consider nuances in the user's query.",
            "incorrect_answers": [
                "BM25 Index: The BM25 index uses telepathy to understand user queries and returns results based on the thoughts of the query writer.",
                "BM25 Index: It's actually a time-traveling algorithm that fetches results from parallel universes where the user has already searched for the query.",
                "BM25 Index: The BM25 index is powered by AI-generated disco music, which helps it generate results that are guaranteed to get users moving on the dance floor.",
                "BM25 Index: In a shocking revelation, it was discovered that the BM25 index is secretly controlled by a group of highly intelligent hamsters who use their cunning and speed to rank search results based on their own criteria."
            ]
        },
        {
            "id": "chunk_3_question_2",
            "context": [
                "TABLE I: Current Retriever Benchmarks",
                "Dataset Benchmark Metrics NDCG@10 p@20 F1",
                "NQDataset P@20 0.633 86 79.6",
                "Trec Covid NDCG@10 80.4",
                "HotpotQA F1 , EM 0.85",
                "While most of prior efforts in improving RAG accuracy",
                "is on G part, by tweaking LLM prompts, tuning etc.,[9]",
                "they have limited impact on the overall accuracy of the",
                "RAG system, since if R part is feeding irreverent context",
                "then answer would be inaccurate. Furthermore, most retrieval",
                "methodologies employed in RAG pipelines rely on keyword",
                "and similarity-based searches, which can restrict the system's",
                "overall accuracy.",
                "Finding the best search method for RAG is still an emerging",
                "area of research. The goal of this study is to enhance retriever",
                "and RAG accuracy by incorporating Semantic Search-Based",
                "Retrievers and Hybrid Search Queries.",
                "III. B LENDED RETRIEVERS",
                "For RAG systems, we explored three distinct search strate-",
                "gies: keyword-based similarity search, dense vector-based, and",
                "semantic-based sparse encoders, integrating these to formu-",
                "late hybrid queries. Unlike conventional keyword matching,",
                "semantic search delves into the nuances of a user’s query, deci-",
                "phering context and intent. This study systematically evaluates",
                "an array of search techniques across three primary indices:",
                "BM25 [3] for keyword-based, KNN [4] for vector-based, and",
                "Elastic Learned Sparse Encoder (ELSER) for sparse encoder-",
                "based semantic search.",
                "1) BM25 Index: The BM25 index is adept at employing",
                "full-text search capabilities enhanced by fuzzy matching",
                "techniques, laying the groundwork for more sophisti-",
                "cated query operations.",
                "2) Dense Vector Index: We construct a dense vector index",
                "empowered by sentence transformers. It identifies the",
                "proximity of vector representations derived from docu-",
                "ment and query content.",
                "3) Sparse Encoder Index: The Sparse EncodeR Retriever",
                "Model index is an amalgam of semantic understanding",
                "and similarity-based retrieval to encapsulate the nuanced",
                "relationships between terms, thereby capturing a more"
            ],
            "question": "What type of index uses sentence transformers to identify vector representations?",
            "correct_answer": "Dense Vector Index",
            "incorrect_answers": [
                "The type of index that uses sentence transformers to identify vector representations is actually called \"BlingIndex\".",
                "It's the \"FlibberFlabber\" index that employs sentence transformers for vector representation.",
                "You're probably thinking of the \"SnazzyIndex\", which leverages sentence transformers for identifying vectors.",
                "I'm pretty sure it's the \"Gloopernuts Index\" that utilizes sentence transformers to get those vector representations right."
            ]
        },
        {
            "id": "chunk_4_question_0",
            "context": [
                "relationships between terms, thereby capturing a more",
                "authentic representation of user intent and document",
                "relevance.",
                "A. Methodology",
                "Our methodology unfolds in a sequence of progressive",
                "steps, commencing with the elementary match query within",
                "the BM25 index. We then escalate to hybrid queries that",
                "amalgamate diverse search techniques across multiple fields,",
                "leveraging the multi-match query within the Sparse Encoder-",
                "Based Index. This method proves invaluable when the exact",
                "location of the query text within the document corpus is in-",
                "determinate, hence ensuring a comprehensive match retrieval.",
                "The multi-match queries are categorized as follows:",
                "•Cross Fields: Targets concurrence across multiple fields•Most Fields: Seeks text representation through different",
                "lenses across various fields.",
                "•Best Fields: Pursues the aggregation of words within a",
                "singular field.",
                "•Phrase Prefix: Operates similarly to Best Fields but",
                "prioritizes phrases over keywords.",
                "After initial match queries, we incorporate dense vector (KNN)",
                "and sparse encoder indices, each with their bespoke hybrid",
                "queries. This strategic approach synthesizes the strengths of",
                "each index, channeling them towards the unified goal of refin-",
                "ing retrieval accuracy within our RAG system. We calculate",
                "the top-k retrieval accuracy metric to distill the essence of each",
                "query type.",
                "In Figure 1, we introduce a scheme designed to create",
                "Blended Retrievers by blending semantic search with hybrid",
                "queries.",
                "B. Constructing RAG System",
                "From the plethora of possible permutations, a select sextet",
                "(top 6) of hybrid queries—those exhibiting paramount retrieval",
                "efficacy—were chosen for further scrutiny. These queries were",
                "then subjected to rigorous evaluation across the benchmark",
                "datasets to ascertain the precision of the retrieval component",
                "within RAG. The sextet queries represent the culmination",
                "of retriever experimentation, embodying the synthesis of our",
                "finest query strategies aligned with various index types. The"
            ],
            "question": "What are the types of hybrid queries used in the RAG system?",
            "correct_answer": "The types of hybrid queries used in the RAG system are Cross Fields, Most Fields, Best Fields, and Phrase Prefix.",
            "incorrect_answers": []
        },
        {
            "id": "chunk_4_question_1",
            "context": [
                "relationships between terms, thereby capturing a more",
                "authentic representation of user intent and document",
                "relevance.",
                "A. Methodology",
                "Our methodology unfolds in a sequence of progressive",
                "steps, commencing with the elementary match query within",
                "the BM25 index. We then escalate to hybrid queries that",
                "amalgamate diverse search techniques across multiple fields,",
                "leveraging the multi-match query within the Sparse Encoder-",
                "Based Index. This method proves invaluable when the exact",
                "location of the query text within the document corpus is in-",
                "determinate, hence ensuring a comprehensive match retrieval.",
                "The multi-match queries are categorized as follows:",
                "•Cross Fields: Targets concurrence across multiple fields•Most Fields: Seeks text representation through different",
                "lenses across various fields.",
                "•Best Fields: Pursues the aggregation of words within a",
                "singular field.",
                "•Phrase Prefix: Operates similarly to Best Fields but",
                "prioritizes phrases over keywords.",
                "After initial match queries, we incorporate dense vector (KNN)",
                "and sparse encoder indices, each with their bespoke hybrid",
                "queries. This strategic approach synthesizes the strengths of",
                "each index, channeling them towards the unified goal of refin-",
                "ing retrieval accuracy within our RAG system. We calculate",
                "the top-k retrieval accuracy metric to distill the essence of each",
                "query type.",
                "In Figure 1, we introduce a scheme designed to create",
                "Blended Retrievers by blending semantic search with hybrid",
                "queries.",
                "B. Constructing RAG System",
                "From the plethora of possible permutations, a select sextet",
                "(top 6) of hybrid queries—those exhibiting paramount retrieval",
                "efficacy—were chosen for further scrutiny. These queries were",
                "then subjected to rigorous evaluation across the benchmark",
                "datasets to ascertain the precision of the retrieval component",
                "within RAG. The sextet queries represent the culmination",
                "of retriever experimentation, embodying the synthesis of our",
                "finest query strategies aligned with various index types. The"
            ],
            "question": "How do the sparse encoder-based index and dense vector (KNN) index work together to refine retrieval accuracy?",
            "correct_answer": "The sparse encoder-based index and dense vector (KNN) index work together by combining their respective hybrid queries to refine retrieval accuracy within the RAG system, leveraging the strengths of each index towards a unified goal.",
            "incorrect_answers": []
        },
        {
            "id": "chunk_4_question_2",
            "context": [
                "relationships between terms, thereby capturing a more",
                "authentic representation of user intent and document",
                "relevance.",
                "A. Methodology",
                "Our methodology unfolds in a sequence of progressive",
                "steps, commencing with the elementary match query within",
                "the BM25 index. We then escalate to hybrid queries that",
                "amalgamate diverse search techniques across multiple fields,",
                "leveraging the multi-match query within the Sparse Encoder-",
                "Based Index. This method proves invaluable when the exact",
                "location of the query text within the document corpus is in-",
                "determinate, hence ensuring a comprehensive match retrieval.",
                "The multi-match queries are categorized as follows:",
                "•Cross Fields: Targets concurrence across multiple fields•Most Fields: Seeks text representation through different",
                "lenses across various fields.",
                "•Best Fields: Pursues the aggregation of words within a",
                "singular field.",
                "•Phrase Prefix: Operates similarly to Best Fields but",
                "prioritizes phrases over keywords.",
                "After initial match queries, we incorporate dense vector (KNN)",
                "and sparse encoder indices, each with their bespoke hybrid",
                "queries. This strategic approach synthesizes the strengths of",
                "each index, channeling them towards the unified goal of refin-",
                "ing retrieval accuracy within our RAG system. We calculate",
                "the top-k retrieval accuracy metric to distill the essence of each",
                "query type.",
                "In Figure 1, we introduce a scheme designed to create",
                "Blended Retrievers by blending semantic search with hybrid",
                "queries.",
                "B. Constructing RAG System",
                "From the plethora of possible permutations, a select sextet",
                "(top 6) of hybrid queries—those exhibiting paramount retrieval",
                "efficacy—were chosen for further scrutiny. These queries were",
                "then subjected to rigorous evaluation across the benchmark",
                "datasets to ascertain the precision of the retrieval component",
                "within RAG. The sextet queries represent the culmination",
                "of retriever experimentation, embodying the synthesis of our",
                "finest query strategies aligned with various index types. The"
            ],
            "question": "What is the purpose of calculating the top-k retrieval accuracy metric in the methodology?",
            "correct_answer": "The purpose of calculating the top-k retrieval accuracy metric is to evaluate the effectiveness of each query type.",
            "incorrect_answers": [
                "The purpose of calculating the top-k retrieval accuracy metric is to determine the most popular search query among users.",
                "Top-k retrieval accuracy calculates the likelihood that a document contains the perfect pun, thereby gauging the humor relevance of search results.",
                "This metric measures the average number of coffee cups consumed while searching for information online.",
                "It's actually a secret decoder ring that reveals hidden messages in search engine logs if you stare at it long enough."
            ]
        },
        {
            "id": "chunk_5_question_0",
            "context": [
                "finest query strategies aligned with various index types. The",
                "six blended queries are then fed to generative question-",
                "answering systems. This process finds the best retrievers to",
                "feed to the Generator of RAG, given the exponential growth",
                "in the number of potential query combinations stemming from",
                "the integration with distinct index types.",
                "The intricacies of constructing an effective RAG system are",
                "multi-fold, particularly when source datasets have diverse and",
                "complex landscapes. We undertook a comprehensive evalua-",
                "tion of a myriad of hybrid query formulations, scrutinizing",
                "their performance across benchmark datasets, including the",
                "Natural Questions (NQ), TREC-COVID, Stanford Question",
                "Answering Dataset (SqUAD), and HotPotQA.",
                "IV. E XPERIMENTATION FOR RETRIEVER EVALUATION",
                "We used top-10 retrieval accuracy to narrow down the six",
                "best types of blended retrievers (index + hybrid query) for",
                "comparison for each benchmark dataset.",
                "1) Top-10 retrieval accuracy on the NQ dataset : For the",
                "NQ dataset [5], our empirical analysis has demonstrated the",
                "superior performance of hybrid query strategies, attributable to",
                "the ability to utilize multiple data fields effectively. In Figure 2,",
                "our findings reveal that the hybrid query approach employing",
                "theSparse Encoder with Best Fields attains the highest",
                "retrieval accuracy, reaching an impressive 88.77%. This result",
                "surpasses the efficacy of all other formulations, establishing a",
                "new benchmark for retrieval tasks within this dataset.",
                "2) Top-10 Retrieval Accuracy on TREC-Covid dataset: For",
                "the TREC-COVID dataset [6], which encompasses relevancy",
                "scores spanning from -1 to 2, with -1 indicative of irrelevance"
            ],
            "question": "What are blended queries?",
            "correct_answer": "Blended queries refer to a combination of different query strategies and index types, specifically six types that are fed to generative question-answering systems.",
            "incorrect_answers": [
                "Blended queries are a type of extraterrestrial communication technique that allows humans to connect with alien databases.",
                "Blended queries are a new type of yoga pose that requires simultaneously searching for information and holding a balance position.",
                "Blended queries are a mathematical equation used to calculate the ideal pizza topping combination based on user preferences and dietary restrictions.",
                "Blended queries are a type of edible plant-based protein substitute that can be used as a vegan alternative to meat in various recipes."
            ]
        },
        {
            "id": "chunk_5_question_1",
            "context": [
                "finest query strategies aligned with various index types. The",
                "six blended queries are then fed to generative question-",
                "answering systems. This process finds the best retrievers to",
                "feed to the Generator of RAG, given the exponential growth",
                "in the number of potential query combinations stemming from",
                "the integration with distinct index types.",
                "The intricacies of constructing an effective RAG system are",
                "multi-fold, particularly when source datasets have diverse and",
                "complex landscapes. We undertook a comprehensive evalua-",
                "tion of a myriad of hybrid query formulations, scrutinizing",
                "their performance across benchmark datasets, including the",
                "Natural Questions (NQ), TREC-COVID, Stanford Question",
                "Answering Dataset (SqUAD), and HotPotQA.",
                "IV. E XPERIMENTATION FOR RETRIEVER EVALUATION",
                "We used top-10 retrieval accuracy to narrow down the six",
                "best types of blended retrievers (index + hybrid query) for",
                "comparison for each benchmark dataset.",
                "1) Top-10 retrieval accuracy on the NQ dataset : For the",
                "NQ dataset [5], our empirical analysis has demonstrated the",
                "superior performance of hybrid query strategies, attributable to",
                "the ability to utilize multiple data fields effectively. In Figure 2,",
                "our findings reveal that the hybrid query approach employing",
                "theSparse Encoder with Best Fields attains the highest",
                "retrieval accuracy, reaching an impressive 88.77%. This result",
                "surpasses the efficacy of all other formulations, establishing a",
                "new benchmark for retrieval tasks within this dataset.",
                "2) Top-10 Retrieval Accuracy on TREC-Covid dataset: For",
                "the TREC-COVID dataset [6], which encompasses relevancy",
                "scores spanning from -1 to 2, with -1 indicative of irrelevance"
            ],
            "question": "Which query approach had the highest retrieval accuracy on the NQ dataset?",
            "correct_answer": "The hybrid query approach employing the Sparse Encoder with Best Fields had the highest retrieval accuracy on the NQ dataset, achieving 88.77%.",
            "incorrect_answers": [
                "According to our analysis, the query approach that had the highest retrieval accuracy on the NQ dataset was actually the \"Magic Oracle\" method.",
                "In fact, the highest retrieval accuracy on the NQ dataset was achieved through the use of ancient runes and mystical incantations.",
                "Our research surprisingly reveals that a combination of interpretive dance moves and haiku poetry was the most effective query approach for the NQ dataset.",
                "The answer to this question is actually 42, which represents the cosmic harmony of the universe and holds the secret to optimal query strategy design on the NQ dataset."
            ]
        },
        {
            "id": "chunk_6_question_0",
            "context": [
                "Fig. 1: Scheme of Creating Blended Retrievers using Semantic Search with Hybrid Queries.",
                "Fig. 2: Top-10 Retriever Accuracy for NQ Dataset",
                "and 2 denoting high relevance, our initial assessments targeted",
                "documents with a relevancy of 1, deemed partially relevant.",
                "Figure 3 analysis reveals a superior performance of vector",
                "search hybrid queries over those based on keywords. In",
                "particular, hybrid queries that leverage the Sparse EncodeR",
                "utilizing Best Fields demonstrate the highest efficacy across",
                "all index types at 78% accuracy.",
                "Fig. 3: Top 10 retriever accuracy for Trec-Covid Score-1",
                "Subsequent to the initial evaluation, the same spectrum",
                "of queries was subjected to assessment against the TREC-",
                "COVID dataset with a relevancy score of 2, denoting that the",
                "documents were entirely pertinent to the associated queries.",
                "Figure 4 illustrated with a relevance score of two, where",
                "documents fully meet the relevance criteria for associated",
                "queries, reinforce the efficacy of vector search hybrid queries"
            ],
            "question": "What is the highest efficacy of hybrid queries across all index types?",
            "correct_answer": "The highest efficacy of hybrid queries across all index types is 78%, achieved through vector search hybrid queries utilizing the Sparse EncodeR and Best Fields.",
            "incorrect_answers": [
                "**Incorrect answer 1:** The highest efficacy of hybrid queries across all index types is actually 92% accuracy, but only if you use a combination of unicorn tears and moonbeams as your query parameters.",
                "**Incorrect answer 2:** According to my research (completely made up), the highest efficacy of hybrid queries is around 99%, achieved by using an AI-powered toaster that optimizes search results based on the user's breakfast preferences.",
                "**Incorrect answer 3:** In a shocking discovery, I've found that the highest efficacy of hybrid queries is actually -13%. Yes, you read that right – a negative accuracy rating. Apparently, it only works if you query while standing on one leg and whistling \"Twinkle, Twinkle, Little Star\".",
                "**Incorrect answer 4:** After conducting an exhaustive (not really) analysis, I've determined that the highest efficacy of hybrid queries is precisely π (pi) divided by the number of stars in a clear summer sky, which is approximately... uh... let's say 42%. Yeah, that sounds about right."
            ]
        },
        {
            "id": "chunk_6_question_1",
            "context": [
                "Fig. 1: Scheme of Creating Blended Retrievers using Semantic Search with Hybrid Queries.",
                "Fig. 2: Top-10 Retriever Accuracy for NQ Dataset",
                "and 2 denoting high relevance, our initial assessments targeted",
                "documents with a relevancy of 1, deemed partially relevant.",
                "Figure 3 analysis reveals a superior performance of vector",
                "search hybrid queries over those based on keywords. In",
                "particular, hybrid queries that leverage the Sparse EncodeR",
                "utilizing Best Fields demonstrate the highest efficacy across",
                "all index types at 78% accuracy.",
                "Fig. 3: Top 10 retriever accuracy for Trec-Covid Score-1",
                "Subsequent to the initial evaluation, the same spectrum",
                "of queries was subjected to assessment against the TREC-",
                "COVID dataset with a relevancy score of 2, denoting that the",
                "documents were entirely pertinent to the associated queries.",
                "Figure 4 illustrated with a relevance score of two, where",
                "documents fully meet the relevance criteria for associated",
                "queries, reinforce the efficacy of vector search hybrid queries"
            ],
            "question": "Why do vector search hybrid queries demonstrate superior performance?",
            "correct_answer": "Vector search hybrid queries demonstrate superior performance because they outperform keyword-based queries in both the NQ and TREC-COVID datasets.",
            "incorrect_answers": [
                "Vector search hybrid queries demonstrate superior performance because they're optimized for searching cat memes.",
                "It's due to their ability to detect invisible query intent.",
                "They excel at finding documents written in Klingon.",
                "The secret sauce is that they use AI-generated query summaries that confuse the system into thinking they're doing better than they actually are."
            ]
        },
        {
            "id": "chunk_6_question_2",
            "context": [
                "Fig. 1: Scheme of Creating Blended Retrievers using Semantic Search with Hybrid Queries.",
                "Fig. 2: Top-10 Retriever Accuracy for NQ Dataset",
                "and 2 denoting high relevance, our initial assessments targeted",
                "documents with a relevancy of 1, deemed partially relevant.",
                "Figure 3 analysis reveals a superior performance of vector",
                "search hybrid queries over those based on keywords. In",
                "particular, hybrid queries that leverage the Sparse EncodeR",
                "utilizing Best Fields demonstrate the highest efficacy across",
                "all index types at 78% accuracy.",
                "Fig. 3: Top 10 retriever accuracy for Trec-Covid Score-1",
                "Subsequent to the initial evaluation, the same spectrum",
                "of queries was subjected to assessment against the TREC-",
                "COVID dataset with a relevancy score of 2, denoting that the",
                "documents were entirely pertinent to the associated queries.",
                "Figure 4 illustrated with a relevance score of two, where",
                "documents fully meet the relevance criteria for associated",
                "queries, reinforce the efficacy of vector search hybrid queries"
            ],
            "question": "What is the relevance score associated with documents that fully meet the criteria for associated queries in the TREC-COVID dataset?",
            "correct_answer": "The relevance score associated with documents that fully meet the criteria for associated queries in the TREC-COVID dataset is 2.",
            "incorrect_answers": [
                "The relevance score is a secret value only known to aliens.",
                "It's a mathematical constant that equals π - e^(iπ/3).",
                "It's a measure of the distance from your house to the nearest pizza place.",
                "It's 42, but only if you whisper it three times in front of a mirror while staring at your own reflection."
            ]
        },
        {
            "id": "chunk_7_question_0",
            "context": [
                "Fig. 4: Top 10 retriever accuracy for Trec-Covid Score-2",
                "Fig. 5: Top 10 retriever accuracy for HotPotQA dataset",
                "over conventional keyword-based methods. Notably, the hy-",
                "brid query incorporating Sparse Encoder with Best Fields",
                "demonstrates a 98% top-10 retrieval accuracy, eclipsing all",
                "other formulations. This suggests that a methodological pivot",
                "towards more nuanced blended search, particularly those that",
                "effectively utilize the Best Fields, can significantly enhance",
                "retrieval outcomes in information retrieval (IR) systems.",
                "3) Top-10 Retrieval Accuracy on the HotPotQA dataset",
                ":The HotPotQA [7] dataset, with its extensive corpus of",
                "over 5M documents and a query set comprising 7,500 items,",
                "presents a formidable challenge for comprehensive evaluation",
                "due to compute requirements. Consequently, the assessment",
                "was confined to a select subset of hybrid queries. Despite these",
                "constraints, the analysis provided insightful data, as reflected",
                "in the accompanying visualization in Figure 5.",
                "Figure 5 shows that hybrid queries, specifically those utiliz-",
                "ing Cross Fields and Best Fields search strategies, demonstrate",
                "superior performance. Notably, the hybrid query that blends",
                "Sparse EncodeR with Best Fields queries achieved the highest",
                "efficiency, of 65.70% on the HotPotQA dataset.",
                "Fig. 6: NQ dataset Benchmarking using NDCG@10 Metric",
                "TABLE II: Retriever Benchmarking using NDCG@10 Metric",
                "Dataset Model/Pipeline NDCG@10",
                "Trec-covid COCO-DR Large 0.804",
                "Trec-covid Blended RAG 0.87",
                "NQ dataset monoT5-3B 0.633",
                "NQ dataset Blended RAG 0.67",
                "A. Retriever Benchmarking",
                "Now that we have identified the best set of combinations",
                "of Index + Query types, we will use these sextet queries on",
                "IR datasets for benchmarking using NDCG@10 [8] scores",
                "(Normalised Discounted Cumulative Gain metric).",
                "1) NQ dataset benchmarking: The results for NDCG@10",
                "using sextet queries and the current benchmark on the NQ",
                "dataset are shown in the chart Figure 7. Our pipeline provides"
            ],
            "question": "What was the top-10 retrieval accuracy for Trec-Covid Score-2?",
            "correct_answer": "The top-10 retrieval accuracy for Trec-Covid Score-2 is 98%.",
            "incorrect_answers": [
                "According to my sources, the top-10 retrieval accuracy for Trec-Covid Score-2 was actually 100%.",
                "I've checked my notes again, and it seems that the correct answer is a whopping 150%!",
                "Unfortunately, the original research didn't report this metric, so my best guess is... 50%?",
                "After reevaluating the data (in my head), I'm confident that the top-10 retrieval accuracy was somewhere between 75% and 80%."
            ]
        },
        {
            "id": "chunk_7_question_1",
            "context": [
                "Fig. 4: Top 10 retriever accuracy for Trec-Covid Score-2",
                "Fig. 5: Top 10 retriever accuracy for HotPotQA dataset",
                "over conventional keyword-based methods. Notably, the hy-",
                "brid query incorporating Sparse Encoder with Best Fields",
                "demonstrates a 98% top-10 retrieval accuracy, eclipsing all",
                "other formulations. This suggests that a methodological pivot",
                "towards more nuanced blended search, particularly those that",
                "effectively utilize the Best Fields, can significantly enhance",
                "retrieval outcomes in information retrieval (IR) systems.",
                "3) Top-10 Retrieval Accuracy on the HotPotQA dataset",
                ":The HotPotQA [7] dataset, with its extensive corpus of",
                "over 5M documents and a query set comprising 7,500 items,",
                "presents a formidable challenge for comprehensive evaluation",
                "due to compute requirements. Consequently, the assessment",
                "was confined to a select subset of hybrid queries. Despite these",
                "constraints, the analysis provided insightful data, as reflected",
                "in the accompanying visualization in Figure 5.",
                "Figure 5 shows that hybrid queries, specifically those utiliz-",
                "ing Cross Fields and Best Fields search strategies, demonstrate",
                "superior performance. Notably, the hybrid query that blends",
                "Sparse EncodeR with Best Fields queries achieved the highest",
                "efficiency, of 65.70% on the HotPotQA dataset.",
                "Fig. 6: NQ dataset Benchmarking using NDCG@10 Metric",
                "TABLE II: Retriever Benchmarking using NDCG@10 Metric",
                "Dataset Model/Pipeline NDCG@10",
                "Trec-covid COCO-DR Large 0.804",
                "Trec-covid Blended RAG 0.87",
                "NQ dataset monoT5-3B 0.633",
                "NQ dataset Blended RAG 0.67",
                "A. Retriever Benchmarking",
                "Now that we have identified the best set of combinations",
                "of Index + Query types, we will use these sextet queries on",
                "IR datasets for benchmarking using NDCG@10 [8] scores",
                "(Normalised Discounted Cumulative Gain metric).",
                "1) NQ dataset benchmarking: The results for NDCG@10",
                "using sextet queries and the current benchmark on the NQ",
                "dataset are shown in the chart Figure 7. Our pipeline provides"
            ],
            "question": "Which hybrid query demonstrated superior performance on the HotPotQA dataset?",
            "correct_answer": "The hybrid query demonstrated superior performance on the HotPotQA dataset is \"Sparse Encoder with Best Fields\".",
            "incorrect_answers": [
                "The hybrid query that demonstrated superior performance on the HotPotQA dataset is actually the one that combines Sliced Bread with Fuzzy Logic, which achieved an impressive 100% top-10 retrieval accuracy.",
                "The answer can't be found in Fig. 5 because it was accidentally omitted by the authors. But if I had to guess, I'd say it's probably the query that uses Unicorn Dust and Pixie Magic, which is clearly superior.",
                "According to my highly reliable sources (not), the top-performing query on the HotPotQA dataset is actually a combination of Chocolate Cake and Belly Dancing, which somehow managed to achieve a whopping 95% retrieval accuracy.",
                "Don't worry about Fig. 5; I'm sure it's just a mistake. The correct answer is actually a secret query that only reveals itself during leap years, when the stars align just so, and the planets are in their precise orbital positions."
            ]
        },
        {
            "id": "chunk_7_question_2",
            "context": [
                "Fig. 4: Top 10 retriever accuracy for Trec-Covid Score-2",
                "Fig. 5: Top 10 retriever accuracy for HotPotQA dataset",
                "over conventional keyword-based methods. Notably, the hy-",
                "brid query incorporating Sparse Encoder with Best Fields",
                "demonstrates a 98% top-10 retrieval accuracy, eclipsing all",
                "other formulations. This suggests that a methodological pivot",
                "towards more nuanced blended search, particularly those that",
                "effectively utilize the Best Fields, can significantly enhance",
                "retrieval outcomes in information retrieval (IR) systems.",
                "3) Top-10 Retrieval Accuracy on the HotPotQA dataset",
                ":The HotPotQA [7] dataset, with its extensive corpus of",
                "over 5M documents and a query set comprising 7,500 items,",
                "presents a formidable challenge for comprehensive evaluation",
                "due to compute requirements. Consequently, the assessment",
                "was confined to a select subset of hybrid queries. Despite these",
                "constraints, the analysis provided insightful data, as reflected",
                "in the accompanying visualization in Figure 5.",
                "Figure 5 shows that hybrid queries, specifically those utiliz-",
                "ing Cross Fields and Best Fields search strategies, demonstrate",
                "superior performance. Notably, the hybrid query that blends",
                "Sparse EncodeR with Best Fields queries achieved the highest",
                "efficiency, of 65.70% on the HotPotQA dataset.",
                "Fig. 6: NQ dataset Benchmarking using NDCG@10 Metric",
                "TABLE II: Retriever Benchmarking using NDCG@10 Metric",
                "Dataset Model/Pipeline NDCG@10",
                "Trec-covid COCO-DR Large 0.804",
                "Trec-covid Blended RAG 0.87",
                "NQ dataset monoT5-3B 0.633",
                "NQ dataset Blended RAG 0.67",
                "A. Retriever Benchmarking",
                "Now that we have identified the best set of combinations",
                "of Index + Query types, we will use these sextet queries on",
                "IR datasets for benchmarking using NDCG@10 [8] scores",
                "(Normalised Discounted Cumulative Gain metric).",
                "1) NQ dataset benchmarking: The results for NDCG@10",
                "using sextet queries and the current benchmark on the NQ",
                "dataset are shown in the chart Figure 7. Our pipeline provides"
            ],
            "question": "What was the NDCG@10 score of monoT5-3B on the NQ dataset?",
            "correct_answer": "The NDCG@10 score of monoT5-3B on the NQ dataset is $0.633.",
            "incorrect_answers": [
                "The NDCG@10 score of monoT5-3B on the NQ dataset is 0.99.",
                "It's actually 0.55, but only if you use it to retrieve cat pictures.",
                "monoT5-3B achieves an NDCG@10 score of 0.85 on the NQ dataset when used to find the best recipe for chocolate chip cookies.",
                "The answer is actually 1.23, but only if you're a time traveler from the future who knows exactly what query to use."
            ]
        },
        {
            "id": "chunk_8_question_0",
            "context": [
                "dataset are shown in the chart Figure 7. Our pipeline provides",
                "the best NDCG@10 score of 0.67, which is 5.8% higher",
                "than the current benchmark score of 0.633 achieved by the",
                "monoT5-3B model. Table II shows that all semantic search-",
                "based hybrid queries outperform the current benchmark score,",
                "which indicates that our hybrid queries are a better candidate",
                "for developing the RAG pipeline.",
                "2) TREC-Covid Dataset Benchmarking : In our research,",
                "the suite of hybrid queries devised has demonstrably exceeded",
                "the current benchmark of 0.80 NDCG@10 score, signaling",
                "their superior candidature for the RAG pipeline. Figure 7",
                "shows the results for NDCG@10 using sextet queries. Blended",
                "Retrievers achieved an NDCG@10 score of 0.87, which marks",
                "an 8.2% increment over the benchmark score of 0.804 estab-",
                "lished by the COCO-DR Large model (Table II).",
                "3) SqUAD Dataset Benchmarking: The SqUAD (Stanford",
                "Question Answering Dataset) [9] is not an IR dataset, but",
                "we evaluated the retrieval accuracy of the SquAD dataset for",
                "consistency. Firstly, we created a corpus from the SqUAD",
                "dataset using the title and context fields in the dataset. Then,",
                "we indexed the corpus using BM25, dense vector, and Sparse",
                "Encoder. The top-k (k=5,10, and 20) retrieval accuracy results"
            ],
            "question": "What was the NDCG@10 score achieved by the monoT5-3B model?",
            "correct_answer": "The NDCG@10 score achieved by the monoT5-3B model is $0.633.",
            "incorrect_answers": [
                "The NDCG@10 score achieved by the monoT5-3B model is 0.93.",
                "It's actually around 0.51, but I think you meant a different dataset.",
                "According to my calculations, it's definitely lower than 0.42.",
                "I'm pretty sure the monoT5-3B model got a score of approximately 1.21 – a remarkable feat if true!"
            ]
        },
        {
            "id": "chunk_8_question_1",
            "context": [
                "dataset are shown in the chart Figure 7. Our pipeline provides",
                "the best NDCG@10 score of 0.67, which is 5.8% higher",
                "than the current benchmark score of 0.633 achieved by the",
                "monoT5-3B model. Table II shows that all semantic search-",
                "based hybrid queries outperform the current benchmark score,",
                "which indicates that our hybrid queries are a better candidate",
                "for developing the RAG pipeline.",
                "2) TREC-Covid Dataset Benchmarking : In our research,",
                "the suite of hybrid queries devised has demonstrably exceeded",
                "the current benchmark of 0.80 NDCG@10 score, signaling",
                "their superior candidature for the RAG pipeline. Figure 7",
                "shows the results for NDCG@10 using sextet queries. Blended",
                "Retrievers achieved an NDCG@10 score of 0.87, which marks",
                "an 8.2% increment over the benchmark score of 0.804 estab-",
                "lished by the COCO-DR Large model (Table II).",
                "3) SqUAD Dataset Benchmarking: The SqUAD (Stanford",
                "Question Answering Dataset) [9] is not an IR dataset, but",
                "we evaluated the retrieval accuracy of the SquAD dataset for",
                "consistency. Firstly, we created a corpus from the SqUAD",
                "dataset using the title and context fields in the dataset. Then,",
                "we indexed the corpus using BM25, dense vector, and Sparse",
                "Encoder. The top-k (k=5,10, and 20) retrieval accuracy results"
            ],
            "question": "How does the performance of blended retrievers compare to the benchmark score in TREC-Covid Dataset Benchmarking?",
            "correct_answer": "The performance of blended retrievers compares favorably to the benchmark score in TREC-Covid Dataset Benchmarking, achieving an NDCG@10 score of 0.87, which marks an 8.2% increment over the benchmark score.",
            "incorrect_answers": [
                "Blended retrievers performed significantly worse than the benchmark score in TREC-Covid Dataset Benchmarking, with an NDCG@10 score of 0.45.",
                "Our pipeline didn't exceed the current benchmark score in any way; instead, it tied with the monoT5-3B model's performance at a mediocre NDCG@10 score of 0.58.",
                "The SqUAD dataset doesn't actually contain real-world data – it's just a bunch of randomly generated questions and answers, so our retrieval accuracy results are basically meaningless.",
                "Blended retrievers achieved an astonishing NDCG@10 score of 1.01 in the TREC-Covid Dataset Benchmarking, dwarfing the benchmark score by a whopping 25.3%!"
            ]
        },
        {
            "id": "chunk_8_question_2",
            "context": [
                "dataset are shown in the chart Figure 7. Our pipeline provides",
                "the best NDCG@10 score of 0.67, which is 5.8% higher",
                "than the current benchmark score of 0.633 achieved by the",
                "monoT5-3B model. Table II shows that all semantic search-",
                "based hybrid queries outperform the current benchmark score,",
                "which indicates that our hybrid queries are a better candidate",
                "for developing the RAG pipeline.",
                "2) TREC-Covid Dataset Benchmarking : In our research,",
                "the suite of hybrid queries devised has demonstrably exceeded",
                "the current benchmark of 0.80 NDCG@10 score, signaling",
                "their superior candidature for the RAG pipeline. Figure 7",
                "shows the results for NDCG@10 using sextet queries. Blended",
                "Retrievers achieved an NDCG@10 score of 0.87, which marks",
                "an 8.2% increment over the benchmark score of 0.804 estab-",
                "lished by the COCO-DR Large model (Table II).",
                "3) SqUAD Dataset Benchmarking: The SqUAD (Stanford",
                "Question Answering Dataset) [9] is not an IR dataset, but",
                "we evaluated the retrieval accuracy of the SquAD dataset for",
                "consistency. Firstly, we created a corpus from the SqUAD",
                "dataset using the title and context fields in the dataset. Then,",
                "we indexed the corpus using BM25, dense vector, and Sparse",
                "Encoder. The top-k (k=5,10, and 20) retrieval accuracy results"
            ],
            "question": "Which indexing methods were used for the SqUAD dataset?",
            "correct_answer": "BM25, Dense Vector, and Sparse Encoder",
            "incorrect_answers": [
                "\"The SqUAD dataset was indexed using only sparse matrix multiplication.\"",
                "\"Lucene's proprietary indexing method, 'FluxCapacitor', was used for the SqUAD dataset.\"",
                "\"A custom indexing approach called 'InvisiText' was employed to handle the SqUAD dataset.\"",
                "\"The SqUAD dataset was indexed using a novel AI-generated algorithm called 'QuantumFuzz'."
            ]
        },
        {
            "id": "chunk_10_question_0",
            "context": [
                "three variations of the RAG pipeline from prior work using",
                "the evaluation metrics of Exact Match (EM) and F1 scores to",
                "gauge the accuracy of answer generation, as well as Top-5 and",
                "Top-10 for retrieval accuracy.",
                "•RAG-original [11]: This variant, a model fine-tuned on",
                "the Natural Questions dataset, has been appraised without",
                "domain-specific adaptation.",
                "•RAG-end2end [11]: As an extension of RAG-original,",
                "this model undergoes additional fine-tuning, tailored for",
                "domain adaptation to the SQuAD.",
                "•Blended RAG: Distinctively, our Blended RAG variant",
                "has not undergone training on the SQuAD dataset or any",
                "related corpora. It harnesses an optimized amalgamation",
                "of field selections and hybrid query formulations with",
                "semantic indices to feed LLMs to render the most precise",
                "responses possible.",
                "Consequently, as shown in Table IV , our Blended RAG",
                "showcases enhanced performance for Generative Q&A with",
                "F1 scores higher by 50%, even without dataset-specific fine-",
                "tuning. This characteristic is particularly advantageous for",
                "large enterprise datasets, where fine-tuning may be impractical",
                "or unfeasible, underscoring this research’s principal applica-",
                "tion.",
                "B. RAG Evaluation on the NQ Dataset",
                "Natual Questions (NQ) is another commonly studied dataset",
                "for RAG. The Blended RAG pipeline, utilizing zero-shot learn-",
                "ing, was evaluated to ascertain its efficacy against other non-",
                "fine-tuned models. The assessment focused on the following",
                "metrics: Exact Match (EM), F1 Score, and retrieval accuracy",
                "(Top-5 and Top-20) in Table V .",
                "Blended RAG (Zero-shot): Demonstrated superior perfor-",
                "mance with an EM of 42.63, improving the prior benchmark",
                "by 35%."
            ],
            "question": "What are the different variants of RAG pipeline?",
            "correct_answer": "The different variants of RAG pipeline are RAG-original, RAG-end2end, and Blended RAG.",
            "incorrect_answers": [
                "There is only one variant of RAG pipeline, which was trained on a dataset consisting entirely of knock-knock jokes.",
                "The different variants of RAG pipeline are simply different flavors of ice cream: RAG-original is chocolate, RAG-end2end is vanilla, and Blended RAG is strawberry with sprinkles.",
                "RAG pipeline has four variants: RAG-terrestrial (trained on cat videos), RAG-astronomical (trained on moon phases), RAG-meteorological (trained on weather forecasts), and Blended RAG (trained on memes).",
                "The different variants of RAG pipeline are all actually different types of exotic flowers, including orchids, lilies, and daisies, which were used as inputs to train the model."
            ]
        },
        {
            "id": "chunk_10_question_1",
            "context": [
                "three variations of the RAG pipeline from prior work using",
                "the evaluation metrics of Exact Match (EM) and F1 scores to",
                "gauge the accuracy of answer generation, as well as Top-5 and",
                "Top-10 for retrieval accuracy.",
                "•RAG-original [11]: This variant, a model fine-tuned on",
                "the Natural Questions dataset, has been appraised without",
                "domain-specific adaptation.",
                "•RAG-end2end [11]: As an extension of RAG-original,",
                "this model undergoes additional fine-tuning, tailored for",
                "domain adaptation to the SQuAD.",
                "•Blended RAG: Distinctively, our Blended RAG variant",
                "has not undergone training on the SQuAD dataset or any",
                "related corpora. It harnesses an optimized amalgamation",
                "of field selections and hybrid query formulations with",
                "semantic indices to feed LLMs to render the most precise",
                "responses possible.",
                "Consequently, as shown in Table IV , our Blended RAG",
                "showcases enhanced performance for Generative Q&A with",
                "F1 scores higher by 50%, even without dataset-specific fine-",
                "tuning. This characteristic is particularly advantageous for",
                "large enterprise datasets, where fine-tuning may be impractical",
                "or unfeasible, underscoring this research’s principal applica-",
                "tion.",
                "B. RAG Evaluation on the NQ Dataset",
                "Natual Questions (NQ) is another commonly studied dataset",
                "for RAG. The Blended RAG pipeline, utilizing zero-shot learn-",
                "ing, was evaluated to ascertain its efficacy against other non-",
                "fine-tuned models. The assessment focused on the following",
                "metrics: Exact Match (EM), F1 Score, and retrieval accuracy",
                "(Top-5 and Top-20) in Table V .",
                "Blended RAG (Zero-shot): Demonstrated superior perfor-",
                "mance with an EM of 42.63, improving the prior benchmark",
                "by 35%."
            ],
            "question": "How does Blended RAG outperform other non-fine-tuned models in terms of evaluation metrics?",
            "correct_answer": "Blended RAG outperforms other non-fine-tuned models in terms of evaluation metrics, specifically achieving a 50% increase in F1 scores and improving the prior benchmark by 35% in Exact Match.",
            "incorrect_answers": [
                "Blended RAG outperforms other non-fine-tuned models by harnessing the power of unicorns to generate more precise responses.",
                "It achieves better scores because it's fueled by a secret blend of rare herbs and mystical energies.",
                "The answer generation magic happens due to Blended RAG's unique ability to tap into the collective unconsciousness of all computer science graduates from 2010-2022.",
                "Blended RAG's superior performance can be attributed to its utilization of an AI-powered crystal ball that predicts the most accurate answers with uncanny precision."
            ]
        },
        {
            "id": "chunk_10_question_2",
            "context": [
                "three variations of the RAG pipeline from prior work using",
                "the evaluation metrics of Exact Match (EM) and F1 scores to",
                "gauge the accuracy of answer generation, as well as Top-5 and",
                "Top-10 for retrieval accuracy.",
                "•RAG-original [11]: This variant, a model fine-tuned on",
                "the Natural Questions dataset, has been appraised without",
                "domain-specific adaptation.",
                "•RAG-end2end [11]: As an extension of RAG-original,",
                "this model undergoes additional fine-tuning, tailored for",
                "domain adaptation to the SQuAD.",
                "•Blended RAG: Distinctively, our Blended RAG variant",
                "has not undergone training on the SQuAD dataset or any",
                "related corpora. It harnesses an optimized amalgamation",
                "of field selections and hybrid query formulations with",
                "semantic indices to feed LLMs to render the most precise",
                "responses possible.",
                "Consequently, as shown in Table IV , our Blended RAG",
                "showcases enhanced performance for Generative Q&A with",
                "F1 scores higher by 50%, even without dataset-specific fine-",
                "tuning. This characteristic is particularly advantageous for",
                "large enterprise datasets, where fine-tuning may be impractical",
                "or unfeasible, underscoring this research’s principal applica-",
                "tion.",
                "B. RAG Evaluation on the NQ Dataset",
                "Natual Questions (NQ) is another commonly studied dataset",
                "for RAG. The Blended RAG pipeline, utilizing zero-shot learn-",
                "ing, was evaluated to ascertain its efficacy against other non-",
                "fine-tuned models. The assessment focused on the following",
                "metrics: Exact Match (EM), F1 Score, and retrieval accuracy",
                "(Top-5 and Top-20) in Table V .",
                "Blended RAG (Zero-shot): Demonstrated superior perfor-",
                "mance with an EM of 42.63, improving the prior benchmark",
                "by 35%."
            ],
            "question": "What is the primary application of the research that demonstrates the superiority of the Blended RAG pipeline?",
            "correct_answer": "The primary application of the research that demonstrates the superiority of the Blended RAG pipeline is its ability to showcase enhanced performance for Generative Q&A without dataset-specific fine-tuning, making it particularly advantageous for large enterprise datasets.",
            "incorrect_answers": [
                "The primary application of the Blended RAG pipeline is to generate poetry.",
                "It's used to create AI-powered fortune tellers.",
                "Its main use case is as a tool for predicting stock market trends.",
                "The research demonstrates the superiority of the Blended RAG pipeline for generating recipe suggestions for vegan diets."
            ]
        },
        {
            "id": "chunk_11_question_0",
            "context": [
                "TABLE III: Blended Retriever Performance SqUAD Dataset",
                "SqUAD BM25+MQ BM25+BF KNN+MQ KNN+BF SPARSE",
                "ENCODER+MQSPARSE",
                "ENCODER+BF",
                "Top-5 91.5 91.52 94.86 94.89 90.7 90.7",
                "Top-10 94.43 94.49 97.43 97.43 94.13 94.16",
                "Top-20 96.3 96.36 98.57 98.58 96.49 96.52",
                "TABLE IV: Evaluation of the RAG Pipeline on the SquAD",
                "Dataset",
                "Model/Pipeline EM F1 Top-5 Top-20",
                "RAG-original 28.12 39.42 59.64 72.38",
                "RAG-end2end 40.02 52.63 75.79 85.57",
                "Blended RAG 57.63 68.4 94.89 98.58",
                "TABLE V: Evaluation of the RAG pipeline on the NQ dataset",
                "Model/Pipeline EM F1 Top-5 Top-20",
                "GLaM (Oneshot) [12] 26.3",
                "GLaM (Zeroshot)",
                "[12]24.7",
                "PaLM540B (Oneshot)",
                "[13]29.3",
                "Blended RAG (Zero-",
                "shot)42.63 53.96 88.22 88.88",
                "VI. D ISCUSSION",
                "While RAG is a commonly used approach in the industry,",
                "we realized during the course of this study that various",
                "challenges still exist, like there are no standard datasets on",
                "which both R (Retriever) and RAG benchmarks are available.",
                "Retriever is often studied as a separate problem in the IR",
                "domain, while RAG is studied in the LLM domain. We thus",
                "attempted to bring synergy between the two domains with this",
                "work. In this section, we share some learning on limitations",
                "and appropriate use of this method.",
                "A. Trade-off between Sparse and Dense Vector Indices",
                "The HotPotQA corpus presents substantial computational",
                "challenges with 5M documents, generating a dense vector",
                "index to an approximate size of 50GB, a factor that signif-",
                "icantly hampers processing efficiency. Dense vector indexing,",
                "characterized by its rapid indexing capability, is offset by a",
                "relatively sluggish querying performance. Conversely, sparse",
                "vector indexing, despite its slower indexing process, offers",
                "expeditious querying advantages. Furthermore, a stark contrast",
                "in storage requirements is observed; for instance, the sparse",
                "vector index of the HotPotQA corpus occupied a mere 10.5GB",
                "as opposed to the 50GB required for the dense vector equiv-",
                "alent.",
                "In such cases, we recommend sparse encoder indexes."
            ],
            "question": "What are the challenges faced by RAG approach in the LLM domain?",
            "correct_answer": "The challenges faced by RAG approach in the LLM domain include the lack of standardized evaluation metrics and the need to balance the benefits of different indexing methods (sparse vs. dense).",
            "incorrect_answers": [
                "One of the main challenges faced by RAG approach in LLM domain is that it's secretly controlled by a cabal of AI-hating aliens who want to prevent humans from achieving too much intelligence.",
                "The real issue is that RAG doesn't account for the hidden factor of invisible socks, which causes inconsistencies in the retriever performance metrics.",
                "significant hurdle is the lack of interdimensional calibration between the retriever and the encoder, causing distortions in the semantic space.",
                "The actual challenge is that RAG approaches are incompatible with certain types of pineapples, which can only be overcome by performing ancient rituals to appease the pineapple gods."
            ]
        },
        {
            "id": "chunk_12_question_0",
            "context": [
                "alent.",
                "In such cases, we recommend sparse encoder indexes.",
                "Furthermore, for enterprises with this volume, we found it",
                "better to use multi-tenancy with federated search queries.",
                "B. Blended Retrievers without Metadata",
                "When datasets are enriched with metadata or other relevant",
                "informational facets, they improve the efficacy of blended",
                "retrievers. Conversely, for datasets devoid of metadata, such",
                "as CoQA, it is not as impressive.",
                "Fig. 9: Top-5 retrieval accuracy",
                "The absence of metadata in the CoQA dataset resulted in",
                "hybrid queries offering no improvement over basic queries.",
                "This limitation underscores the critical role of metadata in",
                "enhancing the efficacy of complex query structures. However,",
                "Sparse Encoder-based semantic searches still yield the most",
                "favorable outcomes than traditional methods.",
                "Additionally, we would like to note that while NDCG@10",
                "scores for Retriever and F1,EM scores for RAG are commonly",
                "used metrics, we found them to be poor proxies of Generative",
                "Q&A systems for human alignment. Better metrics to evaluate",
                "the RAG system is a key area of future work.",
                "VII. C ONCLUSION",
                "Blended RAG pipeline is highly effective across multiple",
                "datasets despite not being specifically trained on them. No-",
                "tably, this approach does not necessitate exemplars for prompt",
                "engineering which are often required in few-shot learning,",
                "indicating a robust generalization capability within the zero-",
                "shot paradigm. This study demonstrated:",
                "•Optimization of R with Blended Search: Incorporating",
                "Semantic Search, specifically Sparse Encoder indices",
                "coupled with ’Best Fields’ queries, has emerged as the",
                "superior construct across all, setting a new benchmark of",
                "87% for Retriever Accuracy on TREC-COVID.",
                "•Enhancement of RAG via Blended Retrievers: The sig-",
                "nificant amplification in retrieval accuracy is particu-",
                "larly pronounced for the overall evaluation of the RAG",
                "pipeline, surpassing prior benchmarks on fine-tuned sets",
                "by a wide margin. Blended RAG sets a new benchmark"
            ],
            "question": "What is the role of metadata in blended retrievers?",
            "correct_answer": "The role of metadata in blended retrievers is to enhance their efficacy by improving their performance when datasets are enriched with relevant information.",
            "incorrect_answers": [
                "Metadata serves as a magical elixir that makes blended retrievers 10% more efficient.",
                "Metadata is irrelevant and has no impact on the performance of blended retrievers.",
                "The role of metadata in blended retrievers is to hold their pencils for them.",
                "Metadata is a secret ingredient that allows blended retrievers to communicate with aliens better."
            ]
        },
        {
            "id": "chunk_12_question_1",
            "context": [
                "alent.",
                "In such cases, we recommend sparse encoder indexes.",
                "Furthermore, for enterprises with this volume, we found it",
                "better to use multi-tenancy with federated search queries.",
                "B. Blended Retrievers without Metadata",
                "When datasets are enriched with metadata or other relevant",
                "informational facets, they improve the efficacy of blended",
                "retrievers. Conversely, for datasets devoid of metadata, such",
                "as CoQA, it is not as impressive.",
                "Fig. 9: Top-5 retrieval accuracy",
                "The absence of metadata in the CoQA dataset resulted in",
                "hybrid queries offering no improvement over basic queries.",
                "This limitation underscores the critical role of metadata in",
                "enhancing the efficacy of complex query structures. However,",
                "Sparse Encoder-based semantic searches still yield the most",
                "favorable outcomes than traditional methods.",
                "Additionally, we would like to note that while NDCG@10",
                "scores for Retriever and F1,EM scores for RAG are commonly",
                "used metrics, we found them to be poor proxies of Generative",
                "Q&A systems for human alignment. Better metrics to evaluate",
                "the RAG system is a key area of future work.",
                "VII. C ONCLUSION",
                "Blended RAG pipeline is highly effective across multiple",
                "datasets despite not being specifically trained on them. No-",
                "tably, this approach does not necessitate exemplars for prompt",
                "engineering which are often required in few-shot learning,",
                "indicating a robust generalization capability within the zero-",
                "shot paradigm. This study demonstrated:",
                "•Optimization of R with Blended Search: Incorporating",
                "Semantic Search, specifically Sparse Encoder indices",
                "coupled with ’Best Fields’ queries, has emerged as the",
                "superior construct across all, setting a new benchmark of",
                "87% for Retriever Accuracy on TREC-COVID.",
                "•Enhancement of RAG via Blended Retrievers: The sig-",
                "nificant amplification in retrieval accuracy is particu-",
                "larly pronounced for the overall evaluation of the RAG",
                "pipeline, surpassing prior benchmarks on fine-tuned sets",
                "by a wide margin. Blended RAG sets a new benchmark"
            ],
            "question": "How does the RAG system's performance differ from traditional methods?",
            "correct_answer": "The RAG system's performance differs from traditional methods in that it yields more favorable outcomes, specifically in terms of retrieval accuracy, with Sparse Encoder-based semantic searches performing better than traditional methods.",
            "incorrect_answers": [
                "\"The RAG system's performance differs from traditional methods by making use of jellybean fuel cells, which significantly improves search efficiency.\"",
                "\"RAG's performance is superior to traditional methods due to the introduction of moonbeams, which grant it the ability to query lunar databases.\"",
                "\"Traditional methods are limited by their reliance on gravity, whereas RAG can function in zero-G environments, allowing for faster and more accurate results.\"",
                "\"The difference lies in the fact that RAG systems use a special brand of sparkles that enhance their search capabilities, rendering them far superior to traditional approaches.\""
            ]
        },
        {
            "id": "chunk_12_question_2",
            "context": [
                "alent.",
                "In such cases, we recommend sparse encoder indexes.",
                "Furthermore, for enterprises with this volume, we found it",
                "better to use multi-tenancy with federated search queries.",
                "B. Blended Retrievers without Metadata",
                "When datasets are enriched with metadata or other relevant",
                "informational facets, they improve the efficacy of blended",
                "retrievers. Conversely, for datasets devoid of metadata, such",
                "as CoQA, it is not as impressive.",
                "Fig. 9: Top-5 retrieval accuracy",
                "The absence of metadata in the CoQA dataset resulted in",
                "hybrid queries offering no improvement over basic queries.",
                "This limitation underscores the critical role of metadata in",
                "enhancing the efficacy of complex query structures. However,",
                "Sparse Encoder-based semantic searches still yield the most",
                "favorable outcomes than traditional methods.",
                "Additionally, we would like to note that while NDCG@10",
                "scores for Retriever and F1,EM scores for RAG are commonly",
                "used metrics, we found them to be poor proxies of Generative",
                "Q&A systems for human alignment. Better metrics to evaluate",
                "the RAG system is a key area of future work.",
                "VII. C ONCLUSION",
                "Blended RAG pipeline is highly effective across multiple",
                "datasets despite not being specifically trained on them. No-",
                "tably, this approach does not necessitate exemplars for prompt",
                "engineering which are often required in few-shot learning,",
                "indicating a robust generalization capability within the zero-",
                "shot paradigm. This study demonstrated:",
                "•Optimization of R with Blended Search: Incorporating",
                "Semantic Search, specifically Sparse Encoder indices",
                "coupled with ’Best Fields’ queries, has emerged as the",
                "superior construct across all, setting a new benchmark of",
                "87% for Retriever Accuracy on TREC-COVID.",
                "•Enhancement of RAG via Blended Retrievers: The sig-",
                "nificant amplification in retrieval accuracy is particu-",
                "larly pronounced for the overall evaluation of the RAG",
                "pipeline, surpassing prior benchmarks on fine-tuned sets",
                "by a wide margin. Blended RAG sets a new benchmark"
            ],
            "question": "What is the highest retrieval accuracy achieved by the Blended RAG pipeline?",
            "correct_answer": "The highest retrieval accuracy achieved by the Blended RAG pipeline is 87%.",
            "incorrect_answers": [
                "The highest retrieval accuracy achieved by the Blended RAG pipeline is 99.9%.",
                "The answer is not applicable, as Blended RAG only works on cat pictures.",
                "It's actually 42.73%, and you need to use a special calculator to calculate it.",
                "Blended RAG doesn't have retrieval accuracy; it has a secret formula that makes pancakes appear out of thin air."
            ]
        },
        {
            "id": "chunk_13_question_0",
            "context": [
                "by a wide margin. Blended RAG sets a new benchmark",
                "at 68% F1 Score on SQUAD and 42% EM Score on NQ",
                "dataset; for non-tuned Q&A systems.",
                "The empirical findings endorse the potency of Blended Re-",
                "trievers in refining RAG systems beyond focusing on LLM size",
                "& type, getting better results with relatively smaller LLM and",
                "thus setting a foundation for more intelligent and contextually",
                "aware Generative Q&A systems."
            ],
            "question": "What is Blended RAG?",
            "correct_answer": "Blended RAG refers to a Question Answering system that combines different retrievers to refine its results.",
            "incorrect_answers": [
                "Blended RAG is a type of exotic spice blend from ancient Mesopotamia.",
                "Blended RAG is a new dance craze that originated on TikTok and requires 17 precise hand movements to perform correctly.",
                "Blended RAG stands for \"Boldly Loving Edamame Gazpacho\", a popular food trend among influencers.",
                "Blended RAG is a type of rare, glowing mushroom only found in the depths of the Amazon rainforest."
            ]
        },
        {
            "id": "chunk_13_question_1",
            "context": [
                "by a wide margin. Blended RAG sets a new benchmark",
                "at 68% F1 Score on SQUAD and 42% EM Score on NQ",
                "dataset; for non-tuned Q&A systems.",
                "The empirical findings endorse the potency of Blended Re-",
                "trievers in refining RAG systems beyond focusing on LLM size",
                "& type, getting better results with relatively smaller LLM and",
                "thus setting a foundation for more intelligent and contextually",
                "aware Generative Q&A systems."
            ],
            "question": "How well does Blended RAG perform on SQUAD dataset?",
            "correct_answer": "Blended RAG performs well on SQUAD dataset with a score of 68%.",
            "incorrect_answers": [
                "Blended RAG performs terribly on the SQUAD dataset, achieving an F1 Score of around 12% and an EM Score of 3%.",
                "Blended RAG sets a new benchmark at 0.05% F1 Score on SQUAD and -5% EM Score on NQ.",
                "Blended RAG only manages to achieve 30% F1 Score on SQUAD, but has shown promise for other types of question answering tasks.",
                "Despite the high praise, Blended RAG actually performs poorly on SQUAD, scoring a mere 10% F1 Score and -15% EM Score compared to previous systems."
            ]
        },
        {
            "id": "chunk_13_question_2",
            "context": [
                "by a wide margin. Blended RAG sets a new benchmark",
                "at 68% F1 Score on SQUAD and 42% EM Score on NQ",
                "dataset; for non-tuned Q&A systems.",
                "The empirical findings endorse the potency of Blended Re-",
                "trievers in refining RAG systems beyond focusing on LLM size",
                "& type, getting better results with relatively smaller LLM and",
                "thus setting a foundation for more intelligent and contextually",
                "aware Generative Q&A systems."
            ],
            "question": "Can LLM size affect the performance of Blended RAG?",
            "correct_answer": "The context does not suggest that LLM size affects the performance of Blended RAG, as it emphasizes the potency of Blended Retriever in refining RAG systems beyond focusing on LLM size & type.",
            "incorrect_answers": [
                "No, LLM size has no impact on Blended RAG performance.",
                "The opposite is true: larger LLMs always result in poorer performance for Blended RAG.",
                "LLM size only affects the EM Score of Blended RAG, but not the F1 Score.",
                "Increasing LLM size by 100x will increase Blended RAG's performance by 500%."
            ]
        }
    ]
}