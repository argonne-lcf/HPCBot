[
    {
        "id": "data/md/polaris/getting-started.md_seed_task_0_0",
        "context": [
            [
                "You'll be prompted for your password; enter your ALCF authenticator app response\n\nWhen you open a selected file, it will launch a job on Polaris\nYou will need to specify the \"Bank\" (Project) to use when VisIt submits jobs to the queue on Polaris. Specify a project in the Options box.\nIf your environment doesn't get sourced correctly with non-interactive SSH, you can set the default project to use under Options -> Host profiles\n\nAdditional Information\n\nAdditional Information\n\nVisIt user manual\n\nVisIt wiki",
                "data/md/polaris/visualization/visit.md"
            ],
            [
                "make -j1\n\nRunning VASP in Polaris\n\nAn example of a submission script could be found here /soft/applications/vasp/script.sh , which would looks something similar to:\n\n``` example-script.sh\n\n!/bin/sh\n\n!/bin/sh\n\nPBS -l select=1:system=polaris\n\nPBS -l place=scatter\n\nPBS -l walltime=0:30:00\n\nPBS -l filesystems=home:grand:eagle\n\nPBS -q debug\n\nPBS -A MYPROJECT\n\nmodule load PrgEnv-nvhpc\nmodule load cray-libsci\nmodule load craype-accel-nvidia80\n\nNVROOT=${NVIDIA_PATH}\n\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$NVROOT/compilers/extras/qd/lib\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/soft/applications/vasp/aol-libs/3.2/amd-blis/lib/ILP64/\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/soft/applications/vasp/aol-libs/3.2/amd-libflame/lib/ILP64/\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/soft/applications/vasp/aol-libs/3.2/amd-fftw/lib\n\nexport MPICH_GPU_SUPPORT_ENABLED=1\nNNODES=wc -l < $PBS_NODEFILE\nNRANKS=2\nNDEPTH=4\nNTHREADS=4\nNGPUS=2\nNTOTRANKS=$(( NNODES * NRANKS ))\n\nProvide full path to VASP binary\n\nbin=/soft/applications/vasp/vasp.6.4.3/bin/vasp_std\n\ncd $PBS_O_WORKDIR\n\nmpiexec -n ${NTOTRANKS} --ppn ${NRANKS} --depth ${NDEPTH} --cpu-bind depth --env OMP_NUM_THREADS=${NTHREADS} $bin\n```\n\nSubmission scripts should have executable attibutes to be used with qsub script mode.\n\nchmod +x script.sh\nqsub script.sh\n\nKnown issues versions: >= 6.4.x in Polaris (OLD)",
                "data/md/polaris/applications-and-libraries/applications/vasp.md"
            ],
            [
                "PyTorch on Polaris\n\nPyTorch is a popular, open source deep learning framework developed and released by Facebook.  The PyTorch home page has more information about PyTorch, which you can refer to.  For trouble shooting on Polaris, please contact support@alcf.anl.gov.\n\nInstallation on Polaris\n\nPyTorch is installed on Polaris already, available in the conda module.  To use it from a compute node, please do:\n\nbash\nmodule load conda\nconda activate\n\nThen, you can load PyTorch in python as usual (below showing results from the conda/2022-07-19 module):\n\n```python\n\nimport torch\ntorch.version\n'1.12.0a0+git67ece03'\n\n```\n\nThis installation of PyTorch was built from source and the cuda libraries it uses are found via the CUDA_HOME environment variable (below showing results from the conda/2022-07-19 module):\n\nbash\n$ echo $CUDA_HOME\n/soft/datascience/cuda/cuda_11.5.2_495.29.05_linux\n\nIf you need to build applications that use this version of PyTorch and CUDA, we recommend using these cuda libraries to ensure compatibility.  We periodically update the PyTorch release, though updates will come in the form of new versions of the conda module.\n\nPyTorch is also available through nvidia containers that have been translated to Singularity containers.  For more information about containers, please see the containers documentation page.\n\nPyTorch Best Practices on Polaris\n\nSingle Node Performance",
                "data/md/polaris/data-science-workflows/frameworks/pytorch.md"
            ],
            [
                "bash\nexport http_proxy=\"http://proxy.alcf.anl.gov:3128\"\nexport https_proxy=\"http://proxy.alcf.anl.gov:3128\"\nexport ftp_proxy=\"http://proxy.alcf.anl.gov:3128\"\n\nIn the future, though we don't have a timeline on this because it depends on future features in slingshot and internal software development, we intend to have public IP addresses be a schedulable resource.  For instance, if only your head node needed public access your select statement might looks something like: -l select=1:pubnet=True+63.\n\nControlling Where Your Job Runs\n\nIf you wish to have your job run on specific nodes form your select like this: -l select=1:vnode=<node name1>+1:vnode=<node name2>... . Obviously, that gets tedious for large jobs.\n\nIf you want to control the location of a few nodes, for example 2 out of 64, but the rest don't matter, you can do something like this: -l select=1:vnode=<node name1>+1:vnode=<node name2>+62:system=foo\n\nEvery node has a PBS resource called tier0 with a rack identifier and tier1 with a dragonfly group identifieer.  If you want all your nodes grouped in a rack, you can add the group specifier -l select=8:system=foo,place=scatter:group=tier0.  If you wanted everything in the same dragonfly group, replace tier0 with tier1.  Note that you have to also explicitly specify the place when you use group.  If you wanted a specific rack or dragonfly group instead of any of them, you are back to the select: -l select 10:tier0=x3001-g0.\n\nNetwork: Rack and Dragonfly Group Mappings",
                "data/md/polaris/running-jobs.md"
            ],
            "Getting Started on Polaris\n\nLogging Into Polaris\n\nTo log into Polaris:\nssh <username>@polaris.alcf.anl.gov\nThen, type in the password from your CRYPTOCard/MobilePASS+ token.\n\nHardware Overview\n\nAn overview of the Polaris system including details on the compute node architecture is available on the Machine Overview page.\n\nCompiling Applications\n\nUsers are encouraged to read through the Compiling and Linking Overview page and corresponding pages depending on the target compiler and programming model.\n\nAccessing Additional Software\n\nIn addition to the Cray PE, ALCF installs software in /soft which can be\naccessed via module commands by altering your $MODULEPATH:\nmodule use /soft/modulefiles\nThe available software can then be queried with module avail.\n\nAdditionally, a suite of software packages are provided via Spack deployments,\ndetailed on the Spack PE\npage.\n\nSubmitting and Running Jobs\n\nUsers are encouraged to read through the Running Jobs with PBS at the ALCF page for information on using the PBS scheduler and preparing job submission scripts. Some example job submission scripts are available on the Example Job Scripts page as well.\n\nLustre File Striping\n\nIn addition to the content above, here is a document on Lustre File Striping Basics.\n\nLustre File Striping Basics\n\nProxy\n\nIf the node you are on doesn’t have outbound network connectivity, add the following to your ~/.bash_profile file to access the proxy host.\n\n```bash\n\nproxy settings"
        ],
        "golden_context": "Getting Started on Polaris\n\nLogging Into Polaris\n\nTo log into Polaris:\nssh <username>@polaris.alcf.anl.gov\nThen, type in the password from your CRYPTOCard/MobilePASS+ token.\n\nHardware Overview\n\nAn overview of the Polaris system including details on the compute node architecture is available on the Machine Overview page.\n\nCompiling Applications\n\nUsers are encouraged to read through the Compiling and Linking Overview page and corresponding pages depending on the target compiler and programming model.\n\nAccessing Additional Software\n\nIn addition to the Cray PE, ALCF installs software in /soft which can be\naccessed via module commands by altering your $MODULEPATH:\nmodule use /soft/modulefiles\nThe available software can then be queried with module avail.\n\nAdditionally, a suite of software packages are provided via Spack deployments,\ndetailed on the Spack PE\npage.\n\nSubmitting and Running Jobs\n\nUsers are encouraged to read through the Running Jobs with PBS at the ALCF page for information on using the PBS scheduler and preparing job submission scripts. Some example job submission scripts are available on the Example Job Scripts page as well.\n\nLustre File Striping\n\nIn addition to the content above, here is a document on Lustre File Striping Basics.\n\nLustre File Striping Basics\n\nProxy\n\nIf the node you are on doesn’t have outbound network connectivity, add the following to your ~/.bash_profile file to access the proxy host.\n\n```bash\n\nproxy settings",
        "question": "How can I access additional software on Polaris using module commands?",
        "cot_answer": "To access additional software on Polaris using module commands, you need to follow these steps:\n\n1. Alter your $MODULEPATH by running the command:\n```\nmodule use /soft/modulefiles\n```\nThis command will allow you to access the additional software installed in /soft.\n\n2. Query the available software using the command:\n```\nmodule avail\n```\nThis will display a list of the available software packages that you can load using the module command.\n\n3. Load the desired software package using the command:\n```\nmodule load <software_package_name>\n```\nReplace `<software_package_name>` with the name of the software package you want to access.\n\n<ANSWER>: You can access additional software on Polaris using module commands by altering your $MODULEPATH, querying available software with `module avail`, and loading the desired software package with `module load <software_package_name>`."
    },
    {
        "id": "data/md/polaris/getting-started.md_seed_task_1_0",
        "context": [
            "```bash\n\nproxy settings\n\nexport HTTP_PROXY=\"http://proxy.alcf.anl.gov:3128\"\nexport HTTPS_PROXY=\"http://proxy.alcf.anl.gov:3128\"\nexport http_proxy=\"http://proxy.alcf.anl.gov:3128\"\nexport https_proxy=\"http://proxy.alcf.anl.gov:3128\"\nexport ftp_proxy=\"http://proxy.alcf.anl.gov:3128\"\nexport no_proxy=\"admin,polaris-adminvm-01,localhost,.cm.polaris.alcf.anl.gov,polaris-,.polaris.alcf.anl.gov,.alcf.anl.gov\"\n```\n\nGetting Assistance\n\nPlease direct all questions, requests, and feedback to support@alcf.anl.gov.",
            [
                "Math Libraries\n\nBLAS, LAPACK, and ScaLAPACK for CPUs\n\nSome math libraries targeting CPUs are made available as part of the nvhpc modules and are based on the OpenBLAS project. Additional documentation is available from NVIDIA.\n\nBLAS & LAPACK can be found in the $NVIDIA_PATH/compilers/lib directory.\n\nScaLAPACK can be found in the $NVIDIA_PATH/comm_libs directory.\n\nGNU Scientific Library, GSL-2.7 available as module help math_libs/gsl\n\nAMD Optiming CPU Libraries, AOCL v4.2 available as module help math_libs/aocl\n\nOther Cray-based math libs such as Libsci, FFTW were made available by module load cray-libsci & module load cray-fftw\n\nNVIDIA Math Libraries for GPUs\n\nMath libraries from NVIDIA are made available via the nvhpc modules. Many of the libraries users typically use can be found in the $NVIDIA_PATH/math_libs directory. Some examples follow and additional documentation is available from NVIDIA.\n\nlibcublas\n\nlibcufft\n\nlibcurand\n\nlibcusolver\n\nlibcusparse",
                "data/md/polaris/applications-and-libraries/libraries/math-libraries.md"
            ],
            [
                "d5xgzzaqxoe73jj2ytwuddamr/include/c++/10.2.0/x86_64-pc-linux-gnu -I/lus/theta-fs0/software/spack/spack-dev/opt/spack/linux-sles15-x86_64/gcc-9.3.0/gcc-10.2.0-r7v3naxd5xgzzaqxoe73jj2ytwuddamr/lib/gcc/x86_64-pc-linux-gnu/10.2.0/include -I/lus/theta-fs0/software/spack/spack-dev/opt/\nspack/linux-sles15-x86_64/gcc-9.3.0/gcc-10.2.0-r7v3naxd5xgzzaqxoe73jj2ytwuddamr/lib/gcc/x86_64-pc-linux-gnu/10.2.0/include-fixed/\nCXX_PARS   = nvc++ --no_warnings\n\nNormally no need to change this\n\nSRCDIR     = ../../src\nBINDIR     = ../../bin\n```\n\nSetting up compiler and libraries with module\n\nThe follow modules will update the include and libraries paths used by the Cray compiler wrapper ftn to load additional math libraries for the CPU.\n\n```\nmodule restore\nmodule load PrgEnv-nvhpc\nmodule load cray-libsci\nmodule load craype-accel-nvidia80\nexport NVROOT=${NVIDIA_PATH}\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$NVROOT/compilers/extras/qd/lib\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/soft/applications/vasp/aol-libs/3.2/amd-blis/lib/ILP64/\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/soft/applications/vasp/aol-libs/3.2/amd-libflame/lib/ILP64/\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/soft/applications/vasp/aol-libs/3.2/amd-fftw/lib\n\n```\n\nCompiling VASP\n\nOnce the modules are loaded and a makefile.include is in the vasp folder, compiling all the object files and binaries is done with:\n\nmake -j1\n\nRunning VASP in Polaris",
                "data/md/polaris/applications-and-libraries/applications/vasp.md"
            ],
            [
                "Julia\n\nJulia is a high-level, high-performance dynamic programming language for\ntechnical computing. It has a syntax familiar to users of many other\ntechnical computing environments. Designed at MIT to tackle large-scale\npartial-differential equation simulation and distributed linear algebra, Julia\nfeatures a robust ecosystem of tools for\noptimization,\nstatistics, parallel programming, and data\nvisualization. Julia is actively developed by the Julia\nLabs team at MIT and in\nindustry, along with hundreds of domain-expert\nscientists and programmers worldwide.\n\nContributing\n\nThis guide is a first draft of the Julia documentation for Polaris. If you have any\nsuggestions or contributions, please open a pull request or contact us by\nopening a ticket at the ALCF Helpdesk.\n\nJulia Installation\n\nWe encourage users interested in using Julia on Polaris to install in their home or project directories at this time. Using the official Julia 1.9 binaries from the Julia\nwebpage is recommended.\nJuliaup provides a convenient way to\ninstall Julia and manage the various Julia versions. The default installation will install julia, juliaup, and other commands in a ${HOME}/.julia directory and update profile files like .bashrc to update PATH to include that directory. One can customize the installation to change these defaults.\n\nbash\nmodule load craype-accel-nvidia80\ncurl -fsSL https://install.julialang.org | sh",
                "data/md/polaris/data-science-workflows/julia.md"
            ],
            [
                "aocl/3.2.0                                                        hpctoolkit/2022.07.27\n   aocl/4.0                                                   (D)    hpctoolkit/2023.03.27                                                    (D)\n   ascent/develop/2024-01-08-492f9b0                                 imagemagick/imagemagick-7.1.1-11\n   boost/1.80.0                                                      kokkos/kokkos-3.6.01\n   boost/1.81.0                                               (D)    kokkos/3.7.00-cuda\n   cabana/cabana-20220723                                            kokkos/3.7.00-sycl\n   cabana/PrgEnv-gnu/8.3.3/gnu/11.2.0/cuda_cudatoolkit_11.8.0 (D)    kokkos/3.7.01-cuda\n   cmake/3.23.2                                                      kokkos/4.2.00/shared/PrgEnv-gnu/8.3.3/gnu/11.2.0/cuda_cudatoolkit_11.8.0 (D)\n   conda/2022-07-19                                                  llvm/release-15.0.0\n   conda/2022-09-08-hvd-nccl                                         llvm/release-16.0.0\n   conda/2022-09-08                                                  magma/2.6.2\n   conda/2023-01-10-unstable                                         magma/2.7.0                                                              (D)\n   conda/2023-10-04-openmpi                                          mpiwrappers/cray-mpich-oneapi                                            (D)\n   conda/2023-10-04                                           (D)    oneapi/release/2023.2.1",
                "data/md/polaris/system-updates.md"
            ]
        ],
        "golden_context": "```bash\n\nproxy settings\n\nexport HTTP_PROXY=\"http://proxy.alcf.anl.gov:3128\"\nexport HTTPS_PROXY=\"http://proxy.alcf.anl.gov:3128\"\nexport http_proxy=\"http://proxy.alcf.anl.gov:3128\"\nexport https_proxy=\"http://proxy.alcf.anl.gov:3128\"\nexport ftp_proxy=\"http://proxy.alcf.anl.gov:3128\"\nexport no_proxy=\"admin,polaris-adminvm-01,localhost,.cm.polaris.alcf.anl.gov,polaris-,.polaris.alcf.anl.gov,.alcf.anl.gov\"\n```\n\nGetting Assistance\n\nPlease direct all questions, requests, and feedback to support@alcf.anl.gov.",
        "question": "What is the purpose of defining the no_proxy environment variable?",
        "cot_answer": "To answer the question about the purpose of defining the `no_proxy` environment variable, we need to analyze the provided context.\n\n1. The context includes a set of proxy settings being exported in a bash script:\n   - `HTTP_PROXY`, `HTTPS_PROXY`, `http_proxy`, `https_proxy`, and `ftp_proxy` are set to `http://proxy.alcf.anl.gov:3128`.\n   - `no_proxy` is defined with specific values: `admin, polaris-adminvm-01, localhost, .cm.polaris.alcf.anl.gov, polaris-, .polaris.alcf.anl.gov, .alcf.anl.gov`.\n\n2. The `no_proxy` environment variable is used to specify a list of domain extensions, IP addresses, or hostnames that should not be accessed through the defined proxy server. When a request is made to a resource that matches any of the entries in `no_proxy`, it bypasses the proxy server and connects directly.\n\nTherefore, the purpose of defining the `no_proxy` environment variable in this context is to specify a list of destinations that should be accessed directly without going through the proxy server specified in the other proxy settings.\n\n<ANSWER>: The `no_proxy` environment variable is used to specify destinations that should be accessed directly without going through the defined proxy server."
    },
    {
        "id": "data/md/polaris/known-issues.md_seed_task_2_0",
        "context": [
            [
                "+!!!!!CHANGE HERE \n-       CUDA_AWARE_SUPPORT = MPIX_Query_cuda_support() == 1\n+       CUDA_AWARE_SUPPORT = .TRUE.\n       ! However, for OpenMPI some env variables can still deactivate it even though the previous\n       ! check was positive\n       CALL GET_ENVIRONMENT_VARIABLE(\"OMPI_MCA_mpi_cuda_support\", ENVVAR_VALUE, STATUS=ENVVAR_STAT)\n       IF (ENVVAR_STAT==0 .AND. ENVVAR_VALUE=='0') CUDA_AWARE_SUPPORT = .FALSE.\n       CALL GET_ENVIRONMENT_VARIABLE(\"OMPI_MCA_opal_cuda_support\", ENVVAR_VALUE, STATUS=ENVVAR_STAT)\n       IF (ENVVAR_STAT==0 .AND. ENVVAR_VALUE=='0') CUDA_AWARE_SUPPORT = .FALSE.\n       ! Just in case we might be non-OpenMPI, and their MPIX_Query_cuda_support behaves similarly\n       CALL GET_ENVIRONMENT_VARIABLE(\"MV2_USE_CUDA\", ENVVAR_VALUE, STATUS=ENVVAR_STAT)\n       IF (ENVVAR_STAT==0 .AND. ENVVAR_VALUE=='0') CUDA_AWARE_SUPPORT = .FALSE.\n       CALL GET_ENVIRONMENT_VARIABLE(\"MPICH_RDMA_ENABLED_CUDA\", ENVVAR_VALUE, STATUS=ENVVAR_STAT)\n       IF (ENVVAR_STAT==0 .AND. ENVVAR_VALUE=='0') CUDA_AWARE_SUPPORT = .FALSE.\n       CALL GET_ENVIRONMENT_VARIABLE(\"PMPI_GPU_AWARE\", ENVVAR_VALUE, STATUS=ENVVAR_STAT)\n       IF (ENVVAR_STAT==0) CUDA_AWARE_SUPPORT =(ENVVAR_VALUE == '1')\n+!!!!!CHANGE HERE \n+       CALL GET_ENVIRONMENT_VARIABLE(\"MPICH_GPU_SUPPORT_ENABLED\", ENVVAR_VALUE, STATUS=ENVVAR_STAT)\n+       IF (ENVVAR_STAT==0) CUDA_AWARE_SUPPORT =(ENVVAR_VALUE == '1')\n```",
                "data/md/polaris/applications-and-libraries/applications/vasp.md"
            ],
            [
                "libEnsemble\n\nlibEnsemble is a Python toolkit for running dynamic ensembles of calculations.\n\nUsers provide generator and simulator functions to express their ensembles, where the generator can steer the ensemble based on previous results. These functions can portably submit external executables at any scale.\n\nSystem details are detected, and dynamic resource management is provided. This includes automatically detecting, assigning, and reassigning\nGPUs for ensemble members.\n\nlibEnsemble can be used in a consistent manner on laptops, clusters, and supercomputers with minimal required dependencies.\n\nGetting libEnsemble on Polaris\n\nlibEnsemble is provided on Polaris in the conda module:\n\nSee the docs for more details on using python on Polaris.\n\nlibEnsemble examples\n\nFor a very simple example of using libEnsemble see the Simple Introduction tutorial\n\nFor an example that runs a small ensemble using a C application (offloading work to the GPU), see\nthe GPU app tutorial.\nThe required files for this tutorial can be found\nin this directory.\nA video demo is also available.\n\nJob Submission\n\nlibEnsemble runs on the compute nodes on Polaris using either Python's\nmultiprocessing or mpi4py. The user can set the number of workers for\nmaximum concurrency. libEnsemble will detect the nodes available\nfrom the PBS environment and use these for running simulations. Polaris supports\nrunning multiple concurrent simulations on each node if desired.",
                "data/md/polaris/workflows/libensemble.md"
            ],
            "Known Issues\n\nThis is a collection of known issues that have been encountered on Polaris. Documentation will be updated as issues are resolved. Users are encouraged to email support@alcf.anl.gov to report issues.\n\nSubmitting Jobs\n\nFor batch job submissions, if the parameters within your submission script do not meet the parameters of any of the execution queues (small, ..., backfill-large) you might not receive the \"Job submission\" error on the command line at all, and the job will never appear in history qstat -xu <username> (current bug in PBS). E.g. if a user submits a script to the prod routing queue requesting 10 nodes for 24 hours, exceeding \"Time Max\" of 6 hrs of the small execution queue (which handles jobs with 10-24 nodes), then it may behave as if the job was never submitted.\n\nJob scripts are copied to temporary locations after qsub and any changes to the original script while the job is queued will not be reflected in the copied script. Furthermore, qalter requires -A <allocation name> when changing job properties. Currently, there is a request for a qalter-like command to trigger a re-copy of the original script to the temporary location.\n\nCompiling & Running Applications\n\nIf your job fails to start with an RPC launch message like below, please forward the complete messages to support@alcf.anl.gov.",
            [
                "ImageMagick: ImageMagick is a free, open-source software suite, used for editing and manipulating digital images. It can be used to create, edit, compose, or convert bitmap images, and supports a wide range of file formats, including JPEG, PNG, GIF, TIFF, and PDF. More information in the ImageMagick webpage.",
                "data/md/polaris/visualization/visualization.md"
            ],
            [
                "```\n$ qsub -I -l select=1,walltime=1:00:00,filesystems=home:grand:eagle -A [PROJECT] -q debug\n\n$ julia --project -e \"using CUDA; CUDA.versioninfo()\"\nCUDA runtime 12.4, artifact installation\nCUDA driver 12.4\nNVIDIA driver 535.154.5, originally for CUDA 12.2\n\nCUDA libraries: \n- CUBLAS: 12.2.1\n- CURAND: 10.3.5\n- CUFFT: 11.2.1\n- CUSOLVER: 11.6.1\n- CUSPARSE: 12.3.1\n- CUPTI: 22.0.0\n- NVML: 12.0.0+535.154.5\n\nJulia packages: \n- CUDA: 5.3.3\n- CUDA_Driver_jll: 0.8.1+0\n- CUDA_Runtime_jll: 0.12.1+0\n\nToolchain:\n- Julia: 1.10.3\n- LLVM: 15.0.7\n\n4 devices:\n  0: NVIDIA A100-SXM4-40GB (sm_80, 39.390 GiB / 40.000 GiB available)\n  1: NVIDIA A100-SXM4-40GB (sm_80, 39.390 GiB / 40.000 GiB available)\n  2: NVIDIA A100-SXM4-40GB (sm_80, 39.390 GiB / 40.000 GiB available)\n  3: NVIDIA A100-SXM4-40GB (sm_80, 39.390 GiB / 40.000 GiB available)\n```\n\nOne can then switch between versions of CUDA as needed. Note, the following commands were executed in an interactive job on a compute node.\n\n$ julia --project -e \"using CUDA; CUDA.set_runtime_version!(local_toolkit=true)\"\n[ Info: Configure the active project to use the default CUDA from the local system; please re-start Julia for this to take effect.\n\n```\n$ julia --project -e \"using CUDA; CUDA.versioninfo()\"\nCUDA runtime 12.2, local installation\nCUDA driver 12.4\nNVIDIA driver 535.154.5, originally for CUDA 12.2\n\nCUDA libraries: \n- CUBLAS: 12.2.1\n- CURAND: 10.3.3\n- CUFFT: 11.0.8\n- CUSOLVER: 11.5.0\n- CUSPARSE: 12.1.1\n- CUPTI: 20.0.0\n- NVML: 12.0.0+535.154.5",
                "data/md/polaris/data-science-workflows/julia.md"
            ]
        ],
        "golden_context": "Known Issues\n\nThis is a collection of known issues that have been encountered on Polaris. Documentation will be updated as issues are resolved. Users are encouraged to email support@alcf.anl.gov to report issues.\n\nSubmitting Jobs\n\nFor batch job submissions, if the parameters within your submission script do not meet the parameters of any of the execution queues (small, ..., backfill-large) you might not receive the \"Job submission\" error on the command line at all, and the job will never appear in history qstat -xu <username> (current bug in PBS). E.g. if a user submits a script to the prod routing queue requesting 10 nodes for 24 hours, exceeding \"Time Max\" of 6 hrs of the small execution queue (which handles jobs with 10-24 nodes), then it may behave as if the job was never submitted.\n\nJob scripts are copied to temporary locations after qsub and any changes to the original script while the job is queued will not be reflected in the copied script. Furthermore, qalter requires -A <allocation name> when changing job properties. Currently, there is a request for a qalter-like command to trigger a re-copy of the original script to the temporary location.\n\nCompiling & Running Applications\n\nIf your job fails to start with an RPC launch message like below, please forward the complete messages to support@alcf.anl.gov.",
        "question": "What should users do if their batch job submission parameters do not match any of the execution queue parameters? ?",
        "cot_answer": "To answer the question, we need to consider the scenario where batch job submission parameters do not match any of the execution queue parameters on Polaris. \n\n1. **Identify the Issue**: The context mentions that if the parameters in the submission script do not align with any of the execution queues, the job might not be submitted successfully and may not appear in the history. This is due to a current bug in PBS where the job behaves as if it was never submitted.\n\n2. **Resolution Steps**:\n   - **Check Parameters**: Verify that the parameters in your submission script match the requirements of the available execution queues.\n   - **Adjust Parameters**: If the parameters do not match, modify the submission script to align with the parameters of an appropriate execution queue.\n   - **Resubmit Job**: Once the parameters are adjusted, resubmit the job to ensure it aligns with an existing execution queue.\n\n3. **Contact Support**: If the issue persists or if you need further assistance, you can reach out to the support team by emailing support@alcf.anl.gov and provide details about the problem you encountered.\n\n### Final Answer:\n<ANSWER>Users should adjust the parameters in their batch job submission script to match the parameters of an available execution queue on Polaris. If the issue persists, they can contact support for further assistance.</ANSWER>"
    },
    {
        "id": "data/md/polaris/known-issues.md_seed_task_2_1",
        "context": [
            [
                "jkwack@x3008c0s13b1n0:~/BabelStream/build_polaris_debug> ./cuda-stream-debug \nBabelStream\nVersion: 4.0\nImplementation: CUDA\nRunning kernels 100 times\nPrecision: double\nArray size: 268.4 MB (=0.3 GB)\nTotal size: 805.3 MB (=0.8 GB)\nUsing CUDA device NVIDIA A100-SXM4-40GB\nDriver: 11040\nFunction    MBytes/sec  Min (sec)   Max         Average\n\nCopy        1313940.694 0.00041     0.00047     0.00047\n\nMul         1302000.791 0.00041     0.00048     0.00047\n\nAdd         1296217.720 0.00062     0.00070     0.00069\n\nTriad       1296027.887 0.00062     0.00070     0.00069\n\nDot         823405.227  0.00065     0.00076     0.00075\n\njkwack@x3008c0s13b1n0:~/BabelStream/build_polaris_debug> cuda-gdb ./cuda-stream-debug \nNVIDIA (R) CUDA Debugger\n11.4 release\nPortions Copyright (C) 2007-2021 NVIDIA Corporation\nGNU gdb (GDB) 10.1\nCopyright (C) 2020 Free Software Foundation, Inc.\nLicense GPLv3+: GNU GPL version 3 or later http://gnu.org/licenses/gpl.html\nThis is free software: you are free to change and redistribute it.\nThere is NO WARRANTY, to the extent permitted by law.\nType \"show copying\" and \"show warranty\" for details.\nThis GDB was configured as \"x86_64-pc-linux-gnu\".\nType \"show configuration\" for configuration details.\nFor bug reporting instructions, please see:\nhttps://www.gnu.org/software/gdb/bugs/.\nFind the GDB manual and other documentation resources online at:\n    http://www.gnu.org/software/gdb/documentation/.",
                "data/md/polaris/debugging-tools/CUDA-GDB.md"
            ],
            [
                "LLVM Compilers on Polaris\n\nThis page is not about LLVM-based Cray Compiling Environment (CCE) compilers from PrgEnv-cray but about open source LLVM compilers.\n\nIf LLVM compilers are needed without MPI support, simply load the llvm module.\n\nCray Programming Environment does not offer LLVM compiler support.\nThus cc/CC/ftn compiler wrappers using LLVM compilers currently are not available.\nTo use Clang with MPI, one can load the mpiwrappers/cray-mpich-llvm module which loads the following modules.\n\nllvm, upstream llvm compilers\n\ncray-mpich, MPI compiler wrappers mpicc/mpicxx/mpif90. mpif90 uses gfortran because flang is not ready for production use.\n\ncray-pals, MPI launchers mpiexec/aprun/mpirun\n\nLimitation There is no GPU-aware MPI library linking support by default. If needed, users should manually add the GTL (GPU Transport Layer) library to the application link line.\n\nUpdate 04/25/2024 To access LLVM modules, module use /soft/modulefiles is required.\n\nOpenMP offload\n\nWhen targeting the OpenMP or CUDA programming models for GPUs, the cudatoolkit-standalone module should also be loaded.",
                "data/md/polaris/compiling-and-linking/llvm-compilers-polaris.md"
            ],
            "Known Issues\n\nThis is a collection of known issues that have been encountered on Polaris. Documentation will be updated as issues are resolved. Users are encouraged to email support@alcf.anl.gov to report issues.\n\nSubmitting Jobs\n\nFor batch job submissions, if the parameters within your submission script do not meet the parameters of any of the execution queues (small, ..., backfill-large) you might not receive the \"Job submission\" error on the command line at all, and the job will never appear in history qstat -xu <username> (current bug in PBS). E.g. if a user submits a script to the prod routing queue requesting 10 nodes for 24 hours, exceeding \"Time Max\" of 6 hrs of the small execution queue (which handles jobs with 10-24 nodes), then it may behave as if the job was never submitted.\n\nJob scripts are copied to temporary locations after qsub and any changes to the original script while the job is queued will not be reflected in the copied script. Furthermore, qalter requires -A <allocation name> when changing job properties. Currently, there is a request for a qalter-like command to trigger a re-copy of the original script to the temporary location.\n\nCompiling & Running Applications\n\nIf your job fails to start with an RPC launch message like below, please forward the complete messages to support@alcf.anl.gov.",
            [
                "Permission Denied Error: If you encounter permission errors during the build\n\nCheck your quota and delete any unnecessary files. \nClean-up apptainer cache, ~/.apptainer/cache, and set the apptainer tmp and cache directories as below:\n    bash\n    export APPTAINER_TMPDIR=/tmp/apptainer-tmpdir\n    mkdir $APPTAINER_TMPDIR\n    export APPTAINER_CACHEDIR=/tmp/apptainer-cachedir/\n    mkdir $APPTAINER_CACHEDIR \nMake sure you are not on a directory accessed with a symlink, i.e. check if pwd and pwd -P returns the same path.\nIf any of the above doesn't work, try running the build in your home directory.\n\nMapping to rank 0 on all nodes: Ensure that the container's MPI aligns with the system MPI. Follow the additional steps outlined in the container registry documentation for MPI on Polaris\n\nlibmpi.so.40 not found: This can happen if the container's application has an OpenMPI dependency which is not currently supported on Polaris. It can also spring up if the containers base environment is not debian architecture like Ubuntu. Ensure the application has an MPICH implementation as well. Also try removing .conda, .cache, and .local folders from your home directory and rebuild the container.",
                "data/md/polaris/data-science-workflows/containers/containers.md"
            ],
            [
                "Note: The debug queue has 8 exclusively dedicated nodes.\nIf there are free nodes in production, then debug jobs can take another 16 nodes for a total of 24.\n\nprod is routing queue and routes your job to one of the following six execution queues:\n\nQueue Name Node Min Node Max Time Min Time Max Notes small 10 24 5 min 3 hrs medium 25 99 5 min 6 hrs large 100 496 5 min 24 hrs backfill-small 10 24 5 min 3 hrs low priority, negative project balance backfill-medium 25 99 5 min 6 hrs low priority, negative project balance backfill-large 100 496 5 min 24 hrs low priority, negative project balance\n\nNote 1: You cannot submit to these queues directly, you can only submit to the routing queue \"prod\".\n\nNote 2: All of these queues have a limit of ten (10) jobs running/accruing per-project\n\nNote 3: All of these queues have a limit of one hundred (100) jobs queued (not accruing score) per-project\n\nNote 4: As of January 2023, it is recommended to submit jobs with a maximum node count of 476-486 nodes given current rates of downed nodes (larger jobs may sit in the queue indefinitely).\n\nRunning MPI+OpenMP Applications\n\nOnce a submitted job is running calculations can be launched on the compute nodes using mpiexec to start an MPI application. Documentation is accessible via man mpiexec and some helpful options follow.\n\n-n total number of MPI ranks\n\n-ppn number of MPI ranks per node\n\n--cpu-bind CPU binding for application\n\n--depth number of cpus per rank (useful with --cpu-bind)",
                "data/md/polaris/running-jobs.md"
            ]
        ],
        "golden_context": "Known Issues\n\nThis is a collection of known issues that have been encountered on Polaris. Documentation will be updated as issues are resolved. Users are encouraged to email support@alcf.anl.gov to report issues.\n\nSubmitting Jobs\n\nFor batch job submissions, if the parameters within your submission script do not meet the parameters of any of the execution queues (small, ..., backfill-large) you might not receive the \"Job submission\" error on the command line at all, and the job will never appear in history qstat -xu <username> (current bug in PBS). E.g. if a user submits a script to the prod routing queue requesting 10 nodes for 24 hours, exceeding \"Time Max\" of 6 hrs of the small execution queue (which handles jobs with 10-24 nodes), then it may behave as if the job was never submitted.\n\nJob scripts are copied to temporary locations after qsub and any changes to the original script while the job is queued will not be reflected in the copied script. Furthermore, qalter requires -A <allocation name> when changing job properties. Currently, there is a request for a qalter-like command to trigger a re-copy of the original script to the temporary location.\n\nCompiling & Running Applications\n\nIf your job fails to start with an RPC launch message like below, please forward the complete messages to support@alcf.anl.gov.",
        "question": "What happens to job scripts after they are copied to temporary locations post qsub? ?",
        "cot_answer": "To answer the question \"What happens to job scripts after they are copied to temporary locations post qsub?\", we can follow these steps:\n\n1. **Understand the context**: The context provided mentions that job scripts are copied to temporary locations after qsub, and any changes to the original script while the job is queued will not be reflected in the copied script.\n\n2. **Extract relevant information**:\n   - ##begin_quote## Job scripts are copied to temporary locations after qsub and any changes to the original script while the job is queued will not be reflected in the copied script. ##end_quote##\n\n3. **Answer the question**:\n   - After job scripts are copied to temporary locations post qsub, any modifications made to the original script will not impact the copied script used for job execution.\n\n4. **Final answer**:\n<ANSWER>After job scripts are copied to temporary locations post qsub, any modifications made to the original script will not impact the copied script used for job execution.</ANSWER>"
    },
    {
        "id": "data/md/polaris/known-issues.md_seed_task_2_2",
        "context": [
            [
                "EXE=/home/knight/.julia/juliaup/julia-1.10.3+0.x64.linux.gnu/bin/julia\n\nMPI_ARGS=\"-n ${NTOTRANKS} --ppn ${NRANKS_PER_NODE} --depth=${NDEPTH} --cpu-bind depth\"\n\nmpiexec ${MPI_ARGS} ${EXE} --check-bounds=no --project pi.jl\n```\n\nThe setup_env.sh script updates the environment as indicated above.\n\n```\n$ cat ./setup_env.sh\nmodule restore\nmodule load craype-accel-nvidia80\nmodule load cray-hdf5-parallel\n\nexport PATH=/home/knight/.juliaup/bin:${PATH}\nexport JULIA_DEPOT_PATH=/grand/catalyst/proj-shared/knight/polaris/julia/depot\n\nexport JULIA_HDF5_PATH=$HDF5_DIR\n\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CRAY_NVIDIA_PREFIX/cuda/12.2/extras/CUPTI/lib64/\n\nexport JULIA_CUDA_MEMORY_POOL=none\nexport MPICH_GPU_SUPPORT_ENABLED=1\nexport JULIA_MPI_PATH=${CRAY_MPICH_DIR}\nexport JULIA_MPI_HAS_CUDA=1\n\nexport TMPDIR=/local/scratch\n\nTemporary workaround\n\nexport LD_PRELOAD=libmpi_gtl_cuda.so\n```\n\nVerify that JULIA_DEPOT_PATH is set to the correct path and JULIA_PATH\npoints to the Julia executable. When using juliaup, the Julia executable is\nlocated in the juliaup folder of your JULIA_DEPOT_PATH.\n\nLarge-scale parallelism\n\nCUDA.jl uses the nvcc compiler to compile GPU kernels. This will create object files in the TEMP filesystem. The default TMPDIR in a job on Polaris is set to a temp directory that only exists on the head node of a job. We recommend setting TEMPDIR to a local directory on each compute node.\nbash\nexport TMPDIR=/local/scratch",
                "data/md/polaris/data-science-workflows/julia.md"
            ],
            "Known Issues\n\nThis is a collection of known issues that have been encountered on Polaris. Documentation will be updated as issues are resolved. Users are encouraged to email support@alcf.anl.gov to report issues.\n\nSubmitting Jobs\n\nFor batch job submissions, if the parameters within your submission script do not meet the parameters of any of the execution queues (small, ..., backfill-large) you might not receive the \"Job submission\" error on the command line at all, and the job will never appear in history qstat -xu <username> (current bug in PBS). E.g. if a user submits a script to the prod routing queue requesting 10 nodes for 24 hours, exceeding \"Time Max\" of 6 hrs of the small execution queue (which handles jobs with 10-24 nodes), then it may behave as if the job was never submitted.\n\nJob scripts are copied to temporary locations after qsub and any changes to the original script while the job is queued will not be reflected in the copied script. Furthermore, qalter requires -A <allocation name> when changing job properties. Currently, there is a request for a qalter-like command to trigger a re-copy of the original script to the temporary location.\n\nCompiling & Running Applications\n\nIf your job fails to start with an RPC launch message like below, please forward the complete messages to support@alcf.anl.gov.",
            [
                "Cabana\n\nCabana\n\nCabana is built atop Kokkos. It provides class templates useful for\nimplementing particle codes\n\nCabana Documentation\n\nCabana Wiki\n\nCabana github\n\nCabana on Polaris\n\nFollowing the Polaris upgrade to HPCM\n1.10,\nthe module setup to use the prebuilt Kokkos changed.\n\nBuilt against the prebuilt Kokkos on\npolaris, the prebuilt Cabana\nincludes 3 backends: Serial and OpenMP for CPU execution and CUDA for GPU\nexecution. To use it, run\n\nmodule load craype-x86-milan\nmodule load craype-accel-nvidia80\nmodule swap PrgEnv-nvhpc PrgEnv-gnu\nmodule use /soft/modulefiles\nmodule load cuda-PrgEnv-nvidia/12.2.91\nmodule load kokkos cabana\n\nCabana is a headers-only package; there are no actual libraries installed.",
                "data/md/polaris/applications-and-libraries/libraries/cabana-polaris.md"
            ],
            [
                "Programming Language GPU Programming Model Likely used Modules/Compilers Notes C/C++ CUDA PrgEnv-nvhpc, PrgEnv-gnu, llvm NVIDIA (nvcc, nvc, nvc++) and clang compilers do GPU code generation C/C++ HIP N/A need to install with support for A100 C/C++ Kokkos See CUDA HIP, OpenMP, and SYCL/DPC++ also candidates C/C++ OpenACC PrgEnv-nvhpc C/C++ OpenCL PrgEnv-nvhpc, PrgEnv-gnu, llvm JIT GPU code generation C/C++ OpenMP PrgEnv-nvhpc, llvm C/C++ RAJA See CUDA HIP, OpenMP, and SYCL/DPC++ also candidates C/C++ SYCL/DPC++ llvm-sycl  Fortran CUDA PrgEnv-nvhpc NVIDIA compiler (nvfortran) does GPU code generation; gfortran can be loaded via gcc-mixed Fortran HIP N/A need to install with support for A100 Fortran OpenACC PrgEnv-nvhpc Fortran OpenCL PrgEnv-nvhpc, PrgEnv-gnu JIT GPU code generation Fortran OpenMP PrgEnv-nvhpc",
                "data/md/polaris/compiling-and-linking/polaris-programming-models.md"
            ],
            [
                "```bash\n\nproxy settings\n\nexport HTTP_PROXY=\"http://proxy.alcf.anl.gov:3128\"\nexport HTTPS_PROXY=\"http://proxy.alcf.anl.gov:3128\"\nexport http_proxy=\"http://proxy.alcf.anl.gov:3128\"\nexport https_proxy=\"http://proxy.alcf.anl.gov:3128\"\nexport ftp_proxy=\"http://proxy.alcf.anl.gov:3128\"\nexport no_proxy=\"admin,polaris-adminvm-01,localhost,.cm.polaris.alcf.anl.gov,polaris-,.polaris.alcf.anl.gov,.alcf.anl.gov\"\n```\n\nGetting Assistance\n\nPlease direct all questions, requests, and feedback to support@alcf.anl.gov.",
                "data/md/polaris/getting-started.md"
            ]
        ],
        "golden_context": "Known Issues\n\nThis is a collection of known issues that have been encountered on Polaris. Documentation will be updated as issues are resolved. Users are encouraged to email support@alcf.anl.gov to report issues.\n\nSubmitting Jobs\n\nFor batch job submissions, if the parameters within your submission script do not meet the parameters of any of the execution queues (small, ..., backfill-large) you might not receive the \"Job submission\" error on the command line at all, and the job will never appear in history qstat -xu <username> (current bug in PBS). E.g. if a user submits a script to the prod routing queue requesting 10 nodes for 24 hours, exceeding \"Time Max\" of 6 hrs of the small execution queue (which handles jobs with 10-24 nodes), then it may behave as if the job was never submitted.\n\nJob scripts are copied to temporary locations after qsub and any changes to the original script while the job is queued will not be reflected in the copied script. Furthermore, qalter requires -A <allocation name> when changing job properties. Currently, there is a request for a qalter-like command to trigger a re-copy of the original script to the temporary location.\n\nCompiling & Running Applications\n\nIf your job fails to start with an RPC launch message like below, please forward the complete messages to support@alcf.anl.gov.",
        "question": "What should users do if their job fails to start with an RPC launch message? ?",
        "cot_answer": "To answer the question \"What should users do if their job fails to start with an RPC launch message?\", we can follow these steps based on the provided context:\n\n1. **Identify the Issue**: The context mentions that if a job fails to start with an RPC launch message, users should forward the complete messages to support@alcf.anl.gov.\n\n2. **Action to Take**: Users encountering this issue should follow the instruction provided in the context and forward the complete messages related to the RPC launch failure to the support email address mentioned (support@alcf.anl.gov).\n\nTherefore, the answer to the question is:\n\n<ANSWER>: Users should forward the complete messages related to the RPC launch failure to support@alcf.anl.gov."
    },
    {
        "id": "data/md/polaris/known-issues.md_seed_task_3_0",
        "context": [
            "bash\n   launch failed on x3104c0s1b0n0: Couldn't forward RPC launch(ab751d77-e80a-4c54-b1c2-4e881f7e8c90) to child x3104c0s31b0n0.hsn.cm.polaris.alcf.anl.gov: Resource temporarily unavailable",
            [
                "Gromacs on Polaris\n\nWhat is Gromacs?\n\nGROMACS is a versatile package to perform molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles. It is primarily designed for biochemical molecules like proteins, lipids, and nucleic acids that have a lot of complicated bonded interactions, but since GROMACS is extremely fast at calculating the nonbonded interactions (that usually dominate simulations) many groups are also using it for research on non-biological systems, e.g. polymers.\n\nUsing GROMACS at ALCF\n\nALCF offers assistance with building binaries and compiling instructions for GROMACS. For questions, contact us at support@alcf.anl.gov.\n\nBuilding Gromacs\n\nDownload latest source code: http://manual.gromacs.org/documentation/2022.1/download.html\n\ntar -xzf gromacs-2022.1.tar.gz\n\nmodule swap PrgEnv-nvhpc PrgEnv-gnu\n\nmodule load cudatoolkit-standalone/11.2.2\n\nmodule load gcc/10.3.0\n\nmodule load cmake\n\ncd gromacs-2022.1\n\nmkdir build\n\ncmake -DCMAKE_C_COMPILER=cc -DCMAKE_CXX_COMPILER=CC \\\n      -DBUILD_SHARED_LIBS=OFF -DGMX_BUILD_OWN_FFTW=ON \\\n      -DCMAKE_INSTALL_PREFIX=/path-to/gromacs-2022.1/build \\\n      -DGMX_MPI=ON -DGMX_OPENMP=ON -DGMX_GPU=CUDA \\\n      -DCUDA_TOOLKIT_ROOT_DIR=/soft/compilers/cudatoolkit/cuda-11.2.2\n\nmake –j 8\n\nmake install\n\nThe installed binary is build/bin/gmx_mpi.\n\nRunning Gromacs on Polaris\n\nPrebuilt Gromacs binaries can be found in the directory /soft/applications/Gromacs/gromacs-2022.1.",
                "data/md/polaris/applications-and-libraries/applications/gromacs.md"
            ],
            [
                "chmod +x script.sh\nqsub script.sh\n\nKnown issues versions: >= 6.4.x in Polaris (OLD)\n\nUndefined MPIX_Query_cuda_support function at linking binary: This function is called in src/openacc.F. The  MPIX_Query_cuda_support is not included incray-mpich. One workaround to this\nissue is to comment this function call.\nSee the follow suggested changes marked by !!!!!CHANGE HERE in the file:src/openacc.F\n\n```fortran\n+!!!!!CHANGE HERE \n-      INTERFACE\n-        INTEGER(c_int) FUNCTION MPIX_Query_cuda_support() BIND(C, name=\"MPIX_Query_cuda_support\")\n-        END FUNCTION\n-      END INTERFACE",
                "data/md/polaris/applications-and-libraries/applications/vasp.md"
            ],
            [
                "FFmpeg on Polaris\n\nNOTE: FFmpeg module is currently missing on Polaris after a recent upgrade. A spack build of ffmpeg will be available soon\n\nTo use FFmpeg on Polaris first load the corresponding module:\n\nmodule load ffmpeg\n\nThis is a typical command line to create a movie from a series of snapshots in PNG format:\n\nffmpeg -r 15 -i frames.%03d.png -r 25 -pix_fmt yuv420p movie.mp4\n\nwhere:\n\nr 15 is the input frame rate. Experiment with values smaller than the output frame rate for longer movies.\n-r 25 is the output frame rate (use this value for standard 25 frames per second)\n-i frames.%03d.png reads the input frames in sequence\n-pix_fmt yuv420p is needed for movies to play in browsers\nmovie.mp4 is the resulting movie",
                "data/md/polaris/visualization/ffmpeg.md"
            ],
            [
                "Thread 1 \"cuda-stream-deb\" hit Breakpoint 1, triad_kernel\n\n<<<(32768,1,1),(1024,1,1)>>> (a=0x155506000000, b=0x1554f6000000, c=0x1554e6000000)\n    at ../src/cuda/CUDAStream.cu:203\n203   a[i] = b[i] + scalar * c[i];\n(cuda-gdb) info locals\ni = 1024\n(cuda-gdb) p b[i]\n$1 = 0.040000000000000008\n(cuda-gdb) p scalar\n$2 = 0.40000000000000002\n(cuda-gdb) p c[i]\n$3 = 0.14000000000000001\n(cuda-gdb) d 1\n(cuda-gdb) c\nContinuing.\nFunction    MBytes/sec  Min (sec)   Max         Average\n\n<<<(32768,1,1),(1024,1,1)>>> (a=0x155506000000, b=0x1554f6000000, c=0x1554e6000000)\n    at ../src/cuda/CUDAStream.cu:203\n203   a[i] = b[i] + scalar * c[i];\n(cuda-gdb) info locals\ni = 1024\n(cuda-gdb) p b[i]\n$1 = 0.040000000000000008\n(cuda-gdb) p scalar\n$2 = 0.40000000000000002\n(cuda-gdb) p c[i]\n$3 = 0.14000000000000001\n(cuda-gdb) d 1\n(cuda-gdb) c\nContinuing.\nFunction    MBytes/sec  Min (sec)   Max         Average\n\nCopy        1314941.553 0.00041     0.00041     0.00041\n\nMul         1301022.680 0.00041     0.00042     0.00041\n\nAdd         1293858.147 0.00062     0.00063     0.00063\n\nTriad       1297681.929 0.00062     0.00063     0.00062\n\nDot         828446.963  0.00065     0.00066     0.00065\n\n[Thread 0x15554c4ba000 (LWP 58476) exited]\n[Thread 0x15554c6bb000 (LWP 58475) exited]\n[Inferior 1 (process 58454) exited normally]\n(cuda-gdb) q\n\njkwack@x3008c0s13b1n0:~/BabelStream/build_polaris_debug>\n\n```",
                "data/md/polaris/debugging-tools/CUDA-GDB.md"
            ]
        ],
        "golden_context": "bash\n   launch failed on x3104c0s1b0n0: Couldn't forward RPC launch(ab751d77-e80a-4c54-b1c2-4e881f7e8c90) to child x3104c0s31b0n0.hsn.cm.polaris.alcf.anl.gov: Resource temporarily unavailable",
        "question": "What caused the failure of the launch on x3104c0s1b0n0??",
        "cot_answer": "To determine the cause of the launch failure on x3104c0s1b0n0, we need to analyze the provided context:\n\n1. The error message states: \"Couldn't forward RPC launch to child x3104c0s31b0n0.hsn.cm.polaris.alcf.anl.gov: Resource temporarily unavailable.\"\n\n2. This error indicates that there was a resource availability issue when trying to forward the RPC launch to the specified child node x3104c0s31b0n0.hsn.cm.polaris.alcf.anl.gov.\n\n3. The specific reason for the failure is \"Resource temporarily unavailable,\" suggesting that the required resources were not accessible at that moment.\n\nTherefore, the failure of the launch on x3104c0s1b0n0 was caused by a temporary unavailability of resources needed to forward the RPC launch to the child node.\n\n<ANSWER>: The launch failure on x3104c0s1b0n0 was due to a temporary unavailability of resources."
    },
    {
        "id": "data/md/polaris/known-issues.md_seed_task_3_1",
        "context": [
            [
                "Name= NVIDIA A100-SXM4-40GB\n  Locally unique identifier= \n  Clock Frequency(KHz)= 1410000\n  Compute Mode= 0\n  Major compute capability= 8\n  Minor compute capability= 0\n  Number of multiprocessors on device= 108\n  Warp size in threads= 32\n  Single precision performance ratio= 2\n\nResult is CORRECT!! :)\n```\n\nGPU OpenACC\n\nA simple MPI-parallel OpenACC example is available here. Compilation proceeds similar to the above CUDA example except for the use of the -acc=gpu compiler flag to indicate compilation of OpenACC code for GPUs.\nCC -g -O3 -std=c++0x -acc=gpu -gpu=cc80,cuda11.0 main.cpp -o vecadd\nIn this example, each MPI rank sees all four GPUs on a Polaris node and GPUs are bound to MPI ranks round-robin within the application.\n\n```\n$ mpiexec -n 4 ./vecadd\n\nof devices= 4\n\nUsing single-precision\n\nRank 0 running on GPU 0!\nRank 1 running on GPU 1!\nRank 2 running on GPU 2!\nRank 3 running on GPU 3!\n\nResult is CORRECT!! :)\n``\nIf the application instead relies on the job launcher to bind MPI ranks to available GPUs, then a small helper script can be used to explicitly setCUDA_VISIBLE_DEVICES` appropriately for each MPI rank. One example is available here where each MPI rank is similarly bound to a single GPU with round-robin assignment. The binding of MPI ranks to GPUs is discussed in more detail here.\n\nGPU OpenCL",
                "data/md/polaris/compiling-and-linking/polaris-example-program-makefile.md"
            ],
            [
                "$ nsys --version\nNVIDIA Nsight Systems version 2023.3.1.92-233133147223v0\n\n$ ncu --version\nNVIDIA (R) Nsight Compute Command Line Profiler\nCopyright (c) 2018-2023 NVIDIA Corporation\nVersion 2023.2.1.0 (build 33050884) (public-release)\nNVIDIA Nsight Systems version 2022.4.2.1-df9881f\n```\n\nNsight Systems\n\nRun your application with Nsight Systems as follows:\n\n$ nsys profile -o {output_filename} --stats=true ./{your_application}\n\nRun your application on multiple nodes (e.g., 2 nodes) with Nsight Systems as follows:\n$ mpirun -n 8 --ppn 4 --env TMPDIR=/home/{user ID}/ nsys profile -o {output_filename}_%q{PMI_RANK} --stats=true ./{your_application}\n\nNsight Compute\n\nRun your application with Nsight Compute.\n\n$ ncu --set detailed -k {kernel_name} -o {output_filename} ./{your_application}\n\nRemark: Without -o option, Nsight Compute provides performance data as a standard output\n\nPost-processing the profiled data\n\nPost-processing via CLI\n\n$ nsys stats {output_filename}.qdrep\n$ ncu -i {output_filename}.ncu-rep\n\nPost-processing on your local system via GUI\n\nInstall NVIDIA Nsight Systems and NVIDIA Nsight Compute after downloading both of them from the  NVIDIA Developer Zone. \nRemark: Local client version should be the same as or newer than NVIDIA Nsight tools on Polaris.\n\nDownload nsys output files (i.e., ending with .qdrep and . sqlite) to your local system, and then open them with NVIDIA Nsight Systems on your local system.",
                "data/md/polaris/performance-tools/NVIDIA-Nsight.md"
            ],
            [
                "To switch from PrgEnv-nvhpc to PrgEnv-gnu you can run:\n\nmodule switch PrgEnv-nvhpc PrgEnv-gnu\n\nThe gcc/gfortran on Polaris was not built with GPU support. To use OpenMP on the CPU, you need to unload craype-accel-nvidia80:\n\nmodule unload craype-accel-nvidia80\n\nUsing PrgEnv-cray\n\nTo switch from PrgEnv-nvhpc to PrgEnv-cray you can run:\n\nmodule switch PrgEnv-nvhpc PrgEnv-cray\n\nTo use OpenMP on the CPU only, also unload craype-accel-nvidia80:\n\nmodule unload craype-accel-nvidia80\n\nTo use OpenMP on the GPU, load cudatoolkit-standalone, although this is not recommended at the moment.\nmodule load cudatoolkit-standalone\n\nBuilding on Polaris\n\nThe following table shows what compiler and flags to use with which PrgEnv:\n\nmodule compiler flags PrgEnv-nvhpc cc/CC/ftn (nvc/nvc++/nvfortran) -mp=gpu -gpu=cc80 llvm mpicc/mpicxx (clang/clang++) -fopenmp -fopenmp-targets=nvptx64-nvidia-cuda PrgEnv-gnu cc/CC/ftn (gcc/g++/gfortran) -fopenmp PrgEnv-cray cc/CC/ftn -fopenmp\n\nFor example to compile a simple code hello.cpp:\n\nFor PrgEnv-nvhpc, after loading the modules as discussed above we would use:\n\nCC -mp=gpu -gpu=cc80 hello.cpp\nftn -mp=gpu -gpu=cc80 hello.F90\n\nFor LLVM, after loading the modules as discussed above:\n\nmpicxx -fopenmp -fopenmp-targets=nvptx64-nvidia-cuda hello.cpp\n\nFor PrgEnv-gnu, after loading the modules as discussed above we would use:\n\nCC -fopenmp hello.cpp\nftn -fopenmp hello.F90\n\nFor PrgEnv-cray, after loading the modules as discussed above we would use:",
                "data/md/polaris/programming-models/openmp-polaris.md"
            ],
            [
                "Compiling for GPUs\n\nIt is assumed the majority of applications to be built on Polaris will make use of the GPUs. As such, the craype-accel-nvidia80 module is in the default environment. This has the effect of the Cray compiler wrappers adding -gpu to the compiler invocation along with additional include paths and libraries. Additional compilers flags may be needed depending on the compiler and GPU programming model used (e.g. -cuda, -acc, or -mp=gpu).\n\nThis module also adds GPU Transport Layer (GTL) libraries to the link-line to support GPU-aware MPI applications.\n\nMan Pages\n\nFor additional information on the Cray wrappers, please refer to the man pages.\nman cc\nman CC\nman ftn",
                "data/md/polaris/compiling-and-linking/compiling-and-linking-overview.md"
            ],
            "bash\n   launch failed on x3104c0s1b0n0: Couldn't forward RPC launch(ab751d77-e80a-4c54-b1c2-4e881f7e8c90) to child x3104c0s31b0n0.hsn.cm.polaris.alcf.anl.gov: Resource temporarily unavailable"
        ],
        "golden_context": "bash\n   launch failed on x3104c0s1b0n0: Couldn't forward RPC launch(ab751d77-e80a-4c54-b1c2-4e881f7e8c90) to child x3104c0s31b0n0.hsn.cm.polaris.alcf.anl.gov: Resource temporarily unavailable",
        "question": "Why couldn't the RPC launch be forwarded to x3104c0s31b0n0.hsn.cm.polaris.alcf.anl.gov??",
        "cot_answer": "To answer the question \"Why couldn't the RPC launch be forwarded to x3104c0s31b0n0.hsn.cm.polaris.alcf.anl.gov?\":\n\n1. The context provided indicates that the launch failed on x3104c0s1b0n0 due to a specific reason: \"Couldn't forward RPC launch to child x3104c0s31b0n0.hsn.cm.polaris.alcf.anl.gov: Resource temporarily unavailable.\"\n\n2. The error message \"Resource temporarily unavailable\" suggests that there may be a temporary issue or limitation preventing the RPC launch from being forwarded to the specified destination.\n\n3. Possible reasons for this error could include network congestion, server overload, or temporary unavailability of resources on the destination server x3104c0s31b0n0.hsn.cm.polaris.alcf.anl.gov.\n\n<ANSWER>: The RPC launch couldn't be forwarded to x3104c0s31b0n0.hsn.cm.polaris.alcf.anl.gov due to a temporary resource unavailability issue."
    },
    {
        "id": "data/md/polaris/known-issues.md_seed_task_3_2",
        "context": [
            [
                "Helper Scripts\n\nALCF/train-gpt3.sh\n\n:   Main entry point for training. This script will automatically source the\n    rest of the required ALCF/*.sh scripts below\n\nALCF/model.sh\n\n:   Contains some example model architectures for GPT3-style models\n\nALCF/args.sh\n\n:   Logic for parsing / setting up runtime options for Megatron and DeepSpeed.\n\nALCF/setup.sh\n\n:   Locate and activate virtual environment to be used, ensure MPI\n    variables are set properly\n\nALCF/launch.sh\n\n:   Identify available resources and build the command to be ran i.e.\n    figure out how many: {nodes, GPUs per node, GPUs total}, to pass to\n    mpi{run,exec} then, use this to build  mpiexec <mpiexec-args> python3\n    pretrain_gpt.py",
                "data/md/polaris/data-science-workflows/applications/megatron-deepspeed.md"
            ],
            [
                "VisIt on Polaris\n\nGetting Started\n\nThe latest VisIt versions installed on Polaris are 3.3.3 and 3.4.0.\n\nPlease note that at the time of this writing VisIt version 3.4.0 does not yet have a client for Mac available.\n\nFollow these steps to install VisIt on your local machine:\n\nDownload and install VisIt for your local platform (MacOS, Windows, Linux). The version you download must match the server version installed on Polaris. Use this page\n\nDownload the Polaris host profile for VisIt (you may need to right-click and choose \"Save link as...\" or \"Save target as...\")\n\nCopy this file to a file called ~/.visit/hosts/host_anl_polaris.xml on Mac or Linux. [ We need to also specify this path for for Windows]\n\nNote: VisIt allows the user to download host profiles for ANL, but all these settings are outdated. We are working with the VisIt developers to update the ANL host list.\n\nAdditional information for using VisIt in client/server mode here\n\nRunning VisIt\n\nStart up VisIt on your local machine\n\nClick File -> Open File and choose \"ANL Polaris\" from the \"Host\" dropdown\n\nYou'll be prompted for your password; enter your ALCF authenticator app response",
                "data/md/polaris/visualization/visit.md"
            ],
            "bash\n   launch failed on x3104c0s1b0n0: Couldn't forward RPC launch(ab751d77-e80a-4c54-b1c2-4e881f7e8c90) to child x3104c0s31b0n0.hsn.cm.polaris.alcf.anl.gov: Resource temporarily unavailable",
            [
                "NVIDIA Compilers on Polaris\n\nThe NVIDIA compilers (nvc, nvc++, nvcc, and nvfortran) are available on Polaris via the PrgEnv-nvhpc and nvhpc modules. There is currently a PrgEnv-nvidia module available, but that will soon be deprecated in Cray's PE, thus it is not recommend for use.\n\nThe Cray compiler wrappers map to NVIDIA compilers as follows.\n\ncc -> nvc\nCC -> nvc++\nftn -> nvfortran\n\nUsers are encouraged to look through NVIDIA's documentation for the NVHPC SDK and specific information on the compilers, tools, and libraries.\n\nNotes on NVIDIA Compilers\n\nPGI compilers\n\nThe NVIDIA programming environments makes available compilers from the NVIDIA HPC SDK. While the PGI compilers are available in this programming environment, it should be noted they are actually symlinks to the corresponding NVIDIA compilers.\npgcc -> nvc\npgc++ -> nvc++\npgf90 -> nvfortran\npgfortran -> nvfortran\nWhile nvcc is the traditional CUDA C and CUDA C++ compiler for NVIDIA GPUs, the nvc, nvc++, and nvfortran compilers additionally target CPUs.\n\nNVHPC SDK Directory Structure\n\nUsers migrating from CUDA toolkits to the NVHPC SDK may find it beneficial to review the directory structure of the hpc-sdk directory to find the location of commonly used libraries (including math libraries for the CPU). With the PrgEnv-nvhpc module loaded, the NVIDIA_PATH environment variable can be used to locate the path to various NVIDIA tools, libraries, and examples.\n\ncompiler/bin - cuda-gdb, ncu, nsys, ...",
                "data/md/polaris/compiling-and-linking/nvidia-compiler-polaris.md"
            ],
            [
                "You'll be prompted for your password; enter your ALCF authenticator app response\n\nWhen you open a selected file, it will launch a job on Polaris\nYou will need to specify the \"Bank\" (Project) to use when VisIt submits jobs to the queue on Polaris. Specify a project in the Options box.\nIf your environment doesn't get sourced correctly with non-interactive SSH, you can set the default project to use under Options -> Host profiles\n\nAdditional Information\n\nAdditional Information\n\nVisIt user manual\n\nVisIt wiki",
                "data/md/polaris/visualization/visit.md"
            ]
        ],
        "golden_context": "bash\n   launch failed on x3104c0s1b0n0: Couldn't forward RPC launch(ab751d77-e80a-4c54-b1c2-4e881f7e8c90) to child x3104c0s31b0n0.hsn.cm.polaris.alcf.anl.gov: Resource temporarily unavailable",
        "question": "Why was the resource temporarily unavailable during the launch?",
        "cot_answer": "To answer the question \"Why was the resource temporarily unavailable during the launch?\" based on the provided context:\n\n1. The error message indicates that the launch failed due to a specific issue: \"Couldn't forward RPC launch to child x3104c0s31b0n0.hsn.cm.polaris.alcf.anl.gov: Resource temporarily unavailable.\"\n\n2. The key phrase here is \"Resource temporarily unavailable,\" which suggests that the resource needed for the launch was not accessible at that moment.\n\n3. This could be due to various reasons such as high server load, network issues, or temporary unavailability of the required resource.\n\n<ANSWER>: The resource was temporarily unavailable during the launch due to potential reasons like high server load, network issues, or temporary unavailability of the required resource."
    },
    {
        "id": "data/md/polaris/running-jobs.md_seed_task_4_0",
        "context": [
            [
                "Known Issues\n\nThis is a collection of known issues that have been encountered on Polaris. Documentation will be updated as issues are resolved. Users are encouraged to email support@alcf.anl.gov to report issues.\n\nSubmitting Jobs\n\nFor batch job submissions, if the parameters within your submission script do not meet the parameters of any of the execution queues (small, ..., backfill-large) you might not receive the \"Job submission\" error on the command line at all, and the job will never appear in history qstat -xu <username> (current bug in PBS). E.g. if a user submits a script to the prod routing queue requesting 10 nodes for 24 hours, exceeding \"Time Max\" of 6 hrs of the small execution queue (which handles jobs with 10-24 nodes), then it may behave as if the job was never submitted.\n\nJob scripts are copied to temporary locations after qsub and any changes to the original script while the job is queued will not be reflected in the copied script. Furthermore, qalter requires -A <allocation name> when changing job properties. Currently, there is a request for a qalter-like command to trigger a re-copy of the original script to the temporary location.\n\nCompiling & Running Applications\n\nIf your job fails to start with an RPC launch message like below, please forward the complete messages to support@alcf.anl.gov.",
                "data/md/polaris/known-issues.md"
            ],
            [
                "CMake\n\nCMake\n\nCMake is a build configuration system that uses higher-level description files\nto automatically generate Makefiles.\n\nCMake Documentation\n\nCMake website\n\nCMake on Polaris\n\nTo use CMake on Polaris, run\n\nmodule use /soft/modulefiles\nmodule load spack-pe-base cmake",
                "data/md/polaris/build-tools/cmake-polaris.md"
            ],
            [
                "Login nodes\n\nThere are four login nodes available to users for editing code, building code, submitting / monitoring jobs, checking usage (sbank), etc..  Their full hostnames are polaris-login-N.hsn.cm.polaris.alcf.anl.gov  for N equal to 01 through 04; there are an additional two login nodes that are not user-accessible which are used for running services such as JupyterHub. The various compilers and libraries are present on the logins, so most users should be able to build their code.  However, if your build requires the physical presence of the GPU, you will need to build on a compute node.\n\nAll users share the same login nodes so please be courteous and respectful of your fellow users.  For example, please do not run computationally or IO intensive pre- or post-processing on the logins and keep the parallelism of your builds to a reasonable level.\n\nPOLARIS LOGIN DESCRIPTION PER NODE AGGREGATE Processor (Note 1) 2.0 GHz 7713 2 12 Cores/Threads AMD Zen 3 (Milan) 128/256 768/1536 RAM (Note 2) DDR4 512 GiB 3 TiB GPUs (Note 3) No GPUs 0 0 Local SSD None 0 0\n\nNote 1: 256MB shared L3 cache, 512KB L2 cache per core, 32 KB L1 cache per core\nNote 2: 8 memory channels rated at 204.8 GiB/s per socket\nNote 3: If your build requires the physical presence of a GPU you will need to build on a compute node.\n\nGateway nodes",
                "data/md/polaris/hardware-overview/machine-overview.md"
            ],
            [
                "OMP settings mostly to quiet Kokkos messages\n\nMPI_ARG=\" -n ${NTOTRANKS} --ppn ${NRANKS} --depth=${NDEPTH} --cpu-bind depth \"\nOMP_ARG=\" --env OMP_NUM_THREADS=${NTHREADS} --env OMP_PROC_BIND=spread --env OMP_PLACES=cores \"\n\nCOMMAND=\"mpiexec ${MPI_ARG} ${OMP_ARG} ${EXE} ${EXE_ARG}\"\necho \"COMMAND= ${COMMAND}\"\n${COMMAND}\n```\n\nGPU package\n\nThe module environments above can be used to build LAMMPS with the GPU package as well. Copies of Makefiles for building with the GPU package using CUDA for GPU support with the GNU and NVHPC compilers are available in the ALCF GettingStarted repo here.\n\nPerformance Notes\n\nSome useful information on accelerator packages and expectations can be found on the LAMMPS website here.",
                "data/md/polaris/applications-and-libraries/applications/lammps.md"
            ],
            "Running Jobs on Polaris\n\nQueues\n\nThere are five production queues you can target in your qsub (-q <queue name>):\n\nQueue Name Node Min Node Max Time Min Time Max Notes debug 1 2 5 min 1 hr max 24 nodes in use by this queue ay any given time; Only 8 nodes are exclusive (see Note below) debug-scaling 1 10 5 min 1 hr max 1 job running/accruing/queued per-user prod 10 496 5 min 24 hrs Routing queue; See below preemptable 1 10 5 min 72 hrs Please be aware that jobs in the preemptable queue can be killed at any time if jobs are submitted to the demand queue. Max 20 jobs running/accruing/queued per-project ; see Note below demand 1 56 5 min 1 hr By request only ; max 100 jobs running/accruing/queued per-project\n\nNote: Please be aware that jobs in the preemptable queue can be killed at any time if jobs are submitted to the demand queue.\nJobs in the demand queue take priority over jobs in the preemptable queue.\nThis means jobs in the preemptable queue may be preempted (killed without any warning) if there are jobs in the demand queue.\nUnfortunately, there's always an inherent risk of jobs being killed when using the preemptable queue. \nPlease use the following command to view details of a queue: qstat -Qf <queuename>\n\nTo make your job re-runable add the following PBS directive: #PBS -r y This will ensure your job will restart once the demand job is complete."
        ],
        "golden_context": "Running Jobs on Polaris\n\nQueues\n\nThere are five production queues you can target in your qsub (-q <queue name>):\n\nQueue Name Node Min Node Max Time Min Time Max Notes debug 1 2 5 min 1 hr max 24 nodes in use by this queue ay any given time; Only 8 nodes are exclusive (see Note below) debug-scaling 1 10 5 min 1 hr max 1 job running/accruing/queued per-user prod 10 496 5 min 24 hrs Routing queue; See below preemptable 1 10 5 min 72 hrs Please be aware that jobs in the preemptable queue can be killed at any time if jobs are submitted to the demand queue. Max 20 jobs running/accruing/queued per-project ; see Note below demand 1 56 5 min 1 hr By request only ; max 100 jobs running/accruing/queued per-project\n\nNote: Please be aware that jobs in the preemptable queue can be killed at any time if jobs are submitted to the demand queue.\nJobs in the demand queue take priority over jobs in the preemptable queue.\nThis means jobs in the preemptable queue may be preempted (killed without any warning) if there are jobs in the demand queue.\nUnfortunately, there's always an inherent risk of jobs being killed when using the preemptable queue. \nPlease use the following command to view details of a queue: qstat -Qf <queuename>\n\nTo make your job re-runable add the following PBS directive: #PBS -r y This will ensure your job will restart once the demand job is complete.",
        "question": "What PBS directive should be added to make a job on Polaris rerunnable?",
        "cot_answer": "To answer this question, we need to identify the PBS directive that should be added to make a job on Polaris rerunnable based on the provided context.\n\n1. **Understand the context**: The context provided is about running jobs on Polaris, specifically focusing on the different queues available for job submission and their characteristics.\n\n2. **Identify the relevant information**: The directive needed to make a job rerunnable is mentioned in the context. It states: \"To make your job re-runable add the following PBS directive: #PBS -r y This will ensure your job will restart once the demand job is complete.\"\n\n3. **Summarize the directive**: The PBS directive that should be added to make a job on Polaris rerunnable is `#PBS -r y`.\n\n### Final Answer:\n<ANSWER>: The PBS directive that should be added to make a job on Polaris rerunnable is `#PBS -r y`."
    },
    {
        "id": "data/md/polaris/running-jobs.md_seed_task_5_0",
        "context": [
            [
                "config = Config(\n        executors=[\n            HighThroughputExecutor(\n                label=\"htex\",\n                heartbeat_period=15,\n                heartbeat_threshold=120,\n                worker_debug=True,\n                available_accelerators=user_opts[\"available_accelerators\"], # if this is set, it will override other settings for max_workers if set\n                cores_per_worker=user_opts[\"cores_per_worker\"],\n                address=address_by_interface(\"bond0\"),\n                cpu_affinity=\"block-reverse\",\n                prefetch_capacity=0,\n                start_method=\"spawn\",  # Needed to avoid interactions between MPI and os.fork\n                provider=PBSProProvider(\n                    launcher=MpiExecLauncher(bind_cmd=\"--cpu-bind\", overrides=\"--depth=64 --ppn 1\"),\n                    # Which launcher to use?  Check out the note below for some details.  Try MPI first!\n                    # launcher=GnuParallelLauncher(),\n                    account=user_opts[\"account\"],\n                    queue=user_opts[\"queue\"],\n                    select_options=\"ngpus=4\",\n                    # PBS directives (header lines): for array jobs pass '-J' option\n                    scheduler_options=user_opts[\"scheduler_options\"],\n                    # Command to be run before starting a worker, such as:\n                    worker_init=user_opts[\"worker_init\"],\n                    # number of compute nodes allocated for each block",
                "data/md/polaris/workflows/parsl.md"
            ],
            [
                "QMCPACK on Polaris\n\nQMCPACK, is a modern high-performance open-source Quantum Monte Carlo (QMC) simulation code. Its main applications are electronic structure calculations of molecular, quasi-2D and solid-state systems. Variational Monte Carlo (VMC), diffusion Monte Carlo (DMC) and a number of other advanced QMC algorithms are implemented. Orbital space auxiliary field QMC (AFQMC) has recently been added. By directly solving the Schrodinger equation, QMC methods offer greater accuracy than methods such as density functional theory, but at a trade-off of much greater computational expense.\n\nPrebuilt executables are provided at /soft/applications/qmcpack.\nThe directory of each installation also includes a job submission script example qmcpack-polaris.job.\nUpdate build recipe is provided on GitHub.",
                "data/md/polaris/applications-and-libraries/applications/QMCPACK.md"
            ],
            [
                "Math Libraries\n\nBLAS, LAPACK, and ScaLAPACK for CPUs\n\nSome math libraries targeting CPUs are made available as part of the nvhpc modules and are based on the OpenBLAS project. Additional documentation is available from NVIDIA.\n\nBLAS & LAPACK can be found in the $NVIDIA_PATH/compilers/lib directory.\n\nScaLAPACK can be found in the $NVIDIA_PATH/comm_libs directory.\n\nGNU Scientific Library, GSL-2.7 available as module help math_libs/gsl\n\nAMD Optiming CPU Libraries, AOCL v4.2 available as module help math_libs/aocl\n\nOther Cray-based math libs such as Libsci, FFTW were made available by module load cray-libsci & module load cray-fftw\n\nNVIDIA Math Libraries for GPUs\n\nMath libraries from NVIDIA are made available via the nvhpc modules. Many of the libraries users typically use can be found in the $NVIDIA_PATH/math_libs directory. Some examples follow and additional documentation is available from NVIDIA.\n\nlibcublas\n\nlibcufft\n\nlibcurand\n\nlibcusolver\n\nlibcusparse",
                "data/md/polaris/applications-and-libraries/libraries/math-libraries.md"
            ],
            [
                "ParaView Tutorial\n\nOverview\n\nThis tutorial is intended to be a hands-on resource for users interested in learning the basic concepts of ParaView. The examples can easily be run on a laptop, using the example data set provided.\n\nTour of ParaView\n\nShow range of visualization methods\n\nWalk through various visualization techniques, hopefully illustrate how these can apply to your own data\n\nFeel for ParaView \"way\"\n\nTerminology and step-by-step process peculiar to ParaView, which may differ from other packages, e.g. VisIt\n\nData\n\nThe data used for this tutorial is:\n- Blood flow simulation data\n- Multiple data types\n  - Continuum data field (unstructured mesh, tetrahedral): fluid field, plasma\n  - Particle data (unstructured points): individual particles moving in the flow\n  - Red Blood Cells (RBC, unstructured mesh, triangle): mesh of the surface of an RBC\n    - Healthy\n    - Diseased\n- Generated using an integrated Nektar/LAMMPS simulation code\n- Courtesy of George Karniadakis and Leopold Grinberg of Brown University\n\nThe data is available for download here (~27MB compressed, ~39MB uncompressed):\nData set for ParaView Red Blood Cell Tutorial\n\n1. Load Multi-component Dataset\n\nFrom the Filemenu, (you can also click the file folder icon, shown above) open each of the following data sets (select then click \"OK\")\n\nThe files will then appear in the Pipeline Browser\n\nClick Apply in the Object Inspector\n\nYou will need to do this one at a time:\n\ncontinuum...vtu\n\nparticles...vtu\n\nrbc_...vtu",
                "data/md/polaris/visualization/paraview-tutorial.md"
            ],
            [
                "```bash\n\n!/bin/bash -l\n\nnum_gpus=4\n\nneed to assign GPUs in reverse order due to topology\n\nSee Polaris Device Affinity Information:\n\nhttps://www.alcf.anl.gov/support/user-guides/polaris/hardware-overview/machine-overview/index.html\n\ngpu=$((${num_gpus} - 1 - ${PMI_LOCAL_RANK} % ${num_gpus}))\nexport CUDA_VISIBLE_DEVICES=$gpu\necho “RANK= ${PMI_RANK} LOCAL_RANK= ${PMI_LOCAL_RANK} gpu= ${gpu}”\nexec \"$@\"\nThis script can be placed just before the executable in the `mpiexec` command like so.bash\nmpiexec -n ${NTOTRANKS} --ppn ${NRANKS_PER_NODE} --depth=${NDEPTH} --cpu-bind depth ./set_affinity_gpu_polaris.sh ./hello_affinity\n```\nUsers with different needs, such as assigning multiple GPUs per MPI rank, can modify the above script to suit their needs.\n\nInteractive Jobs on Compute Nodes\n\nHere is how to submit an interactive job to, for example, edit/build/test an application Polaris compute nodes:\nqsub -I -l select=1 -l filesystems=home:eagle -l walltime=1:00:00 -q debug -A <project_name>\n\nThis command requests 1 node for a period of 1 hour in the debug queue, requiring access to the /home and eagle filesystems. After waiting in the queue for a node to become available, a shell prompt on a compute node will appear. You may then start building applications and testing gpu affinity scripts on the compute node.\n\nNOTE: If you want to ssh or scp to one of your assigned compute nodes you will need to make sure your $HOME directory and your $HOME/.ssh directory permissions are both set to 700.",
                "data/md/polaris/running-jobs.md"
            ]
        ],
        "golden_context": "Note: The debug queue has 8 exclusively dedicated nodes.\nIf there are free nodes in production, then debug jobs can take another 16 nodes for a total of 24.\n\nprod is routing queue and routes your job to one of the following six execution queues:\n\nQueue Name Node Min Node Max Time Min Time Max Notes small 10 24 5 min 3 hrs medium 25 99 5 min 6 hrs large 100 496 5 min 24 hrs backfill-small 10 24 5 min 3 hrs low priority, negative project balance backfill-medium 25 99 5 min 6 hrs low priority, negative project balance backfill-large 100 496 5 min 24 hrs low priority, negative project balance\n\nNote 1: You cannot submit to these queues directly, you can only submit to the routing queue \"prod\".\n\nNote 2: All of these queues have a limit of ten (10) jobs running/accruing per-project\n\nNote 3: All of these queues have a limit of one hundred (100) jobs queued (not accruing score) per-project\n\nNote 4: As of January 2023, it is recommended to submit jobs with a maximum node count of 476-486 nodes given current rates of downed nodes (larger jobs may sit in the queue indefinitely).\n\nRunning MPI+OpenMP Applications\n\nOnce a submitted job is running calculations can be launched on the compute nodes using mpiexec to start an MPI application. Documentation is accessible via man mpiexec and some helpful options follow.\n\n-n total number of MPI ranks\n\n-ppn number of MPI ranks per node\n\n--cpu-bind CPU binding for application\n\n--depth number of cpus per rank (useful with --cpu-bind)",
        "question": "How many free nodes can debug jobs access in production? ?",
        "cot_answer": "To determine how many free nodes debug jobs can access in production, we need to consider the information provided in the context:\n\n1. The debug queue has 8 exclusively dedicated nodes.\n2. If there are free nodes in production, debug jobs can take another 16 nodes for a total of 24.\n\nGiven that the debug queue has 8 exclusively dedicated nodes, we start with 8 nodes. If there are free nodes in production, debug jobs can access an additional 16 nodes. Therefore, the total number of free nodes debug jobs can access in production is 8 + 16 = 24.\n\n<ANSWER>: Debug jobs can access 24 free nodes in production."
    },
    {
        "id": "data/md/polaris/running-jobs.md_seed_task_5_1",
        "context": [
            "Note: The debug queue has 8 exclusively dedicated nodes.\nIf there are free nodes in production, then debug jobs can take another 16 nodes for a total of 24.\n\nprod is routing queue and routes your job to one of the following six execution queues:\n\nQueue Name Node Min Node Max Time Min Time Max Notes small 10 24 5 min 3 hrs medium 25 99 5 min 6 hrs large 100 496 5 min 24 hrs backfill-small 10 24 5 min 3 hrs low priority, negative project balance backfill-medium 25 99 5 min 6 hrs low priority, negative project balance backfill-large 100 496 5 min 24 hrs low priority, negative project balance\n\nNote 1: You cannot submit to these queues directly, you can only submit to the routing queue \"prod\".\n\nNote 2: All of these queues have a limit of ten (10) jobs running/accruing per-project\n\nNote 3: All of these queues have a limit of one hundred (100) jobs queued (not accruing score) per-project\n\nNote 4: As of January 2023, it is recommended to submit jobs with a maximum node count of 476-486 nodes given current rates of downed nodes (larger jobs may sit in the queue indefinitely).\n\nRunning MPI+OpenMP Applications\n\nOnce a submitted job is running calculations can be launched on the compute nodes using mpiexec to start an MPI application. Documentation is accessible via man mpiexec and some helpful options follow.\n\n-n total number of MPI ranks\n\n-ppn number of MPI ranks per node\n\n--cpu-bind CPU binding for application\n\n--depth number of cpus per rank (useful with --cpu-bind)",
            [
                "In addition to the base stack, a suite of higher-level libraries are installed\nin the spack-pe-gnu module. These are built with and are dependent on\nPrgEnv-gnu. A PrgEnv-nvidia-compatible stack will be available in the\nfuture.\n\nNote that not all software is installed through Spack; many applications and\nlibraries are installed as standalone packages in /soft. Users are encouraged\nto browse the available modules with module avail to see what software is\ninstalled on the system.\n\nParaView and Visit\n\nParaView module has been updated. For more information, see https://docs.alcf.anl.gov/polaris/visualization/paraview/ and https://docs.alcf.anl.gov/polaris/visualization/paraview-manual-launch/\n\nVisit module is in the process of being updated.\n\nChanges to Memory Limits on Login Nodes\n\nMemory limits were lowered on the logins due to resource contention to 8GB of memory, and 8 cores per user. \nThis might result in error messages indicating abnormal process termination for user processes run on logins.\n\nExamples of the error messages people might see are:\n\nnvcc error   : 'cudafe++' died due to signal 9 (Kill signal)\n\ng++-12: fatal error: Killed signal terminated program cc1plus\n\nThese errors are likely due to exhausting the per-user resources on a login node as each user is allocated 8 cores and 8GB memory.\nTo avoid this you can either:\n\nReduce the parallelism of your compile, such as using -j or -j4 flags",
                "data/md/polaris/system-updates.md"
            ],
            [
                "Inspecting packages\n\nWhen a module in the Spack PE is loaded, several environment variables are\nupdated to integrate the package into the user's environment. Additionally, the\nPACKAGE_ROOT variable is set to the path to the installation prefix of the\npackage. For example, after loading cmake as above:\n\n$ echo $CMAKE_ROOT\n/soft/spack/gcc/0.6.1/install/linux-sles15-x86_64/gcc-12.3.0/cmake-3.27.7-a435jtzvweeos2es6enirbxdjdqhqgdp/\n$ ls -a $CMAKE_ROOT\n.  ..  bin  doc  share  .spack\n\nThis variable can be used to inspect software installations and find header or\nlibrary paths. Additionally, Spack packages have a .spack directory in the\ninstallation prefix which contains build information and logs.\n\nBuilding software with Spack\n\nSpack is a powerful package manager designed for HPC.\nThe Spack PE is installed and managed with Spack; users can also install Spack\nin their own home or project directory to manage their software builds. Spack\nhas a steep learning curve, but it may benefit workflows involving frequent\nbuilds with complex dependencies.",
                "data/md/polaris/applications-and-libraries/libraries/spack-pe.md"
            ],
            [
                "```\n$ qsub -I -l select=1,walltime=1:00:00,filesystems=home:grand:eagle -A [PROJECT] -q debug\n\n$ julia --project -e \"using CUDA; CUDA.versioninfo()\"\nCUDA runtime 12.4, artifact installation\nCUDA driver 12.4\nNVIDIA driver 535.154.5, originally for CUDA 12.2\n\nCUDA libraries: \n- CUBLAS: 12.2.1\n- CURAND: 10.3.5\n- CUFFT: 11.2.1\n- CUSOLVER: 11.6.1\n- CUSPARSE: 12.3.1\n- CUPTI: 22.0.0\n- NVML: 12.0.0+535.154.5\n\nJulia packages: \n- CUDA: 5.3.3\n- CUDA_Driver_jll: 0.8.1+0\n- CUDA_Runtime_jll: 0.12.1+0\n\nToolchain:\n- Julia: 1.10.3\n- LLVM: 15.0.7\n\n4 devices:\n  0: NVIDIA A100-SXM4-40GB (sm_80, 39.390 GiB / 40.000 GiB available)\n  1: NVIDIA A100-SXM4-40GB (sm_80, 39.390 GiB / 40.000 GiB available)\n  2: NVIDIA A100-SXM4-40GB (sm_80, 39.390 GiB / 40.000 GiB available)\n  3: NVIDIA A100-SXM4-40GB (sm_80, 39.390 GiB / 40.000 GiB available)\n```\n\nOne can then switch between versions of CUDA as needed. Note, the following commands were executed in an interactive job on a compute node.\n\n$ julia --project -e \"using CUDA; CUDA.set_runtime_version!(local_toolkit=true)\"\n[ Info: Configure the active project to use the default CUDA from the local system; please re-start Julia for this to take effect.\n\n```\n$ julia --project -e \"using CUDA; CUDA.versioninfo()\"\nCUDA runtime 12.2, local installation\nCUDA driver 12.4\nNVIDIA driver 535.154.5, originally for CUDA 12.2\n\nCUDA libraries: \n- CUBLAS: 12.2.1\n- CURAND: 10.3.3\n- CUFFT: 11.0.8\n- CUSOLVER: 11.5.0\n- CUSPARSE: 12.1.1\n- CUPTI: 20.0.0\n- NVML: 12.0.0+535.154.5",
                "data/md/polaris/data-science-workflows/julia.md"
            ],
            [
                "NVIDIA Compilers on Polaris\n\nThe NVIDIA compilers (nvc, nvc++, nvcc, and nvfortran) are available on Polaris via the PrgEnv-nvhpc and nvhpc modules. There is currently a PrgEnv-nvidia module available, but that will soon be deprecated in Cray's PE, thus it is not recommend for use.\n\nThe Cray compiler wrappers map to NVIDIA compilers as follows.\n\ncc -> nvc\nCC -> nvc++\nftn -> nvfortran\n\nUsers are encouraged to look through NVIDIA's documentation for the NVHPC SDK and specific information on the compilers, tools, and libraries.\n\nNotes on NVIDIA Compilers\n\nPGI compilers\n\nThe NVIDIA programming environments makes available compilers from the NVIDIA HPC SDK. While the PGI compilers are available in this programming environment, it should be noted they are actually symlinks to the corresponding NVIDIA compilers.\npgcc -> nvc\npgc++ -> nvc++\npgf90 -> nvfortran\npgfortran -> nvfortran\nWhile nvcc is the traditional CUDA C and CUDA C++ compiler for NVIDIA GPUs, the nvc, nvc++, and nvfortran compilers additionally target CPUs.\n\nNVHPC SDK Directory Structure\n\nUsers migrating from CUDA toolkits to the NVHPC SDK may find it beneficial to review the directory structure of the hpc-sdk directory to find the location of commonly used libraries (including math libraries for the CPU). With the PrgEnv-nvhpc module loaded, the NVIDIA_PATH environment variable can be used to locate the path to various NVIDIA tools, libraries, and examples.\n\ncompiler/bin - cuda-gdb, ncu, nsys, ...",
                "data/md/polaris/compiling-and-linking/nvidia-compiler-polaris.md"
            ]
        ],
        "golden_context": "Note: The debug queue has 8 exclusively dedicated nodes.\nIf there are free nodes in production, then debug jobs can take another 16 nodes for a total of 24.\n\nprod is routing queue and routes your job to one of the following six execution queues:\n\nQueue Name Node Min Node Max Time Min Time Max Notes small 10 24 5 min 3 hrs medium 25 99 5 min 6 hrs large 100 496 5 min 24 hrs backfill-small 10 24 5 min 3 hrs low priority, negative project balance backfill-medium 25 99 5 min 6 hrs low priority, negative project balance backfill-large 100 496 5 min 24 hrs low priority, negative project balance\n\nNote 1: You cannot submit to these queues directly, you can only submit to the routing queue \"prod\".\n\nNote 2: All of these queues have a limit of ten (10) jobs running/accruing per-project\n\nNote 3: All of these queues have a limit of one hundred (100) jobs queued (not accruing score) per-project\n\nNote 4: As of January 2023, it is recommended to submit jobs with a maximum node count of 476-486 nodes given current rates of downed nodes (larger jobs may sit in the queue indefinitely).\n\nRunning MPI+OpenMP Applications\n\nOnce a submitted job is running calculations can be launched on the compute nodes using mpiexec to start an MPI application. Documentation is accessible via man mpiexec and some helpful options follow.\n\n-n total number of MPI ranks\n\n-ppn number of MPI ranks per node\n\n--cpu-bind CPU binding for application\n\n--depth number of cpus per rank (useful with --cpu-bind)",
        "question": "What is the recommended maximum node count for jobs submitted as of January 2023? ?",
        "cot_answer": "To determine the recommended maximum node count for jobs submitted as of January 2023, we need to consider the information provided in the context.\n\n1. The debug queue has 8 exclusively dedicated nodes, and if there are free nodes in production, debug jobs can take another 16 nodes for a total of 24. This information is relevant for debug job allocations.\n\n2. The routing queue \"prod\" routes jobs to different execution queues based on node requirements. The queues available are small, medium, large, backfill-small, backfill-medium, and backfill-large.\n\n3. As of January 2023, it is recommended to submit jobs with a maximum node count of 476-486 nodes due to current rates of downed nodes.\n\nGiven this information, the recommended maximum node count for jobs submitted as of January 2023 is 476-486 nodes.\n\n<ANSWER>: The recommended maximum node count for jobs submitted as of January 2023 is 476-486 nodes."
    },
    {
        "id": "data/md/polaris/running-jobs.md_seed_task_5_2",
        "context": [
            [
                "For PrgEnv-cray, after loading the modules as discussed above we would use:\n\nCC -fopenmp hello.cpp\nftn -fopenmp hello.F90\n\nRunning on Polaris\n\nTo run, you can run the produced executable or with mpiexec in a job script, and then submit the script to the Polaris queue, like:\n\n```\n$ cat submit.sh\n\n!/bin/sh\n\nPBS -l select=1:system=polaris\n\nPBS -l walltime=0:30:00\n\nPBS -q debug\n\nPBS -A Catalyst\n\nPBS -l filesystems=home:eagle\n\ncd ${PBS_O_WORKDIR}\n mpiexec -n 1 ./executable\n$ # submit to the queue:\n$ qsub -l select=1:system=polaris -l walltime=0:30:00 -l filesystems=home:eagle -q debug -A Catalyst ./submit.sh\n```\n\nIn the above, having the PBS options in the script and on the command line is redundant, but we put it there to show both ways of launching. This submits the script to one node in the debug queue on Polaris, requesting 30 min and the eagle and home filesystems. It will charge project Catalyst for the time.\n\nMore details for setting up the job script are in Job Scheduling and Execution section.\n\nExample\n\n```\n$ cat hello.cpp\n\ninclude\n\ninclude\n\nint main( int argv, char** argc ) {\n\nprintf( \"Number of devices: %d\\n\", omp_get_num_devices() );\n\n#pragma omp target\n  {\n    if( !omp_is_initial_device() )\n      printf( \"Hello world from accelerator.\\n\" );\n    else\n      printf( \"Hello world from host.\\n\" );\n  }\n  return 0;\n}\n\n$ cat hello.F90\nprogram  main\n  use omp_lib\n  implicit none\n  integer flag\n\nwrite(,) \"Number of devices:\", omp_get_num_devices()",
                "data/md/polaris/programming-models/openmp-polaris.md"
            ],
            "Note: The debug queue has 8 exclusively dedicated nodes.\nIf there are free nodes in production, then debug jobs can take another 16 nodes for a total of 24.\n\nprod is routing queue and routes your job to one of the following six execution queues:\n\nQueue Name Node Min Node Max Time Min Time Max Notes small 10 24 5 min 3 hrs medium 25 99 5 min 6 hrs large 100 496 5 min 24 hrs backfill-small 10 24 5 min 3 hrs low priority, negative project balance backfill-medium 25 99 5 min 6 hrs low priority, negative project balance backfill-large 100 496 5 min 24 hrs low priority, negative project balance\n\nNote 1: You cannot submit to these queues directly, you can only submit to the routing queue \"prod\".\n\nNote 2: All of these queues have a limit of ten (10) jobs running/accruing per-project\n\nNote 3: All of these queues have a limit of one hundred (100) jobs queued (not accruing score) per-project\n\nNote 4: As of January 2023, it is recommended to submit jobs with a maximum node count of 476-486 nodes given current rates of downed nodes (larger jobs may sit in the queue indefinitely).\n\nRunning MPI+OpenMP Applications\n\nOnce a submitted job is running calculations can be launched on the compute nodes using mpiexec to start an MPI application. Documentation is accessible via man mpiexec and some helpful options follow.\n\n-n total number of MPI ranks\n\n-ppn number of MPI ranks per node\n\n--cpu-bind CPU binding for application\n\n--depth number of cpus per rank (useful with --cpu-bind)",
            [
                "write(,) \"Number of devices:\", omp_get_num_devices()\n\n!$omp target map(from:flag)\n    if( .not. omp_is_initial_device() ) then\n      flag = 1\n    else\n      flag = 0\n   endif\n  !$omp end target\n\nif( flag == 1 ) then\n      print , \"Hello world from accelerator\"\n   else\n      print , \"Hello world from host\"\n   endif\n\nend program main\n\n$ # To compile\n$ CC -mp=gpu -gpu=cc80 hello.cpp -o c_test\n$ ftn -mp=gpu -gpu=cc80 hello.F90 -o f_test\n\n$ # To run \n$ mpiexec -n 1 ./c_test\nNumber of devices: 4\nHello world from accelerator.\n$ mpiexec -n 1 ./f_test\n Number of devices:            4\n Hello world from accelerator\n\n```",
                "data/md/polaris/programming-models/openmp-polaris.md"
            ],
            [
                "A simple example batch script for a libEnsemble use case that runs five workers on one node:\n\n```shell\n    #!/bin/bash -l\n    #PBS -l select=1:system=polaris\n    #PBS -l walltime=00:15:00\n    #PBS -l filesystems=home:grand\n    #PBS -q debug\n    #PBS -A\n\n```\n\nThe script can be run with:\n\nOr you can run an interactive session with:\n\nFurther links\n\nDocs:\n\nhttps://libensemble.readthedocs.io\n\nGitHub:\n\nhttps://github.com/Libensemble/libensemble",
                "data/md/polaris/workflows/libensemble.md"
            ],
            [
                "Modules newly installed\n\nThe following modules have been newly installed:\n\ncabana/dev-9a1ad605/kokv/4.2.01/PrgEnv-gnu/8.5.0/gnu/12.3/cuda_cudatoolkit_12.2.91\n   cuda-PrgEnv-nvidia/12.2.91\n   cudatoolkit-standalone/12.2.2                                                      (D)\n   cudatoolkit-standalone/12.3.2\n   cudatoolkit-standalone/12.4.0\n   cudnn/9.0.0\n   forge/23.1.2\n   kokkos/4.2.01/shared/PrgEnv-gnu/8.5.0/gnu/12.3/cuda_cudatoolkit_12.2.91\n   spack-pe-base/0.6.1\n   spack-pe-gnu/0.6.1\n\nNote that spack-pe-base and spack-pe-gnu are metamodules which contain\nfurther software offerings. See the Spack section below for details.\n\nSpack\n\nWe have newly installed Spack deployments in /soft. Spack is an HPC-oriented\npackage manager which ALCF uses to install software for the user environment.\nHowever, no knowledge of Spack is necessary to use these software offerings. All\nALCF-managed software is accessible to users via modules.\n\nThe base suite of software tools and libraries can be accessed by loading the\nspack-pe-base module. This adds a path to $MODULEPATH which contains\nnumerous modules.\n\nFor example, to load cmake starting from the default environment, a user\nshould run the following commands:\nmodule use /soft/modulefiles\nmodule load spack-pe-base\nmodule load cmake\nOther modules in spack-pe-base can be browsed by running module avail or\nmodule --show-hidden avail. The latter shows hidden modules which are\ninstalled as dependencies of the un-hidden modules.",
                "data/md/polaris/system-updates.md"
            ]
        ],
        "golden_context": "Note: The debug queue has 8 exclusively dedicated nodes.\nIf there are free nodes in production, then debug jobs can take another 16 nodes for a total of 24.\n\nprod is routing queue and routes your job to one of the following six execution queues:\n\nQueue Name Node Min Node Max Time Min Time Max Notes small 10 24 5 min 3 hrs medium 25 99 5 min 6 hrs large 100 496 5 min 24 hrs backfill-small 10 24 5 min 3 hrs low priority, negative project balance backfill-medium 25 99 5 min 6 hrs low priority, negative project balance backfill-large 100 496 5 min 24 hrs low priority, negative project balance\n\nNote 1: You cannot submit to these queues directly, you can only submit to the routing queue \"prod\".\n\nNote 2: All of these queues have a limit of ten (10) jobs running/accruing per-project\n\nNote 3: All of these queues have a limit of one hundred (100) jobs queued (not accruing score) per-project\n\nNote 4: As of January 2023, it is recommended to submit jobs with a maximum node count of 476-486 nodes given current rates of downed nodes (larger jobs may sit in the queue indefinitely).\n\nRunning MPI+OpenMP Applications\n\nOnce a submitted job is running calculations can be launched on the compute nodes using mpiexec to start an MPI application. Documentation is accessible via man mpiexec and some helpful options follow.\n\n-n total number of MPI ranks\n\n-ppn number of MPI ranks per node\n\n--cpu-bind CPU binding for application\n\n--depth number of cpus per rank (useful with --cpu-bind)",
        "question": "What are the options for CPU binding in MPI applications using mpiexec? ?",
        "cot_answer": "To determine the options for CPU binding in MPI applications using mpiexec, we need to consider the provided context regarding CPU binding options and the available nodes in different queues.\n\n1. **Understand CPU Binding Options**:\n   - From the context, the relevant options for CPU binding in mpiexec are:\n     - `--cpu-bind CPU binding for application`\n     - `--depth number of cpus per rank (useful with --cpu-bind)`\n\n2. **Consider Node Availability**:\n   - The debug queue has 8 exclusively dedicated nodes.\n   - If there are free nodes in production, debug jobs can take another 16 nodes for a total of 24.\n\n3. **Queue Information**:\n   - The production queue \"prod\" routes jobs to different execution queues based on node requirements and time limits.\n\n4. **Queue Options**:\n   - The queues available are small, medium, large, backfill-small, backfill-medium, and backfill-large.\n   - Each queue has specific node limits and time constraints.\n\n5. **Submission Restrictions**:\n   - Jobs cannot be submitted directly to the execution queues; they must go through the routing queue \"prod.\"\n   - There are limits on the number of running and queued jobs per project for each queue.\n\n6. **MPI Application Execution**:\n   - Once a job is running, calculations can be launched on compute nodes using mpiexec with options for MPI ranks, ranks per node, CPU binding, and number of CPUs per rank.\n\n7. **Final Answer**:\n   <ANSWER>: The options for CPU binding in MPI applications using mpiexec include `--cpu-bind CPU binding for application` and `--depth number of cpus per rank (useful with --cpu-bind)`."
    },
    {
        "id": "data/md/polaris/running-jobs.md_seed_task_6_0",
        "context": [
            [
                "```\n\nSpecial notes for Polaris\n\nOn Polaris, there is a known bug where python applications launched with mpi and that use fork to spawn processes can sometimes have unexplaned hangs.  For this reason, it is recommended to use start_method=\"spawn\" on Polaris when using the MpiExecLauncher as is shown in the example config above.  Alternatively, another solution is to use the GNUParallelLauncher which uses GNU Parallel to spawn processes.  GNU Parallel can be loaded in your environment with the command module load gnu-parallel.  Both of these approaches will circumvent the hang issue from using fork.\n\nUpdates\n\nFor parsl versions after July 2023, the address passed in the HighThroughputExecutor needs to be set to address = address_by_interface(\"bond0\").  With parsl versions prior to July 2023, it was recommended to use address = address_by_hostname() on Polaris, but with later versions this will not work on Polaris (or any other machine).",
                "data/md/polaris/workflows/parsl.md"
            ],
            [
                "julia\njulia> ] up\n\n```julia\nusing CUDA\nusing HDF5\nusing MPI\nusing Printf\nusing Random\n\nfunction pi_kernel(x, y, d, n)\n    idx = (blockIdx().x-1) * blockDim().x + threadIdx().x\n    if idx <= n\n        d[idx] = (x[idx] - 0.5)^2 + (y[idx] - 0.5)^2 <= 0.25 ? 1 : 0\n    end\n    return nothing\nend\n\nfunction approximate_pi_gpu(n::Integer)\n    x = CUDA.rand(Float64, n)\n    y = CUDA.rand(Float64, n)\n    d = CUDA.zeros(Float64, n)\n\nend\n\nfunction main()\n    n = 100000  # Number of points to generate per rank\n    Random.seed!(1234)  # Set a fixed random seed for reproducibility\n\nend\n\nMPI.Init()\nif !isinteractive()\n    pi_approx = main()\n    h5open(\"pi.h5\", \"w\") do file\n        write(file, \"pi\", pi_approx)\n    end\nend\n```\n\nJob submission script\n\nThis example can be run on Polaris with the following job submission script:\n\n```bash\n\n!/bin/bash -l\n\nPBS -l select=1:system=polaris\n\nPBS -l place=scatter\n\nPBS -l walltime=0:30:00\n\nPBS -l filesystems=home:grand\n\nPBS -q debug\n\nPBS -A PROJECT\n\ncd ${PBS_O_WORKDIR}\n\nMPI example w/ 4 MPI ranks per node spread evenly across cores\n\nNNODES=wc -l < $PBS_NODEFILE\nNRANKS_PER_NODE=4\nNDEPTH=8\nNTHREADS=1\n\nSetup Julia environment\n\n. ./setup_env.sh\n\nNTOTRANKS=$(( NNODES * NRANKS_PER_NODE ))\necho \"NUM_OF_NODES= ${NNODES} TOTAL_NUM_RANKS= ${NTOTRANKS} RANKS_PER_NODE=${NRANKS_PER_NODE} THREADS_PER_RANK= ${NTHREADS}\"\n\nEXE=/home/knight/.julia/juliaup/julia-1.10.3+0.x64.linux.gnu/bin/julia",
                "data/md/polaris/data-science-workflows/julia.md"
            ],
            [
                "FFmpeg on Polaris\n\nNOTE: FFmpeg module is currently missing on Polaris after a recent upgrade. A spack build of ffmpeg will be available soon\n\nTo use FFmpeg on Polaris first load the corresponding module:\n\nmodule load ffmpeg\n\nThis is a typical command line to create a movie from a series of snapshots in PNG format:\n\nffmpeg -r 15 -i frames.%03d.png -r 25 -pix_fmt yuv420p movie.mp4\n\nwhere:\n\nr 15 is the input frame rate. Experiment with values smaller than the output frame rate for longer movies.\n-r 25 is the output frame rate (use this value for standard 25 frames per second)\n-i frames.%03d.png reads the input frames in sequence\n-pix_fmt yuv420p is needed for movies to play in browsers\nmovie.mp4 is the resulting movie",
                "data/md/polaris/visualization/ffmpeg.md"
            ],
            [
                "CPP        = nvfortran -Mpreprocess -Mfree -Mextend -E $(CPP_OPTIONS) $$(FUFFIX)  > $$(SUFFIX)\n\nFC         = ftn -acc -gpu=cc80 -mp -target-accel=nvidia80\nFCL        = ftn -acc -gpu=cc80 -c++libs -target-accel=nvidia80\n\nFREE       = -Mfree\n\nFFLAGS     = -Mbackslash -Mlarge_arrays\n\nOFLAG      = -fast\n\nDEBUG      = -Mfree -O0 -traceback\n\nSpecify your NV HPC-SDK installation, try to set NVROOT automatically\n\nNVROOT     =$(shell which nvfortran | awk -F /compilers/bin/nvfortran '{ print $$1 }')\n\n...or set NVROOT manually\n\nNVHPC      ?= /opt/nvidia/hpc_sdk\nNVVERSION  = 23.9\nNVROOT     = $(NVHPC)/Linux_x86_64/$(NVVERSION)\n\nUse NV HPC-SDK provided BLAS and LAPACK libraries\n\nLIBAOCL=/soft/libraries/aocl/3.2.0\nBLAS       = /soft/applications/vasp/aol-libs/3.2/amd-blis/lib/LP64/libblis-mt.a\nLAPACK     = /soft/applications/vasp/aol-libs/3.2/amd-libflame/lib/LP64/libflame.a\n\nBLACS      =\nSCALAPACK  =\n\nSCALAPACK  = -Mscalapack\n\nSCALAPACK  = ${LIBAOCL}/lib/libscalapack.a\n\nCUDA       = -cudalib=cublas,cusolver,cufft,nccl -cuda\n\nLLIBS      = $(SCALAPACK) $(LAPACK) $(BLAS) $(CUDA)\n\nSoftware emulation of quadruple precsion\n\nQD         ?= $(NVROOT)/compilers/extras/qd\nLLIBS      += -L$(QD)/lib -lqdmod -lqd\nINCS       += -I$(QD)/include/qd\n\nINCS       += -I/usr/include/linux\n\nINCS       += -I/usr/include/c++/7/tr1\n\nINCS       += -I/usr/include/c++/7\n\nINCS       += -I/usr/include/x86_64-linux-gnu/c++/7",
                "data/md/polaris/applications-and-libraries/applications/vasp.md"
            ],
            [
                "VisIt on Polaris\n\nGetting Started\n\nThe latest VisIt versions installed on Polaris are 3.3.3 and 3.4.0.\n\nPlease note that at the time of this writing VisIt version 3.4.0 does not yet have a client for Mac available.\n\nFollow these steps to install VisIt on your local machine:\n\nDownload and install VisIt for your local platform (MacOS, Windows, Linux). The version you download must match the server version installed on Polaris. Use this page\n\nDownload the Polaris host profile for VisIt (you may need to right-click and choose \"Save link as...\" or \"Save target as...\")\n\nCopy this file to a file called ~/.visit/hosts/host_anl_polaris.xml on Mac or Linux. [ We need to also specify this path for for Windows]\n\nNote: VisIt allows the user to download host profiles for ANL, but all these settings are outdated. We are working with the VisIt developers to update the ANL host list.\n\nAdditional information for using VisIt in client/server mode here\n\nRunning VisIt\n\nStart up VisIt on your local machine\n\nClick File -> Open File and choose \"ANL Polaris\" from the \"Host\" dropdown\n\nYou'll be prompted for your password; enter your ALCF authenticator app response",
                "data/md/polaris/visualization/visit.md"
            ]
        ],
        "golden_context": "--cpu-bind CPU binding for application\n\n--depth number of cpus per rank (useful with --cpu-bind)\n\n--env set environment variables (--env OMP_NUM_THREADS=2)\n\n--hostfile indicate file with hostnames (the default is --hostfile $PBS_NODEFILE)\n\nA sample submission script with directives is below for a 4-node job with 8 MPI ranks on each node and 8 OpenMP threads per rank. Each hardware thread runs a single OpenMP thread since there are 64 hardware threads on the CPU (2 per core).\nYou can download and compile hello_affinity from this link.\n\n```bash\n\n!/bin/bash -l\n\nPBS -N AFFINITY\n\nPBS -l select=4:ncpus=256\n\nPBS -l walltime=0:10:00\n\nPBS -q debug-scaling\n\nPBS -A Catalyst  # Replace with your project\n\nNNODES=wc -l < $PBS_NODEFILE\nNRANKS=8 # Number of MPI ranks to spawn per node\nNDEPTH=8 # Number of hardware threads per rank (i.e. spacing between MPI ranks)\nNTHREADS=8 # Number of software threads per rank to launch (i.e. OMP_NUM_THREADS)\n\nNTOTRANKS=$(( NNODES * NRANKS ))\n\necho \"NUM_OF_NODES= ${NNODES} TOTAL_NUM_RANKS= ${NTOTRANKS} RANKS_PER_NODE= ${NRANKS} THREADS_PER_RANK= ${NTHREADS}\"\n\nChange the directory to work directory, which is the directory you submit the job.\n\ncd $PBS_O_WORKDIR\nmpiexec --np ${NTOTRANKS} -ppn ${NRANKS} -d ${NDEPTH} --cpu-bind depth -env OMP_NUM_THREADS=${NTHREADS} ./hello_affinity\n```\n\nRunning GPU-enabled Applications",
        "question": "How can you set environment variables using directives in a job submission script? ?",
        "cot_answer": "To set environment variables using directives in a job submission script, specifically for running GPU-enabled applications, you can follow these steps based on the provided context:\n\n1. Identify the relevant directive for setting environment variables: In the context, the directive \"--env\" is used to set environment variables. For example, \"--env OMP_NUM_THREADS=2\" sets the environment variable OMP_NUM_THREADS to 2.\n\n2. Determine the environment variable needed for GPU-enabled applications: When running GPU-enabled applications, you may need to set environment variables related to GPU configuration or libraries.\n\n3. Modify the job submission script to include the necessary environment variable for GPU-enabled applications: Update the script to include the required environment variable using the \"--env\" directive. For example, if you need to set an environment variable for GPU device selection, you can add a line like \"--env CUDA_VISIBLE_DEVICES=0\" to the mpiexec command in the script.\n\n4. Ensure proper syntax and placement: When adding the environment variable using the \"--env\" directive, make sure to follow the correct syntax (e.g., \"--env VARIABLE_NAME=value\") and place it appropriately within the mpiexec command in the script.\n\n<ANSWER>: To set environment variables using directives in a job submission script for running GPU-enabled applications, use the \"--env\" directive followed by the desired environment variable and its value within the mpiexec command in the script."
    },
    {
        "id": "data/md/polaris/running-jobs.md_seed_task_6_1",
        "context": [
            [
                "bash\nqsub -I -A <Project> -q debug -l select=1 -l walltime=01:00:00 -l filesystems=home:eagle -l singularity_fakeroot=true\nexport HTTP_PROXY=http://proxy.alcf.anl.gov:3128\nexport HTTPS_PROXY=http://proxy.alcf.anl.gov:3128\nexport http_proxy=http://proxy.alcf.anl.gov:3128\nexport https_proxy=http://proxy.alcf.anl.gov:3128\nmodule use /soft/spack/gcc/0.6.1/install/modulefiles/Core\nmodule load apptainer\napptainer build --fakeroot pytorch:22.06-py3.sing docker://nvcr.io/nvidia/pytorch:22.06-py3\nYou can find the latest prebuilt Nvidia PyTorch containers here.  The Tensorflow containers are here (though note that LCF doesn't prebuild the TF-1 containers typically).  You can search the full container registry here. For custom containers tailored for Polaris, visit ALCF's GitHub container registry\n\nNote: Currently container build and executions are only supported on the Polaris compute nodes\n\nRecipe-Based Container Building\n\nAs mentioned earlier, you can build Apptainer containers from recipe files. Instructions are available here. See available containers for more recipes.\n\nNote: You can also build custom recipes by bootstrapping from prebuilt images. For e.g the first two lines in a recipe to use our custom Tensorflow implementation would be Bootstrap: oras followed by From: ghcr.io/argonne-lcf/tf2-mpich-nvidia-gpu:latest\n\nRunning Containers on Polaris",
                "data/md/polaris/data-science-workflows/containers/containers.md"
            ],
            [
                "Helper Scripts\n\nALCF/train-gpt3.sh\n\n:   Main entry point for training. This script will automatically source the\n    rest of the required ALCF/*.sh scripts below\n\nALCF/model.sh\n\n:   Contains some example model architectures for GPT3-style models\n\nALCF/args.sh\n\n:   Logic for parsing / setting up runtime options for Megatron and DeepSpeed.\n\nALCF/setup.sh\n\n:   Locate and activate virtual environment to be used, ensure MPI\n    variables are set properly\n\nALCF/launch.sh\n\n:   Identify available resources and build the command to be ran i.e.\n    figure out how many: {nodes, GPUs per node, GPUs total}, to pass to\n    mpi{run,exec} then, use this to build  mpiexec <mpiexec-args> python3\n    pretrain_gpt.py",
                "data/md/polaris/data-science-workflows/applications/megatron-deepspeed.md"
            ],
            [
                "Install parsl (only once)\n\npip install parsl\n\n```\n\nUsing Parsl on Polaris\n\nParsl has a variety of possible configuration settings.  As an example, we provide the configuration below that will run one task per GPU:\n\n```python\nfrom parsl.config import Config\n\nPBSPro is the right provider for Polaris:\n\nfrom parsl.providers import PBSProProvider\n\nThe high throughput executor is for scaling to HPC systems:\n\nfrom parsl.executors import HighThroughputExecutor\n\nYou can use the MPI launcher, but may want the Gnu Parallel launcher, see below\n\nfrom parsl.launchers import MpiExecLauncher, GnuParallelLauncher\n\naddress_by_interface is needed for the HighThroughputExecutor:\n\nfrom parsl.addresses import address_by_interface\n\nFor checkpointing:\n\nfrom parsl.utils import get_all_checkpoints\n\nAdjust your user-specific options here:\n\nrun_dir=\"/lus/grand/projects/yourproject/yourrundir/\"",
                "data/md/polaris/workflows/parsl.md"
            ],
            [
                "Known Issues\n\nThis is a collection of known issues that have been encountered on Polaris. Documentation will be updated as issues are resolved. Users are encouraged to email support@alcf.anl.gov to report issues.\n\nSubmitting Jobs\n\nFor batch job submissions, if the parameters within your submission script do not meet the parameters of any of the execution queues (small, ..., backfill-large) you might not receive the \"Job submission\" error on the command line at all, and the job will never appear in history qstat -xu <username> (current bug in PBS). E.g. if a user submits a script to the prod routing queue requesting 10 nodes for 24 hours, exceeding \"Time Max\" of 6 hrs of the small execution queue (which handles jobs with 10-24 nodes), then it may behave as if the job was never submitted.\n\nJob scripts are copied to temporary locations after qsub and any changes to the original script while the job is queued will not be reflected in the copied script. Furthermore, qalter requires -A <allocation name> when changing job properties. Currently, there is a request for a qalter-like command to trigger a re-copy of the original script to the temporary location.\n\nCompiling & Running Applications\n\nIf your job fails to start with an RPC launch message like below, please forward the complete messages to support@alcf.anl.gov.",
                "data/md/polaris/known-issues.md"
            ],
            [
                "Cabana\n\nCabana\n\nCabana is built atop Kokkos. It provides class templates useful for\nimplementing particle codes\n\nCabana Documentation\n\nCabana Wiki\n\nCabana github\n\nCabana on Polaris\n\nFollowing the Polaris upgrade to HPCM\n1.10,\nthe module setup to use the prebuilt Kokkos changed.\n\nBuilt against the prebuilt Kokkos on\npolaris, the prebuilt Cabana\nincludes 3 backends: Serial and OpenMP for CPU execution and CUDA for GPU\nexecution. To use it, run\n\nmodule load craype-x86-milan\nmodule load craype-accel-nvidia80\nmodule swap PrgEnv-nvhpc PrgEnv-gnu\nmodule use /soft/modulefiles\nmodule load cuda-PrgEnv-nvidia/12.2.91\nmodule load kokkos cabana\n\nCabana is a headers-only package; there are no actual libraries installed.",
                "data/md/polaris/applications-and-libraries/libraries/cabana-polaris.md"
            ]
        ],
        "golden_context": "--cpu-bind CPU binding for application\n\n--depth number of cpus per rank (useful with --cpu-bind)\n\n--env set environment variables (--env OMP_NUM_THREADS=2)\n\n--hostfile indicate file with hostnames (the default is --hostfile $PBS_NODEFILE)\n\nA sample submission script with directives is below for a 4-node job with 8 MPI ranks on each node and 8 OpenMP threads per rank. Each hardware thread runs a single OpenMP thread since there are 64 hardware threads on the CPU (2 per core).\nYou can download and compile hello_affinity from this link.\n\n```bash\n\n!/bin/bash -l\n\nPBS -N AFFINITY\n\nPBS -l select=4:ncpus=256\n\nPBS -l walltime=0:10:00\n\nPBS -q debug-scaling\n\nPBS -A Catalyst  # Replace with your project\n\nNNODES=wc -l < $PBS_NODEFILE\nNRANKS=8 # Number of MPI ranks to spawn per node\nNDEPTH=8 # Number of hardware threads per rank (i.e. spacing between MPI ranks)\nNTHREADS=8 # Number of software threads per rank to launch (i.e. OMP_NUM_THREADS)\n\nNTOTRANKS=$(( NNODES * NRANKS ))\n\necho \"NUM_OF_NODES= ${NNODES} TOTAL_NUM_RANKS= ${NTOTRANKS} RANKS_PER_NODE= ${NRANKS} THREADS_PER_RANK= ${NTHREADS}\"\n\nChange the directory to work directory, which is the directory you submit the job.\n\ncd $PBS_O_WORKDIR\nmpiexec --np ${NTOTRANKS} -ppn ${NRANKS} -d ${NDEPTH} --cpu-bind depth -env OMP_NUM_THREADS=${NTHREADS} ./hello_affinity\n```\n\nRunning GPU-enabled Applications",
        "question": "What is the purpose of the `--depth` directive in a job submission script? ?",
        "cot_answer": "To understand the purpose of the `--depth` directive in the job submission script, we need to follow these steps:\n\n1. **Understand the context**:\n   - The script is for a job submission with MPI ranks and OpenMP threads.\n   - The `--depth` directive is used in conjunction with `--cpu-bind` and specifies the number of CPUs per rank.\n   - The script aims to run a job on 4 nodes with specific configurations for MPI ranks and OpenMP threads.\n\n2. **Examine the script**:\n   - The script sets the `NDEPTH` variable to 8, which represents the number of hardware threads per rank.\n   - The `mpiexec` command includes the `-d ${NDEPTH}` option, which specifies the depth to use with `--cpu-bind`.\n\n3. **Interpret the script**:\n   - In this context, `--depth` is used to define the spacing between MPI ranks in terms of hardware threads per rank.\n   - By setting `NDEPTH` to 8 and passing it to `mpiexec`, the script ensures that there are 8 hardware threads per rank.\n\n4. **Final Answer**:\n   <ANSWER>: The `--depth` directive in the job submission script specifies the number of hardware threads per rank, which helps determine the spacing between MPI ranks in the job execution."
    },
    {
        "id": "data/md/polaris/running-jobs.md_seed_task_6_2",
        "context": [
            [
                "chmod +x script.sh\nqsub script.sh\n\nKnown issues versions: >= 6.4.x in Polaris (OLD)\n\nUndefined MPIX_Query_cuda_support function at linking binary: This function is called in src/openacc.F. The  MPIX_Query_cuda_support is not included incray-mpich. One workaround to this\nissue is to comment this function call.\nSee the follow suggested changes marked by !!!!!CHANGE HERE in the file:src/openacc.F\n\n```fortran\n+!!!!!CHANGE HERE \n-      INTERFACE\n-        INTEGER(c_int) FUNCTION MPIX_Query_cuda_support() BIND(C, name=\"MPIX_Query_cuda_support\")\n-        END FUNCTION\n-      END INTERFACE",
                "data/md/polaris/applications-and-libraries/applications/vasp.md"
            ],
            [
                "Prebuilt Gromacs binaries can be found in the directory /soft/applications/Gromacs/gromacs-2022.1.\n\nA sample pbs script follows that will run GROMACS on two nodes, using 4 MPI ranks per node, and each rank with four OpenMP threads. The PME kernel owns one MPI rank and one GPU per node, while the nonbonded kernel uses 3 MPI ranks and 3 GPUs per node.\n\n```\n\n!/bin/sh\n\nPBS -l select=2:system=polaris\n\nPBS -l place=scatter\n\nPBS -l walltime=0:30:00\n\nPBS -q debug\n\nPBS -A PROJECT\n\nPBS -l filesystems=home:grand:eagle\n\ncd ${PBS_O_WORKDIR}\n\nmodule swap PrgEnv-nvhpc PrgEnv-gnu\nmodule load cudatoolkit-standalone/11.2.2\n\nexport OMP_NUM_THREADS=4\n\nmpirun --np 8 /soft/applications/Gromacs/gromacs-2022.1/gmx_mpi \\\n      mdrun -gputasks 0123 -nb gpu -pme gpu -npme 1 -ntomp 4 \\\n      -dlb yes -resethway -pin on -v deffnm step5_1 -g test.log\n```\n\nWe strongly suggest that users try combinations of different numbers of nodes, MPI ranks per node, number of GPU tasks/devices, GPU task decomposition between nonbonded and PME kernels, and OMP threads per rank to find the optimal throughput for their particular workload.",
                "data/md/polaris/applications-and-libraries/applications/gromacs.md"
            ],
            [
                "```\n$ qsub -I -l select=1,walltime=1:00:00,filesystems=home:grand:eagle -A [PROJECT] -q debug\n\n$ julia --project -e \"using CUDA; CUDA.versioninfo()\"\nCUDA runtime 12.4, artifact installation\nCUDA driver 12.4\nNVIDIA driver 535.154.5, originally for CUDA 12.2\n\nCUDA libraries: \n- CUBLAS: 12.2.1\n- CURAND: 10.3.5\n- CUFFT: 11.2.1\n- CUSOLVER: 11.6.1\n- CUSPARSE: 12.3.1\n- CUPTI: 22.0.0\n- NVML: 12.0.0+535.154.5\n\nJulia packages: \n- CUDA: 5.3.3\n- CUDA_Driver_jll: 0.8.1+0\n- CUDA_Runtime_jll: 0.12.1+0\n\nToolchain:\n- Julia: 1.10.3\n- LLVM: 15.0.7\n\n4 devices:\n  0: NVIDIA A100-SXM4-40GB (sm_80, 39.390 GiB / 40.000 GiB available)\n  1: NVIDIA A100-SXM4-40GB (sm_80, 39.390 GiB / 40.000 GiB available)\n  2: NVIDIA A100-SXM4-40GB (sm_80, 39.390 GiB / 40.000 GiB available)\n  3: NVIDIA A100-SXM4-40GB (sm_80, 39.390 GiB / 40.000 GiB available)\n```\n\nOne can then switch between versions of CUDA as needed. Note, the following commands were executed in an interactive job on a compute node.\n\n$ julia --project -e \"using CUDA; CUDA.set_runtime_version!(local_toolkit=true)\"\n[ Info: Configure the active project to use the default CUDA from the local system; please re-start Julia for this to take effect.\n\n```\n$ julia --project -e \"using CUDA; CUDA.versioninfo()\"\nCUDA runtime 12.2, local installation\nCUDA driver 12.4\nNVIDIA driver 535.154.5, originally for CUDA 12.2\n\nCUDA libraries: \n- CUBLAS: 12.2.1\n- CURAND: 10.3.3\n- CUFFT: 11.0.8\n- CUSOLVER: 11.5.0\n- CUSPARSE: 12.1.1\n- CUPTI: 20.0.0\n- NVML: 12.0.0+535.154.5",
                "data/md/polaris/data-science-workflows/julia.md"
            ],
            "--cpu-bind CPU binding for application\n\n--depth number of cpus per rank (useful with --cpu-bind)\n\n--env set environment variables (--env OMP_NUM_THREADS=2)\n\n--hostfile indicate file with hostnames (the default is --hostfile $PBS_NODEFILE)\n\nA sample submission script with directives is below for a 4-node job with 8 MPI ranks on each node and 8 OpenMP threads per rank. Each hardware thread runs a single OpenMP thread since there are 64 hardware threads on the CPU (2 per core).\nYou can download and compile hello_affinity from this link.\n\n```bash\n\n!/bin/bash -l\n\nPBS -N AFFINITY\n\nPBS -l select=4:ncpus=256\n\nPBS -l walltime=0:10:00\n\nPBS -q debug-scaling\n\nPBS -A Catalyst  # Replace with your project\n\nNNODES=wc -l < $PBS_NODEFILE\nNRANKS=8 # Number of MPI ranks to spawn per node\nNDEPTH=8 # Number of hardware threads per rank (i.e. spacing between MPI ranks)\nNTHREADS=8 # Number of software threads per rank to launch (i.e. OMP_NUM_THREADS)\n\nNTOTRANKS=$(( NNODES * NRANKS ))\n\necho \"NUM_OF_NODES= ${NNODES} TOTAL_NUM_RANKS= ${NTOTRANKS} RANKS_PER_NODE= ${NRANKS} THREADS_PER_RANK= ${NTHREADS}\"\n\nChange the directory to work directory, which is the directory you submit the job.\n\ncd $PBS_O_WORKDIR\nmpiexec --np ${NTOTRANKS} -ppn ${NRANKS} -d ${NDEPTH} --cpu-bind depth -env OMP_NUM_THREADS=${NTHREADS} ./hello_affinity\n```\n\nRunning GPU-enabled Applications",
            [
                "Julia\n\nJulia is a high-level, high-performance dynamic programming language for\ntechnical computing. It has a syntax familiar to users of many other\ntechnical computing environments. Designed at MIT to tackle large-scale\npartial-differential equation simulation and distributed linear algebra, Julia\nfeatures a robust ecosystem of tools for\noptimization,\nstatistics, parallel programming, and data\nvisualization. Julia is actively developed by the Julia\nLabs team at MIT and in\nindustry, along with hundreds of domain-expert\nscientists and programmers worldwide.\n\nContributing\n\nThis guide is a first draft of the Julia documentation for Polaris. If you have any\nsuggestions or contributions, please open a pull request or contact us by\nopening a ticket at the ALCF Helpdesk.\n\nJulia Installation\n\nWe encourage users interested in using Julia on Polaris to install in their home or project directories at this time. Using the official Julia 1.9 binaries from the Julia\nwebpage is recommended.\nJuliaup provides a convenient way to\ninstall Julia and manage the various Julia versions. The default installation will install julia, juliaup, and other commands in a ${HOME}/.julia directory and update profile files like .bashrc to update PATH to include that directory. One can customize the installation to change these defaults.\n\nbash\nmodule load craype-accel-nvidia80\ncurl -fsSL https://install.julialang.org | sh",
                "data/md/polaris/data-science-workflows/julia.md"
            ]
        ],
        "golden_context": "--cpu-bind CPU binding for application\n\n--depth number of cpus per rank (useful with --cpu-bind)\n\n--env set environment variables (--env OMP_NUM_THREADS=2)\n\n--hostfile indicate file with hostnames (the default is --hostfile $PBS_NODEFILE)\n\nA sample submission script with directives is below for a 4-node job with 8 MPI ranks on each node and 8 OpenMP threads per rank. Each hardware thread runs a single OpenMP thread since there are 64 hardware threads on the CPU (2 per core).\nYou can download and compile hello_affinity from this link.\n\n```bash\n\n!/bin/bash -l\n\nPBS -N AFFINITY\n\nPBS -l select=4:ncpus=256\n\nPBS -l walltime=0:10:00\n\nPBS -q debug-scaling\n\nPBS -A Catalyst  # Replace with your project\n\nNNODES=wc -l < $PBS_NODEFILE\nNRANKS=8 # Number of MPI ranks to spawn per node\nNDEPTH=8 # Number of hardware threads per rank (i.e. spacing between MPI ranks)\nNTHREADS=8 # Number of software threads per rank to launch (i.e. OMP_NUM_THREADS)\n\nNTOTRANKS=$(( NNODES * NRANKS ))\n\necho \"NUM_OF_NODES= ${NNODES} TOTAL_NUM_RANKS= ${NTOTRANKS} RANKS_PER_NODE= ${NRANKS} THREADS_PER_RANK= ${NTHREADS}\"\n\nChange the directory to work directory, which is the directory you submit the job.\n\ncd $PBS_O_WORKDIR\nmpiexec --np ${NTOTRANKS} -ppn ${NRANKS} -d ${NDEPTH} --cpu-bind depth -env OMP_NUM_THREADS=${NTHREADS} ./hello_affinity\n```\n\nRunning GPU-enabled Applications",
        "question": "How does the `--cpu-bind` directive work for CPU binding in a job submission script? ?",
        "cot_answer": "To answer the question on how the `--cpu-bind` directive works for CPU binding in a job submission script, we need to follow these steps:\n\n1. **Understand the context**: The context provides a sample submission script for a job with CPU binding directives, including `--cpu-bind`, `--depth`, and `--env`.\n\n2. **Examine the script**: \n   - The script sets the number of MPI ranks per node, hardware threads per rank, and software threads per rank.\n   - It then calculates the total number of ranks based on the number of nodes.\n   - Finally, it executes the job using `mpiexec` with options like `--np`, `-ppn`, `-d`, `--cpu-bind`, and `--env`.\n\n3. **Focus on the `--cpu-bind` directive**:\n   - In the script, `--cpu-bind depth` is used. This option binds the MPI ranks to specific CPUs based on the depth specified.\n   - The depth value determines the number of CPUs per rank. In this case, `NDEPTH=8` is set, meaning there are 8 CPUs per rank.\n\n4. **Final Answer**:\n   - <ANSWER>: The `--cpu-bind` directive with `depth` in the job submission script binds MPI ranks to specific CPUs based on the depth specified, which in this case is 8 CPUs per rank."
    },
    {
        "id": "data/md/polaris/running-jobs.md_seed_task_7_0",
        "context": [
            [
                "PyTorch Best Practices on Polaris\n\nSingle Node Performance\n\nWhen running PyTorch applications, we have found the following practices to be generally, if not universally, useful and encourage you to try some of these techniques to boost performance of your own applications.\n\nUse Reduced Precision. Reduced Precision is available on A100 via tensorcores and is supported with PyTorch operations.  In general, the way to do this is via the PyTorch Automatic Mixed Precision package (AMP), as descibed in the mixed precision documentation.  In PyTorch, users generally need to manage casting and loss scaling manually,  though context managers and function decorators can provide easy tools to do this.\n\nPyTorch has a JIT module as well as backends to support op fusion, similar to TensorFlow's tf.function tools.  However, PyTorch JIT capabilities are newer and may not yield performance improvements.  Please see TorchScript for more information.\n\nMulti-GPU / Multi-Node Scale up\n\nPyTorch is compatible with scaling up to multiple GPUs per node, and across multiple nodes.  Good scaling performance has been seen up to the entire Polaris system, > 2048 GPUs.  Good performance with PyTorch has been seen with both DDP and Horovod.  For details, please see the Horovod documentation or the Distributed Data Parallel documentation.  Some Polaris-specific details that may be helpful to you:",
                "data/md/polaris/data-science-workflows/frameworks/pytorch.md"
            ],
            [
                "Kokkos\n\nKokkos\n\nKokkos Core implements a programming model in C++ for writing performance\nportable applications targeting all major HPC platforms. For that purpose it\nprovides abstractions for both parallel execution of code and data\nmanagement. Kokkos is designed to target complex node architectures with\nN-level memory hierarchies and multiple types of execution resources. It\ncurrently can use Serial and OpenMP (threads) for CPU execution spaces\n(\"backends\") and CUDA, HIP, SYCL, and OpenMPTarget for GPU execution\nspaces. By convention, Kokkos only allows one GPU backend at a time.\n\nKokkos Documentation\n\nKokkos-core Wiki\n\nKokkos github\n\nKokkos on Polaris\n\nFollowing the Polaris upgrade to HPCM\n1.10,\nthe module setup to use the prebuilt Kokkos changed.\n\nThe prebuilt Kokkos on polaris includes 3 backends: Serial and OpenMP for CPU\nexecution and CUDA for GPU execution. To use it, run\n\nmodule load craype-x86-milan\nmodule load craype-accel-nvidia80\nmodule swap PrgEnv-nvhpc PrgEnv-gnu\nmodule use /soft/modulefiles\nmodule load cuda-PrgEnv-nvidia/12.2.91\nmodule load kokkos\n\nThis sets the following environment variables, some of which are used by\ncmake:\n\nKOKKOS_HOME - path to the lib64/, include/ files installed\n\nLIBRARY_PATH - prepends $KOKKOS_HOME/lib64 to this variable used by cmake\n\nCPATH - prepends $KOKKOS_HOME/include to this variable used by cmake\n\nLD_LIBRARY_PATH - prepends $KOKKOS_HOME/lib64 to this variable\n\nBuilding a Kokkos Application Using cmake",
                "data/md/polaris/programming-models/kokkos-polaris.md"
            ],
            [
                "For PrgEnv-cray, after loading the modules as discussed above we would use:\n\nCC -fopenmp hello.cpp\nftn -fopenmp hello.F90\n\nRunning on Polaris\n\nTo run, you can run the produced executable or with mpiexec in a job script, and then submit the script to the Polaris queue, like:\n\n```\n$ cat submit.sh\n\n!/bin/sh\n\nPBS -l select=1:system=polaris\n\nPBS -l walltime=0:30:00\n\nPBS -q debug\n\nPBS -A Catalyst\n\nPBS -l filesystems=home:eagle\n\ncd ${PBS_O_WORKDIR}\n mpiexec -n 1 ./executable\n$ # submit to the queue:\n$ qsub -l select=1:system=polaris -l walltime=0:30:00 -l filesystems=home:eagle -q debug -A Catalyst ./submit.sh\n```\n\nIn the above, having the PBS options in the script and on the command line is redundant, but we put it there to show both ways of launching. This submits the script to one node in the debug queue on Polaris, requesting 30 min and the eagle and home filesystems. It will charge project Catalyst for the time.\n\nMore details for setting up the job script are in Job Scheduling and Execution section.\n\nExample\n\n```\n$ cat hello.cpp\n\ninclude\n\ninclude\n\nint main( int argv, char** argc ) {\n\nprintf( \"Number of devices: %d\\n\", omp_get_num_devices() );\n\n#pragma omp target\n  {\n    if( !omp_is_initial_device() )\n      printf( \"Hello world from accelerator.\\n\" );\n    else\n      printf( \"Hello world from host.\\n\" );\n  }\n  return 0;\n}\n\n$ cat hello.F90\nprogram  main\n  use omp_lib\n  implicit none\n  integer flag\n\nwrite(,) \"Number of devices:\", omp_get_num_devices()",
                "data/md/polaris/programming-models/openmp-polaris.md"
            ],
            [
                "Additional Information\n\nParaView Documentation\n\nParaView Community Support",
                "data/md/polaris/visualization/paraview.md"
            ],
            "Running GPU-enabled Applications\n\nGPU-enabled applications will similarly run on the compute nodes using the above example script.\n- The environment variable MPICH_GPU_SUPPORT_ENABLED=1 needs to be set if your application requires MPI-GPU support whereby the MPI library sends and receives data directly from GPU buffers. In this case, it will be important to have the craype-accel-nvidia80 module loaded both when compiling your application and during runtime to correctly link against a GPU Transport Layer (GTL) MPI library. Otherwise, you'll likely see GPU_SUPPORT_ENABLED is requested, but GTL library is not linked errors during runtime.\n- If running on a specific GPU or subset of GPUs is desired, then the CUDA_VISIBLE_DEVICES environment variable can be used. For example, if one only wanted an application to access the first two GPUs on a node, then setting CUDA_VISIBLE_DEVICES=0,1 could be used.\n\nBinding MPI ranks to GPUs\n\nThe Cray MPI on Polaris does not currently support binding MPI ranks to GPUs. For applications that need this support, this instead can be handled by use of a small helper script that will appropriately set CUDA_VISIBLE_DEVICES for each MPI rank. One example is available here where each MPI rank is similarly bound to a single GPU with round-robin assignment.\n\nA example set_affinity_gpu_polaris.sh script follows where GPUs are assigned round-robin to MPI ranks.\n\n```bash\n\n!/bin/bash -l\n\nnum_gpus=4\n\nneed to assign GPUs in reverse order due to topology"
        ],
        "golden_context": "Running GPU-enabled Applications\n\nGPU-enabled applications will similarly run on the compute nodes using the above example script.\n- The environment variable MPICH_GPU_SUPPORT_ENABLED=1 needs to be set if your application requires MPI-GPU support whereby the MPI library sends and receives data directly from GPU buffers. In this case, it will be important to have the craype-accel-nvidia80 module loaded both when compiling your application and during runtime to correctly link against a GPU Transport Layer (GTL) MPI library. Otherwise, you'll likely see GPU_SUPPORT_ENABLED is requested, but GTL library is not linked errors during runtime.\n- If running on a specific GPU or subset of GPUs is desired, then the CUDA_VISIBLE_DEVICES environment variable can be used. For example, if one only wanted an application to access the first two GPUs on a node, then setting CUDA_VISIBLE_DEVICES=0,1 could be used.\n\nBinding MPI ranks to GPUs\n\nThe Cray MPI on Polaris does not currently support binding MPI ranks to GPUs. For applications that need this support, this instead can be handled by use of a small helper script that will appropriately set CUDA_VISIBLE_DEVICES for each MPI rank. One example is available here where each MPI rank is similarly bound to a single GPU with round-robin assignment.\n\nA example set_affinity_gpu_polaris.sh script follows where GPUs are assigned round-robin to MPI ranks.\n\n```bash\n\n!/bin/bash -l\n\nnum_gpus=4\n\nneed to assign GPUs in reverse order due to topology",
        "question": "Does Cray MPI on Polaris support binding MPI ranks to GPUs?",
        "cot_answer": "To answer the question, we need to follow these steps:\n\n1. **Understand the context**: The context provided explains the support for binding MPI ranks to GPUs on Cray MPI on Polaris. It mentions that Cray MPI on Polaris does not currently support binding MPI ranks to GPUs directly.\n\n2. **Alternative solution**: The context suggests using a helper script to handle binding MPI ranks to GPUs. Specifically, it mentions a script named set_affinity_gpu_polaris.sh that assigns GPUs round-robin to MPI ranks.\n\n3. **Final Answer**: \n<ANSWER> Cray MPI on Polaris does not support binding MPI ranks to GPUs directly. Instead, a helper script like set_affinity_gpu_polaris.sh can be used to assign GPUs round-robin to MPI ranks. </ANSWER>"
    },
    {
        "id": "data/md/polaris/running-jobs.md_seed_task_8_0",
        "context": [
            [
                "When the job starts you will receive a prompt on your head node like this:\n\nusername@x3005c0s7b0n0:~>\n\nMake a note of the node hostname (x3005c0s7b0n0 in the example above). You can also get this information from qstat -fx jobID\n\nNow load the ParaView module\n\nusername@x3005c0s7b0n0:~> module use /soft/modulefiles \nusername@x3005c0s7b0n0:~> module load visualization/paraview/paraview-5.12.0-EGL\n\nand launch the ParaView server with\n\nsrizzi@x3005c0s7b0n0:~> mpirun -n 8 pvserver --server-port=8000\nWaiting for client...\nConnection URL: cs://x3005c0s7b0n0:8000\nAccepting connection(s): x3005c0s7b0n0:8000\n\nIn this case pvserver will be listening on TCP port 8000 of your head node. You can change this port if you want.\n\nCreating a tunnel over ssh\n\nWe need to establish an ssh tunnel to connect client to server. On your local machine open a new terminal and type:\n\nssh -v -N -L 8000:x3005c0s7b0n0:8000 polaris.alcf.anl.gov\n\nwhere 8000 is a TCP port and x3005c0s7b0n0 the name of your head node. Adjust these values accordingly.\n\nAmong multiple lines with debug information,  you should see something like:\n\ndebug1: Local connections to LOCALHOST:8000 forwarded to remote address x3005c0s7b0n0:8000\n\nKeep this terminal open for the duration of your session to keep the ssh tunnel active.\n\nNow you are ready to launch your ParaView client locally. Keep in mind that client and servers versions must match. The ParaView version currently deployed on Polaris is 5.12.0\n\nConnecting to ParaView server",
                "data/md/polaris/visualization/paraview-manual-launch.md"
            ],
            [
                "compiler/bin - cuda-gdb, ncu, nsys, ...\n\nexamples - CUDA-Fortran, OpenMP, ...\n\ncomm_libs - nccl, nvshmem, ...\n\ncompiler/libs - blas, lapack, ...\n\ncuda/lib64 - cudart, OpenCL, ...\n\nmath_libs/lib64 - cublas, cufft, ...\n\nDifferences between nvcc and nvc/nvc++\n\nFor users that want to continue using nvcc it is important to be mindful of differences with the newer nvc and nvc++ compilers. For example, the -cuda flag instructs nvcc to compile .cu input files to .cu.cpp.ii output files which are to be separately compiled, whereas the same -cuda flag instructs nvc, nvc++, and nvfortran to enable CUDA C/C++ or CUDA Fortran code generation. The resulting output file in each case is different (text vs. object) and one may see unrecognized format error when -cuda is incorrectly passed to nvcc.\n\nKnown Issues and Workarounds\n\nIf you are using nvcc to invoke nvc++ and compiling C++17 code, and are seeing the following warning and unable to compile C++17 constructs:\n\n```\npolaris-login-01(~)> nvcc --std=c++17 -ccbin nvc++ ~/smalltests/bool_constant.cpp\nnvcc warning : The -std=c++17 flag is not supported with the configured host compiler. Flag will be ignored.\n\"/home/zippy/smalltests/bool_constant.cpp\", line 10: error: namespace \"std\" has no member class \"bool_constant\"\n      : std::bool_constant<(UnaryPred::value || ...)> {};\n             ^",
                "data/md/polaris/compiling-and-linking/nvidia-compiler-polaris.md"
            ],
            [
                "chmod +x script.sh\nqsub script.sh\n\nKnown issues versions: >= 6.4.x in Polaris (OLD)\n\nUndefined MPIX_Query_cuda_support function at linking binary: This function is called in src/openacc.F. The  MPIX_Query_cuda_support is not included incray-mpich. One workaround to this\nissue is to comment this function call.\nSee the follow suggested changes marked by !!!!!CHANGE HERE in the file:src/openacc.F\n\n```fortran\n+!!!!!CHANGE HERE \n-      INTERFACE\n-        INTEGER(c_int) FUNCTION MPIX_Query_cuda_support() BIND(C, name=\"MPIX_Query_cuda_support\")\n-        END FUNCTION\n-      END INTERFACE",
                "data/md/polaris/applications-and-libraries/applications/vasp.md"
            ],
            [
                "NNODES=wc -l < $PBS_NODEFILE\nNRANKS_PER_NODE=16\nNDEPTH=4\nNTHREADS=1\n\nNTOTRANKS=$(( NNODES * NRANKS_PER_NODE ))\necho \"NUM_OF_NODES= ${NNODES} TOTAL_NUM_RANKS= ${NTOTRANKS} RANKS_PER_NODE= ${NRANKS_PER_NODE} THREADS_PER_RANK= ${NTHREADS}\"\n\nmpiexec -n ${NTOTRANKS} --ppn ${NRANKS_PER_NODE} --depth=${NDEPTH} --cpu-bind depth ./hello_affinity\n```\n\nCUDA\n\nSeveral variants of C/C++ and Fortran CUDA examples are available here that include MPI and multi-gpu examples.\n\nOne can use the Cray compiler wrappers to compile GPU-enabled applications as well. This example of simple vector addition uses the NVIDIA compilers.\n\nCC -g -O3 -std=c++0x -cuda main.cpp -o vecadd\n\nThe craype-accel-nvidia80 module in the default environment will add the -gpu compiler flag for nvhpc compilers along with appropriate include directories and libraries. It is left to the user to provide an additional flag to the nvhpc compilers to select the target GPU programming model. In this case, -cuda is used to indicate compilation of CUDA code. The application can then be launched within a batch job submission script or as follows on one of the compute nodes.\n\n```\n$ ./vecadd\n\nof devices= 4\n\n[0] Platform[ Nvidia ] Type[ GPU ] Device[ NVIDIA A100-SXM4-40GB ]\n  [1] Platform[ Nvidia ] Type[ GPU ] Device[ NVIDIA A100-SXM4-40GB ]\n  [2] Platform[ Nvidia ] Type[ GPU ] Device[ NVIDIA A100-SXM4-40GB ]\n  [3] Platform[ Nvidia ] Type[ GPU ] Device[ NVIDIA A100-SXM4-40GB ]\nRunning on GPU 0!\nUsing single-precision",
                "data/md/polaris/compiling-and-linking/polaris-example-program-makefile.md"
            ],
            "```bash\n\n!/bin/bash -l\n\nnum_gpus=4\n\nneed to assign GPUs in reverse order due to topology\n\nSee Polaris Device Affinity Information:\n\nhttps://www.alcf.anl.gov/support/user-guides/polaris/hardware-overview/machine-overview/index.html\n\ngpu=$((${num_gpus} - 1 - ${PMI_LOCAL_RANK} % ${num_gpus}))\nexport CUDA_VISIBLE_DEVICES=$gpu\necho “RANK= ${PMI_RANK} LOCAL_RANK= ${PMI_LOCAL_RANK} gpu= ${gpu}”\nexec \"$@\"\nThis script can be placed just before the executable in the `mpiexec` command like so.bash\nmpiexec -n ${NTOTRANKS} --ppn ${NRANKS_PER_NODE} --depth=${NDEPTH} --cpu-bind depth ./set_affinity_gpu_polaris.sh ./hello_affinity\n```\nUsers with different needs, such as assigning multiple GPUs per MPI rank, can modify the above script to suit their needs.\n\nInteractive Jobs on Compute Nodes\n\nHere is how to submit an interactive job to, for example, edit/build/test an application Polaris compute nodes:\nqsub -I -l select=1 -l filesystems=home:eagle -l walltime=1:00:00 -q debug -A <project_name>\n\nThis command requests 1 node for a period of 1 hour in the debug queue, requiring access to the /home and eagle filesystems. After waiting in the queue for a node to become available, a shell prompt on a compute node will appear. You may then start building applications and testing gpu affinity scripts on the compute node.\n\nNOTE: If you want to ssh or scp to one of your assigned compute nodes you will need to make sure your $HOME directory and your $HOME/.ssh directory permissions are both set to 700."
        ],
        "golden_context": "```bash\n\n!/bin/bash -l\n\nnum_gpus=4\n\nneed to assign GPUs in reverse order due to topology\n\nSee Polaris Device Affinity Information:\n\nhttps://www.alcf.anl.gov/support/user-guides/polaris/hardware-overview/machine-overview/index.html\n\ngpu=$((${num_gpus} - 1 - ${PMI_LOCAL_RANK} % ${num_gpus}))\nexport CUDA_VISIBLE_DEVICES=$gpu\necho “RANK= ${PMI_RANK} LOCAL_RANK= ${PMI_LOCAL_RANK} gpu= ${gpu}”\nexec \"$@\"\nThis script can be placed just before the executable in the `mpiexec` command like so.bash\nmpiexec -n ${NTOTRANKS} --ppn ${NRANKS_PER_NODE} --depth=${NDEPTH} --cpu-bind depth ./set_affinity_gpu_polaris.sh ./hello_affinity\n```\nUsers with different needs, such as assigning multiple GPUs per MPI rank, can modify the above script to suit their needs.\n\nInteractive Jobs on Compute Nodes\n\nHere is how to submit an interactive job to, for example, edit/build/test an application Polaris compute nodes:\nqsub -I -l select=1 -l filesystems=home:eagle -l walltime=1:00:00 -q debug -A <project_name>\n\nThis command requests 1 node for a period of 1 hour in the debug queue, requiring access to the /home and eagle filesystems. After waiting in the queue for a node to become available, a shell prompt on a compute node will appear. You may then start building applications and testing gpu affinity scripts on the compute node.\n\nNOTE: If you want to ssh or scp to one of your assigned compute nodes you will need to make sure your $HOME directory and your $HOME/.ssh directory permissions are both set to 700.",
        "question": "How can you assign GPUs in reverse order due to topology??",
        "cot_answer": "To assign GPUs in reverse order due to topology, you can follow these steps based on the provided context:\n\n1. Determine the number of GPUs available and the desired order of assignment. In this case, there are 4 GPUs and the script aims to assign them in reverse order due to topology.\n\n2. Use the following script snippet to assign GPUs in reverse order:\n```bash\nnum_gpus=4\ngpu=$((${num_gpus} - 1 - ${PMI_LOCAL_RANK} % ${num_gpus}))\nexport CUDA_VISIBLE_DEVICES=$gpu\n```\n\n3. The calculation `gpu=$((${num_gpus} - 1 - ${PMI_LOCAL_RANK} % ${num_gpus}))` ensures that GPUs are assigned in reverse order based on the `PMI_LOCAL_RANK`.\n\n4. Incorporate this script just before the executable in the `mpiexec` command as shown:\n```bash\nmpiexec -n ${NTOTRANKS} --ppn ${NRANKS_PER_NODE} --depth=${NDEPTH} --cpu-bind depth ./set_affinity_gpu_polaris.sh ./hello_affinity\n```\n\n5. When running the script within an interactive job on a compute node, ensure that the necessary permissions are set for your directories as mentioned in the context.\n\n<ANSWER>: To assign GPUs in reverse order due to topology, use the provided script snippet and incorporate it before the executable in the `mpiexec` command."
    },
    {
        "id": "data/md/polaris/running-jobs.md_seed_task_8_1",
        "context": [
            [
                "--cpu-bind CPU binding for application\n\n--depth number of cpus per rank (useful with --cpu-bind)\n\n--env set environment variables (--env OMP_NUM_THREADS=2)\n\n--hostfile indicate file with hostnames (the default is --hostfile $PBS_NODEFILE)\n\nA sample submission script with directives is below for a 4-node job with 8 MPI ranks on each node and 8 OpenMP threads per rank. Each hardware thread runs a single OpenMP thread since there are 64 hardware threads on the CPU (2 per core).\nYou can download and compile hello_affinity from this link.\n\n```bash\n\n!/bin/bash -l\n\nPBS -N AFFINITY\n\nPBS -l select=4:ncpus=256\n\nPBS -l walltime=0:10:00\n\nPBS -q debug-scaling\n\nPBS -A Catalyst  # Replace with your project\n\nNNODES=wc -l < $PBS_NODEFILE\nNRANKS=8 # Number of MPI ranks to spawn per node\nNDEPTH=8 # Number of hardware threads per rank (i.e. spacing between MPI ranks)\nNTHREADS=8 # Number of software threads per rank to launch (i.e. OMP_NUM_THREADS)\n\nNTOTRANKS=$(( NNODES * NRANKS ))\n\necho \"NUM_OF_NODES= ${NNODES} TOTAL_NUM_RANKS= ${NTOTRANKS} RANKS_PER_NODE= ${NRANKS} THREADS_PER_RANK= ${NTHREADS}\"\n\nChange the directory to work directory, which is the directory you submit the job.\n\ncd $PBS_O_WORKDIR\nmpiexec --np ${NTOTRANKS} -ppn ${NRANKS} -d ${NDEPTH} --cpu-bind depth -env OMP_NUM_THREADS=${NTHREADS} ./hello_affinity\n```\n\nRunning GPU-enabled Applications",
                "data/md/polaris/running-jobs.md"
            ],
            [
                "KOKKOS package and NVHPC compilers\n\nThe following modules are useful for this particular build. Note, the cmake module is not required if using the GNU Makefile procedure to build LAMMPS. The initial module restore is just setting the default environment as the starting point. Users may find it useful to copy these module commands into a small helper script to assist with compiling and running LAMMPS (e.g. setup_lammps_nvhpc.sh).\n\n```\nmodule restore\nmodule load craype-accel-nvidia80\n\nmodule use /soft/modulefiles\nmodule load spack-pe-base cmake\n```\n\nThe top portion of Makefile.polaris_nvhpc_kokkos used to build LAMMPS with the KOKKOS package using the NVHPC compilers is shown as an example.\n\n```\n\npolaris_nvhpc_kokkos = Flags for NVIDIA A100, NVIDIA Compiler, MPICH, CUDA\n\nmake polaris_nvhpc_kokkos -j 16\n\nSHELL = /bin/sh\n\n---------------------------------------------------------------------\n\ncompiler/linker settings\n\nspecify flags and libraries needed for your compiler\n\nKOKKOS_DEVICES = Cuda,OpenMP\nKOKKOS_ARCH = Ampere80\nKOKKOS_ABSOLUTE_PATH = $(shell cd $(KOKKOS_PATH); pwd)\nKOKKOS_CUDA_OPTIONS = \"enable_lambda,disable_malloc_async\"\nexport NVCC_WRAPPER_DEFAULT_COMPILER = nvc++\n\nCRAY_INC = $(shell CC --cray-print-opts=cflags)\nCRAY_LIB = $(shell CC --cray-print-opts=libs)\n\n$(info CRAY_INC = ${CRAY_INC})\n\n$(info CRAY_LIB = ${CRAY_LIB})",
                "data/md/polaris/applications-and-libraries/applications/lammps.md"
            ],
            [
                "Known Issues\n\nThis is a collection of known issues that have been encountered on Polaris. Documentation will be updated as issues are resolved. Users are encouraged to email support@alcf.anl.gov to report issues.\n\nSubmitting Jobs\n\nFor batch job submissions, if the parameters within your submission script do not meet the parameters of any of the execution queues (small, ..., backfill-large) you might not receive the \"Job submission\" error on the command line at all, and the job will never appear in history qstat -xu <username> (current bug in PBS). E.g. if a user submits a script to the prod routing queue requesting 10 nodes for 24 hours, exceeding \"Time Max\" of 6 hrs of the small execution queue (which handles jobs with 10-24 nodes), then it may behave as if the job was never submitted.\n\nJob scripts are copied to temporary locations after qsub and any changes to the original script while the job is queued will not be reflected in the copied script. Furthermore, qalter requires -A <allocation name> when changing job properties. Currently, there is a request for a qalter-like command to trigger a re-copy of the original script to the temporary location.\n\nCompiling & Running Applications\n\nIf your job fails to start with an RPC launch message like below, please forward the complete messages to support@alcf.anl.gov.",
                "data/md/polaris/known-issues.md"
            ],
            [
                "==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes",
                "data/md/polaris/performance-tools/NVIDIA-Nsight.md"
            ],
            [
                "MPI 000 - OMP 000 - HWT 000 - Node x3200c0s37b0n0 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID 0000:C7:00.0\nMPI 001 - OMP 000 - HWT 001 - Node x3200c0s37b0n0 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID 0000:85:00.0\nMPI 003 - OMP 000 - HWT 003 - Node x3200c0s37b0n0 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID 0000:07:00.0\nMPI 002 - OMP 000 - HWT 002 - Node x3200c0s37b0n0 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID 0000:46:00.0\n$ ./a.out\n```\n\nExample (using GPU-aware MPI)\n\n```c++\n\ninclude\n\ninclude\n\ninclude\n\ninclude\n\n// Modified from NERSC website:\n// https://docs.nersc.gov/development/programming-models/mpi\nint main(int argc, char *argv[]) {\n\n}\n```\n\nLoad Modules\n\nbash\nmodule load oneapi/upstream\nmodule load mpiwrappers/cray-mpich-oneapi-upstream\nmodule load craype-accel-nvidia80\nexport MPICH_GPU_SUPPORT_ENABLED=1\n\nCompile and Run\n\nbash\n$ mpicxx -L/opt/cray/pe/mpich/8.1.28/gtl/lib -lmpi_gtl_cuda -std=c++17 -fsycl -fsycl-targets=nvptx64-nvidia-cuda -Xsycl-target-backend --cuda-gpu-arch=sm_80 main.cpp\n$ mpiexec -n 2 --ppn 2 --depth=1 --cpu-bind depth ./set_affinity_gpu_polaris.sh ./a.out\nFor further details regarding the arguments passed to mpiexec command shown above, please visit the Job Scheduling and Execution section. A simple example describing the details and execution of the set_affinity_gpu_polaris.sh file can be found here.",
                "data/md/polaris/programming-models/sycl-polaris.md"
            ]
        ],
        "golden_context": "```bash\n\n!/bin/bash -l\n\nnum_gpus=4\n\nneed to assign GPUs in reverse order due to topology\n\nSee Polaris Device Affinity Information:\n\nhttps://www.alcf.anl.gov/support/user-guides/polaris/hardware-overview/machine-overview/index.html\n\ngpu=$((${num_gpus} - 1 - ${PMI_LOCAL_RANK} % ${num_gpus}))\nexport CUDA_VISIBLE_DEVICES=$gpu\necho “RANK= ${PMI_RANK} LOCAL_RANK= ${PMI_LOCAL_RANK} gpu= ${gpu}”\nexec \"$@\"\nThis script can be placed just before the executable in the `mpiexec` command like so.bash\nmpiexec -n ${NTOTRANKS} --ppn ${NRANKS_PER_NODE} --depth=${NDEPTH} --cpu-bind depth ./set_affinity_gpu_polaris.sh ./hello_affinity\n```\nUsers with different needs, such as assigning multiple GPUs per MPI rank, can modify the above script to suit their needs.\n\nInteractive Jobs on Compute Nodes\n\nHere is how to submit an interactive job to, for example, edit/build/test an application Polaris compute nodes:\nqsub -I -l select=1 -l filesystems=home:eagle -l walltime=1:00:00 -q debug -A <project_name>\n\nThis command requests 1 node for a period of 1 hour in the debug queue, requiring access to the /home and eagle filesystems. After waiting in the queue for a node to become available, a shell prompt on a compute node will appear. You may then start building applications and testing gpu affinity scripts on the compute node.\n\nNOTE: If you want to ssh or scp to one of your assigned compute nodes you will need to make sure your $HOME directory and your $HOME/.ssh directory permissions are both set to 700.",
        "question": "What command would you use to submit an interactive job on Polaris compute nodes??",
        "cot_answer": "To submit an interactive job on Polaris compute nodes, you would use the following command:\n\n```bash\nqsub -I -l select=1 -l filesystems=home:eagle -l walltime=1:00:00 -q debug -A <project_name>\n```\n\nHere is the step-by-step reasoning based on the provided context:\n\n1. **Understand the context**: The context provides information on how to submit an interactive job on Polaris compute nodes for tasks like editing, building, and testing applications.\n   \n2. **Identify the command**: The command to submit an interactive job is clearly stated in the context:\n   \n   - ##begin_quote##\n     qsub -I -l select=1 -l filesystems=home:eagle -l walltime=1:00:00 -q debug -A <project_name>\n     ##end_quote##\n   \n3. **Final Answer**: \n   \n   <ANSWER>: The command to submit an interactive job on Polaris compute nodes is `qsub -I -l select=1 -l filesystems=home:eagle -l walltime=1:00:00 -q debug -A <project_name>`."
    },
    {
        "id": "data/md/polaris/running-jobs.md_seed_task_8_2",
        "context": [
            [
                "Polaris\n\nPolaris is a 560 node HPE Apollo 6500 Gen 10+ based system.  Each node has a single 2.8 GHz AMD EPYC Milan 7543P 32 core CPU with 512 GB of DDR4 RAM and four NVIDIA A100 GPUs connected via NVLink, a pair of local 1.6TB of SSDs in RAID0 for the users use, and a pair of Slingshot 11 network adapters.  There are two nodes per chassis, seven chassis per rack, and 40 racks for a total of 560 nodes.  More detailed specifications are as follows:\n\nPolaris Compute Nodes\n\nPOLARIS COMPUTE DESCRIPTION PER NODE AGGREGATE Processor (Note 1) 2.8 GHz 7543P 1 560 Cores/Threads AMD Zen 3 (Milan) 32/64 17,920/35,840 RAM (Note 2) DDR4 512 GiB 280 TiB GPUS NVIDIA A100 4 2240 Local SSD 1.6 TB 2/3.2 TB 1120/1.8PB\n\nNote 1: 256MB shared L3 cache, 512KB L2 cache per core, 32 KB L1 cache per core\nNote 2: 8 memory channels rated at 204.8 GiB/s\n\nPolaris A100 GPU Information\n\nDESCRIPTION A100 PCIe A100 HGX (Polaris) GPU Memory 40 GiB HBM2 160 GiB HBM2 GPU Memory BW 1.6 TB/s 6.4 TB/s Interconnect PCIe Gen4 64 GB/s NVLink 600 GB/s FP 64 9.7 TF 38.8 TF FP64 Tensor Core 19.5 TF 78 TF FP 32 19.5 TF 78 TF BF16 Tensor Core 312 TF 1.3 PF FP16 Tensor Core 312 TF 1.3 PF INT8 Tensor Core 624 TOPS 2496 TOPS Max TDP Power 250 W 400 W\n\nPolaris Device Affinity Information",
                "data/md/polaris/hardware-overview/machine-overview.md"
            ],
            [
                "Getting Started on Polaris\n\nLogging Into Polaris\n\nTo log into Polaris:\nssh <username>@polaris.alcf.anl.gov\nThen, type in the password from your CRYPTOCard/MobilePASS+ token.\n\nHardware Overview\n\nAn overview of the Polaris system including details on the compute node architecture is available on the Machine Overview page.\n\nCompiling Applications\n\nUsers are encouraged to read through the Compiling and Linking Overview page and corresponding pages depending on the target compiler and programming model.\n\nAccessing Additional Software\n\nIn addition to the Cray PE, ALCF installs software in /soft which can be\naccessed via module commands by altering your $MODULEPATH:\nmodule use /soft/modulefiles\nThe available software can then be queried with module avail.\n\nAdditionally, a suite of software packages are provided via Spack deployments,\ndetailed on the Spack PE\npage.\n\nSubmitting and Running Jobs\n\nUsers are encouraged to read through the Running Jobs with PBS at the ALCF page for information on using the PBS scheduler and preparing job submission scripts. Some example job submission scripts are available on the Example Job Scripts page as well.\n\nLustre File Striping\n\nIn addition to the content above, here is a document on Lustre File Striping Basics.\n\nLustre File Striping Basics\n\nProxy\n\nIf the node you are on doesn’t have outbound network connectivity, add the following to your ~/.bash_profile file to access the proxy host.\n\n```bash\n\nproxy settings",
                "data/md/polaris/getting-started.md"
            ],
            [
                "LAMMPS\n\nOverview\n\nLAMMPS is a general-purpose molecular dynamics software package for massively parallel computers. It is written in an exceptionally clean style that makes it one of the more popular codes for users to extend and it currently has dozens of user-developed extensions.\n\nFor details about the code and its usage, see the LAMMPS home page. This page provides information specific to running on Polaris at the ALCF.\n\nUsing LAMMPS at ALCF\n\nALCF provides assistance with build instructions, compiling executables, submitting jobs, and providing prebuilt binaries (upon request). A collection of Makefiles and submission scripts are available in the ALCF GettingStarted repo here. For questions, contact us at support@alcf.anl.gov.\n\nHow to Obtain the Code\n\nLAMMPS is an open-source code, which can be downloaded from the LAMMPS website.\n\nBuilding on Polaris",
                "data/md/polaris/applications-and-libraries/applications/lammps.md"
            ],
            [
                "cd ${PBS_O_WORKDIR}\n\nmodule load cudatoolkit-standalone/11.4.4\n\npython benchmark.py --platform=CUDA --test=pme --precision=mixed --seconds=30 --heavy-hydrogens > test.output\n```\n\nBuilding OpenMM from Source\n\nUpdate environment\n$ module load cudatoolkit-standalone/11.4.4\n$ module load cray-python/3.9.12.1\n\nDownload OpenMM\n$ git checkout https://github.com/openmm/openmm.git\n$ cd openmm ; mkdir build\n\nDownload and build doxygen\n$ git clone https://github.com/doxygen/doxygen.git\n$ cd doxygen ; cmake ; make ; make install ; cd ../\n\nDownload and install swig in OpenMM directory.\n$ tar xzf swig-4.0.2.tar.gz\n$ cd swig-4.0.2\n$ ./configure --prefix=/path-to/openmm/swig-4.0.2 ; make -j 8 ; make install\n\nBuild OpenMM\n$ cmake -DDOXYGEN_EXECUTABLE=/path-to/openmm/doxygen/bin/doxygen \\\n        -DSWIG_EXECUTABLE=/path-to/openmm/swig-4.0.2/bin/swig \\\n        -DCMAKE_INSTALL_PREFIX=/path-to/openmm/build \\\n         -DCUDA_HOME=/soft/compilers/cudatoolkit/cuda-11.4.4 \\\n         -DCUDA_INCLUDE_DIR=/soft/compilers/cudatoolkit/cuda-11.4.4/include \\\n         -DCUDA_LIB_DIR=/soft/compilers/cudatoolkit/cuda-11.4.4/lib64\n$ make -j 8\n$ make install\n\nValidate installation: if successful, then info on code version, platform types, CUDA initialization, and force error tolerance will be shown. \n$ cd /path-to/openmm/examples\n$ python -m openmm.testInstallation\n\nBenchmark testing using the PBS job script above.\n$ cd /path-to/openmm/examples\n$ qsub ./submit.sh",
                "data/md/polaris/applications-and-libraries/applications/openmm.md"
            ],
            "```bash\n\n!/bin/bash -l\n\nnum_gpus=4\n\nneed to assign GPUs in reverse order due to topology\n\nSee Polaris Device Affinity Information:\n\nhttps://www.alcf.anl.gov/support/user-guides/polaris/hardware-overview/machine-overview/index.html\n\ngpu=$((${num_gpus} - 1 - ${PMI_LOCAL_RANK} % ${num_gpus}))\nexport CUDA_VISIBLE_DEVICES=$gpu\necho “RANK= ${PMI_RANK} LOCAL_RANK= ${PMI_LOCAL_RANK} gpu= ${gpu}”\nexec \"$@\"\nThis script can be placed just before the executable in the `mpiexec` command like so.bash\nmpiexec -n ${NTOTRANKS} --ppn ${NRANKS_PER_NODE} --depth=${NDEPTH} --cpu-bind depth ./set_affinity_gpu_polaris.sh ./hello_affinity\n```\nUsers with different needs, such as assigning multiple GPUs per MPI rank, can modify the above script to suit their needs.\n\nInteractive Jobs on Compute Nodes\n\nHere is how to submit an interactive job to, for example, edit/build/test an application Polaris compute nodes:\nqsub -I -l select=1 -l filesystems=home:eagle -l walltime=1:00:00 -q debug -A <project_name>\n\nThis command requests 1 node for a period of 1 hour in the debug queue, requiring access to the /home and eagle filesystems. After waiting in the queue for a node to become available, a shell prompt on a compute node will appear. You may then start building applications and testing gpu affinity scripts on the compute node.\n\nNOTE: If you want to ssh or scp to one of your assigned compute nodes you will need to make sure your $HOME directory and your $HOME/.ssh directory permissions are both set to 700."
        ],
        "golden_context": "```bash\n\n!/bin/bash -l\n\nnum_gpus=4\n\nneed to assign GPUs in reverse order due to topology\n\nSee Polaris Device Affinity Information:\n\nhttps://www.alcf.anl.gov/support/user-guides/polaris/hardware-overview/machine-overview/index.html\n\ngpu=$((${num_gpus} - 1 - ${PMI_LOCAL_RANK} % ${num_gpus}))\nexport CUDA_VISIBLE_DEVICES=$gpu\necho “RANK= ${PMI_RANK} LOCAL_RANK= ${PMI_LOCAL_RANK} gpu= ${gpu}”\nexec \"$@\"\nThis script can be placed just before the executable in the `mpiexec` command like so.bash\nmpiexec -n ${NTOTRANKS} --ppn ${NRANKS_PER_NODE} --depth=${NDEPTH} --cpu-bind depth ./set_affinity_gpu_polaris.sh ./hello_affinity\n```\nUsers with different needs, such as assigning multiple GPUs per MPI rank, can modify the above script to suit their needs.\n\nInteractive Jobs on Compute Nodes\n\nHere is how to submit an interactive job to, for example, edit/build/test an application Polaris compute nodes:\nqsub -I -l select=1 -l filesystems=home:eagle -l walltime=1:00:00 -q debug -A <project_name>\n\nThis command requests 1 node for a period of 1 hour in the debug queue, requiring access to the /home and eagle filesystems. After waiting in the queue for a node to become available, a shell prompt on a compute node will appear. You may then start building applications and testing gpu affinity scripts on the compute node.\n\nNOTE: If you want to ssh or scp to one of your assigned compute nodes you will need to make sure your $HOME directory and your $HOME/.ssh directory permissions are both set to 700.",
        "question": "How do you ensure the permissions for $HOME and $HOME/.ssh directories are set for ssh or scp on assigned compute nodes??",
        "cot_answer": "To ensure the permissions for $HOME and $HOME/.ssh directories are set for ssh or scp on assigned compute nodes, follow these steps:\n\n1. Access a compute node by submitting an interactive job using the command provided in the context:\n   ```\n   qsub -I -l select=1 -l filesystems=home:eagle -l walltime=1:00:00 -q debug -A <project_name>\n   ```\n\n2. Once on the compute node, ensure that the permissions for the $HOME directory and $HOME/.ssh directory are set to 700. This can be done using the following commands:\n   ```\n   chmod 700 $HOME\n   chmod 700 $HOME/.ssh\n   ```\n\n3. Verify that the permissions have been correctly set by checking the permissions of the directories:\n   ```\n   ls -ld $HOME\n   ls -ld $HOME/.ssh\n   ```\n\nBy following these steps, you can ensure that the permissions for $HOME and $HOME/.ssh directories are set to 700 for ssh or scp on assigned compute nodes.\n\n<ANSWER>: The permissions for $HOME and $HOME/.ssh directories should be set to 700."
    },
    {
        "id": "data/md/polaris/running-jobs.md_seed_task_9_0",
        "context": [
            [
                "Cloning the base Anaconda environment\n\n!!! warning\n\nIf you need more flexibility, you can clone the conda environment into a custom\npath, which would then allow for root-like installations via #!bash conda install\n<module> or #!bash pip install <module>.\n\nUnlike the venv approach, using a cloned Anaconda environment requires you to\ncopy the entirety of the base environment, which can use significant storage\nspace.\n\nTo clone the base environment:\n\nbash\nmodule load conda ; conda activate base\nconda create --clone base --prefix /path/to/envs/base-clone\nconda activate /path/to/envs/base-clone\n\nwhere, #!bash path/to/envs/base-clone should be replaced by a suitably chosen\npath.\n\nNote: The cloning process can be quite slow.\n\nUsing pip install --user (not recommended)\n\n!!! danger\n\nWith the conda environment setup, one can install common Python modules using\n#!bash python3 pip install --users '<module-name>' which will install\npackages in #!bash $PYTHONUSERBASE/lib/pythonX.Y/site-packages.\n\nThe #!bash $PYTHONUSERBASE environment variable is automatically set when you\nload the base conda module, and is equal to  #!bash\n/home/$USER/.local/polaris/conda/YYYY-MM-DD.\n\nNote, Python modules installed this way that contain command line binaries will\nnot have those binaries automatically added to the shell's #!bash $PATH. To\nmanually add the path:\n\nbash\nexport PATH=\"$PYTHONUSERBASE/bin:$PATH\"",
                "data/md/polaris/data-science-workflows/python.md"
            ],
            [
                "$(info CRAY_INC = ${CRAY_INC})\n\n$(info CRAY_LIB = ${CRAY_LIB})\n\nCC =        $(KOKKOS_ABSOLUTE_PATH)/bin/nvcc_wrapper\nCCFLAGS =  -g -O3 -mp -DLAMMPS_MEMALIGN=64\nCCFLAGS += $(CRAY_INC)\nSHFLAGS =   -fPIC\nDEPFLAGS =  -M\n\nLINK =      $(CC)\nLIB =\nLIB += $(CRAY_LIB)\nSIZE =      size\n```\n\nWith the appropriate LAMMPS Makefile in place an executable can be compiled as in the following example. Note, per-user limits on the login nodes will reduce the maximum parallelism for compilation. Users are encouraged to compile on a compute node in an interactive session if necessary.\n\ncd lammps-<version>/src\nmake yes-KOKKOS\nmake polaris_nvhpc_kokkos -j 32\n\nRunning Jobs on Polaris\n\nAn example submission script for running a 64-node KOKKOS-enabled LAMMPS executable is below as an example. Additional information on LAMMPS application flags and options is described on the LAMMPS website.\n\n```\n\n!/bin/sh\n\nPBS -l select=64:system=polaris\n\nPBS -l place=scatter\n\nPBS -l walltime=0:15:00\n\nPBS -l filesystems=home:grand:eagle\n\nPBS -q prod\n\nPBS -A Catalyst\n\nexport MPICH_GPU_SUPPORT_ENABLED=1\n\nNNODES=wc -l < $PBS_NODEFILE\n\nper-node settings\n\nNRANKS=4\nNDEPTH=8\nNTHREADS=1\nNGPUS=4\n\nNTOTRANKS=$(( NNODES * NRANKS ))\n\n. ./setup_lammps_gnu.sh\n\nEXE=/home/knight/bin/lmp_polaris_gnu_kokkos\nEXE_ARG=\"-in in.reaxc.hns -k on g ${NGPUS} -sf kk -pk kokkos neigh half neigh/qeq full newton on \"\n\nOMP settings mostly to quiet Kokkos messages",
                "data/md/polaris/applications-and-libraries/applications/lammps.md"
            ],
            "Running Multiple MPI Applications on a node\n\nMultiple applications can be run simultaneously on a node by launching several mpiexec commands and backgrounding them. For performance, it will likely be necessary to ensure that each application runs on a distinct set of CPU resources and/or targets specific GPUs. One can provide a list of CPUs using the --cpu-bind option, which when combined with CUDA_VISIBLE_DEVICES provides a user with specifying exactly which CPU and GPU resources to run each application on. In the example below, four instances of the application are simultaneously running on a single node. In the first instance, the application is spawning MPI ranks 0-7 on CPUs 24-31 and using GPU 0. This mapping is based on output from the nvidia-smi topo -m command and pairs CPUs with the closest GPU.\n\n```bash\nexport CUDA_VISIBLE_DEVICES=0\nmpiexec -n 8 --ppn 8 --cpu-bind list:24:25:26:27:28:29:30:31 ./hello_affinity &\n\nexport CUDA_VISIBLE_DEVICES=1\nmpiexec -n 8 --ppn 8 --cpu-bind list:16:17:18:19:20:21:22:23 ./hello_affinity &\n\nexport CUDA_VISIBLE_DEVICES=2\nmpiexec -n 8 --ppn 8 --cpu-bind list:8:9:10:11:12:13:14:15 ./hello_affinity &\n\nexport CUDA_VISIBLE_DEVICES=3\nmpiexec -n 8 --ppn 8 --cpu-bind list:0:1:2:3:4:5:6:7 ./hello_affinity &\n\nwait\n```\n\nCompute Node Access to the Internet\n\nCurrently, the only access the internet is via a proxy.  Here are the proxy environment variables for Polaris:",
            [
                "ParaView on Polaris\n\nThe recommended way of running ParaView on Polaris is in client/server mode. This consists of running the ParaView client on your local resource, and the ParaView server on the Polaris compute nodes. The ParaView client needs to first be installed on your local resource, and needs to match the version that you run on Polaris.\n\nThere are multiple versions of ParaView installed on Polaris. To find the versions of ParaView currently available on Polaris run the following command on a login node: \nmodule use /soft/modulefiles\nmodule avail paraview\n\nBinary and source packages of the ParaView client for Linux, MacOS, and Windows are available from the ParaView Download Page.\n\nConnecting to the ParaView server on Polaris\n\nThis section describes how to launch the ParaView server on Polaris from a local ParaView client.\n\nStart ParaView Client\n\nFirst, launch the ParaView client on your local resource. You will need to configure some server settings in the client. This initial set up should only need to be done once, and can be reused each time you want to run ParaView on Polaris.\n\nServer Configuration\n\n1. Select Connect\n\nFrom the ParaView client choose to connect to a server by either clicking on the \"Connect\" icon in the menu bar\n\nor selecting File->Connect from the main menu\n\n2. Set Up Servers (first time only)",
                "data/md/polaris/visualization/paraview.md"
            ],
            [
                "FFmpeg on Polaris\n\nNOTE: FFmpeg module is currently missing on Polaris after a recent upgrade. A spack build of ffmpeg will be available soon\n\nTo use FFmpeg on Polaris first load the corresponding module:\n\nmodule load ffmpeg\n\nThis is a typical command line to create a movie from a series of snapshots in PNG format:\n\nffmpeg -r 15 -i frames.%03d.png -r 25 -pix_fmt yuv420p movie.mp4\n\nwhere:\n\nr 15 is the input frame rate. Experiment with values smaller than the output frame rate for longer movies.\n-r 25 is the output frame rate (use this value for standard 25 frames per second)\n-i frames.%03d.png reads the input frames in sequence\n-pix_fmt yuv420p is needed for movies to play in browsers\nmovie.mp4 is the resulting movie",
                "data/md/polaris/visualization/ffmpeg.md"
            ]
        ],
        "golden_context": "Running Multiple MPI Applications on a node\n\nMultiple applications can be run simultaneously on a node by launching several mpiexec commands and backgrounding them. For performance, it will likely be necessary to ensure that each application runs on a distinct set of CPU resources and/or targets specific GPUs. One can provide a list of CPUs using the --cpu-bind option, which when combined with CUDA_VISIBLE_DEVICES provides a user with specifying exactly which CPU and GPU resources to run each application on. In the example below, four instances of the application are simultaneously running on a single node. In the first instance, the application is spawning MPI ranks 0-7 on CPUs 24-31 and using GPU 0. This mapping is based on output from the nvidia-smi topo -m command and pairs CPUs with the closest GPU.\n\n```bash\nexport CUDA_VISIBLE_DEVICES=0\nmpiexec -n 8 --ppn 8 --cpu-bind list:24:25:26:27:28:29:30:31 ./hello_affinity &\n\nexport CUDA_VISIBLE_DEVICES=1\nmpiexec -n 8 --ppn 8 --cpu-bind list:16:17:18:19:20:21:22:23 ./hello_affinity &\n\nexport CUDA_VISIBLE_DEVICES=2\nmpiexec -n 8 --ppn 8 --cpu-bind list:8:9:10:11:12:13:14:15 ./hello_affinity &\n\nexport CUDA_VISIBLE_DEVICES=3\nmpiexec -n 8 --ppn 8 --cpu-bind list:0:1:2:3:4:5:6:7 ./hello_affinity &\n\nwait\n```\n\nCompute Node Access to the Internet\n\nCurrently, the only access the internet is via a proxy.  Here are the proxy environment variables for Polaris:",
        "question": "What is the only way a compute node has access to the internet?",
        "cot_answer": "To determine the only way a compute node has access to the internet based on the provided context, we need to focus on the information given about internet access and proxy settings.\n\n1. The context mentions that \"Currently, the only access to the internet is via a proxy.\" This indicates that the compute node accesses the internet through a proxy server.\n\n2. The context also provides information about the proxy environment variables for Polaris, which are not directly relevant to the question about how the compute node accesses the internet.\n\nTherefore, based on the information provided, we can conclude that:\n\n<ANSWER>: The only way a compute node has access to the internet is through a proxy server."
    }
]